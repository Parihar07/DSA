{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"text","id":"F2sG5VM-sVHB"},"cell_type":"markdown","source":["\n","\n","\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n","## Autograd Basics and NN Module"]},{"metadata":{"id":"YA1rSjjpkiP4","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"UZ_c5nIikpCP","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P19A06E_test\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6vtYjmuqksc_","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"981234567\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D4g7GNtxkvGk","colab_type":"code","cellView":"form","outputId":"2a5d5e4f-4f00-42db-fed6-136ff3ad9a09","executionInfo":{"status":"ok","timestamp":1541219749699,"user_tz":-330,"elapsed":5331,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M2W7_DEMO_EXP2\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx pip3 install torch\")\n","   \n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful. Ref:\", submission_id)\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"_pK7fj0BhmMx","colab":{}},"cell_type":"code","source":["import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MJS4XfP0gvSA","colab_type":"text"},"cell_type":"markdown","source":["###Creating a random tensor"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219752965,"user_tz":-330,"elapsed":1534,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"eyl7jcIDibGi","outputId":"58f267ed-62bd-453d-9037-2ab055a335d4","colab":{"base_uri":"https://localhost:8080/","height":86}},"cell_type":"code","source":["x = torch.rand(3,3)\n","print(x)\n","print(x.type())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0.1768, 0.9096, 0.6366],\n","        [0.0256, 0.1732, 0.7924],\n","        [0.2874, 0.4250, 0.3732]])\n","torch.FloatTensor\n"],"name":"stdout"}]},{"metadata":{"id":"jtFCKEaXhAWe","colab_type":"text"},"cell_type":"markdown","source":["###checking the requires_grad flag which tells whether the tensor support gradient calculation\n","###By default it is false"]},{"metadata":{"id":"9pnHOZcjg6ZL","colab_type":"code","outputId":"ac843135-333b-4bd9-9bf4-d4ae61f8cae8","executionInfo":{"status":"ok","timestamp":1541219754693,"user_tz":-330,"elapsed":1580,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(x.requires_grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"gfYirFA1hk4E"},"cell_type":"markdown","source":["**Autograd-1** (Automatic Differentiation) "]},{"metadata":{"colab_type":"text","id":"ON44udvSktwJ"},"cell_type":"markdown","source":["Example 1: <br>\n","Consider the following analytical function:- <br>\n","$y=4x^2\\\\\n","z=3y+5\\\\\n","l=\\frac{1}{4} \\sum_{i}^4{z_i}\\\\\n","$\n","<br>\n","Let, $x = \\left[ \\begin{matrix}\n","  1 & 1 \\\\\n","  1 & 1\n"," \\end{matrix}\\right ]$\n"," <br>\n","The above set of functions can be visualized as a **computational graph**."]},{"metadata":{"id":"MViM4SIQhI9X","colab_type":"text"},"cell_type":"markdown","source":["###Initialiation of x (notice we enforced the computation of gradients)"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219756989,"user_tz":-330,"elapsed":2097,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"yz4zwLeEhkuZ","outputId":"a9d7ea82-6147-4484-9978-8e95ef70aca0","colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["x = torch.ones(2,2,requires_grad=True)\n","print(x)\n","print(x.type())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n","torch.FloatTensor\n"],"name":"stdout"}]},{"metadata":{"id":"emiFhdxehPZF","colab_type":"text"},"cell_type":"markdown","source":["###Creating the computational graph\n","###The graph is created in a dynamic manner (on the fly)"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219758298,"user_tz":-330,"elapsed":1182,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"oc7ECcGwhkjN","outputId":"54092c40-090b-4002-faa2-015de05f5171","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["y = 4*x*x\n","z = 3*y+5\n","z.retain_grad() #Only used if we want to preserve the gradients in intermediate variables. (Generally not retained)\n","out = z.mean()\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(17., grad_fn=<MeanBackward1>)\n"],"name":"stdout"}]},{"metadata":{"id":"aCz9p9athWNa","colab_type":"text"},"cell_type":"markdown","source":["###Edges in the graph are connected with grad_fn function"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219759779,"user_tz":-330,"elapsed":1306,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"Hn7ab8x9hkaR","outputId":"31cc42ee-dc1b-44ef-e82c-406776fe6f81","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(z.grad_fn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<AddBackward object at 0x7f3951feddd8>\n"],"name":"stdout"}]},{"metadata":{"id":"-joCtUt7hbrg","colab_type":"text"},"cell_type":"markdown","source":["###Calling the backward operation to compute gradients of \"out\" variable w.r.t input tensor"]},{"metadata":{"colab_type":"code","id":"hvydBUdfhkQ8","colab":{}},"cell_type":"code","source":["out.backward()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219762908,"user_tz":-330,"elapsed":1602,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"KxDHQEashkGg","outputId":"bdd375f2-4f8f-4e29-f6f7-619ca4061ced","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["print(x.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[6., 6.],\n","        [6., 6.]])\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"iZFobKFqhj3k"},"cell_type":"markdown","source":["#**Autograd-1** \n","###Example 2"]},{"metadata":{"id":"R_efMxXBhzVZ","colab_type":"text"},"cell_type":"markdown","source":["###Consider the following linear function (line equation)\n","###Here w and b can be treated as parameters of a one layer neural network."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219764552,"user_tz":-330,"elapsed":1451,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"p0wW6RxDhiHa","outputId":"195236ae-5021-4f56-a608-c86ea9e68b2f","colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["x = torch.tensor(1.0, requires_grad=True)\n","w = torch.tensor(2.0, requires_grad=True)\n","b = torch.tensor(3.0, requires_grad=True)\n","\n","y = w * x + b    #e.g. y = 2 * x + 3\n","\n","# Compute gradients.\n","y.backward()\n","\n","# Print out the gradients.\n","print(x.grad)    # x.grad = 2 \n","print(w.grad)    # w.grad = 1 \n","print(b.grad)    # b.grad = 1 "],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"wLCgBsfSknED"},"cell_type":"markdown","source":["# **Autograd-2 with nn.Module**\n","\n","###Designing a simple (one hidden layer) neural network using torch.nn module "]},{"metadata":{"id":"c0ziRR1QiGp-","colab_type":"text"},"cell_type":"markdown","source":["* torch.nn is a specialized package for neural network functions\n","* torch.optim provides various inbuilt optimization routines for training neural network"]},{"metadata":{"colab_type":"code","id":"9zeppDpsRt_c","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CTqPITioi2hb","colab_type":"text"},"cell_type":"markdown","source":["###Create a random input x and expected output y "]},{"metadata":{"colab_type":"code","id":"G-yLBiAIejiH","colab":{}},"cell_type":"code","source":["x = torch.rand(3)\n","y = torch.tensor(1.0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HkvwwKvci7zc","colab_type":"text"},"cell_type":"markdown","source":["###Creating a Linear layer using nn Module with weight and bias paramters (just like example 2 in Autograd-1)\n","\n","* weights (initialized randomly) but notice by default the grad is enabled\n"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541219813639,"user_tz":-330,"elapsed":936,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"OILdfIYhiLjx","outputId":"a010451f-9ea6-4d03-daeb-2e54377ddf09","colab":{"base_uri":"https://localhost:8080/","height":208}},"cell_type":"code","source":["fc1 = nn.Linear(3,1)\n","print(\"x: \\n\\n \", x)\n","\n","\n","print(\"Weights: \\n\\n \", fc1.weight)\n","\n","print(\"Bias: \\n\\n \", fc1.bias)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x: \n","\n","  tensor([0.6626, 0.5846, 0.2830])\n","Weights: \n","\n","  Parameter containing:\n","tensor([[-0.3329,  0.2321, -0.4230]], requires_grad=True)\n","Bias: \n","\n","  Parameter containing:\n","tensor([0.2266], requires_grad=True)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183347762,"user_tz":-330,"elapsed":4411,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"ZVa2CfxZelGc","outputId":"72ee3feb-81e7-449f-cfd7-d50bd83b725a","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(fc1.weight.requires_grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"metadata":{"id":"s0wVbOKzjPTl","colab_type":"text"},"cell_type":"markdown","source":["###Printing all the paramters given by the linear layer"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183351113,"user_tz":-330,"elapsed":3117,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"0TCnWKZdg-Bj","outputId":"7ac23a18-a861-42dd-a6ac-bd9d1a6a17aa","colab":{"base_uri":"https://localhost:8080/","height":86}},"cell_type":"code","source":["for param in fc1.parameters():\n","  print(param)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parameter containing:\n","tensor([[ 0.4188, -0.5132, -0.5575]], requires_grad=True)\n","Parameter containing:\n","tensor([-0.0243], requires_grad=True)\n"],"name":"stdout"}]},{"metadata":{"id":"MEc8iQCIjgAq","colab_type":"text"},"cell_type":"markdown","source":["###Defining the loss function again from nn Module\n","###Here we use mean square loss function"]},{"metadata":{"colab_type":"code","id":"CkcSc6tqggc3","colab":{}},"cell_type":"code","source":["criterion=nn.MSELoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jrFK4jodjjPr","colab_type":"text"},"cell_type":"markdown","source":["###Defining the optimization function using torch.optim package.\n","###Here the first param is the \"list of parameters\" which we want to optimize and \"lr\" is the learning rate"]},{"metadata":{"colab_type":"code","id":"D0tyKmYjgtYf","colab":{}},"cell_type":"code","source":["optimizer = optim.SGD(fc1.parameters(),lr=0.1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R3tLPX9Yjy2t","colab_type":"text"},"cell_type":"markdown","source":["###Clears the gradients of parameters"]},{"metadata":{"colab_type":"code","id":"txekstt7hj3V","colab":{}},"cell_type":"code","source":["optimizer.zero_grad()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rNHqmMTgjrs4","colab_type":"text"},"cell_type":"markdown","source":["###performing the forward pass"]},{"metadata":{"id":"zGdfEhcXjtZt","colab_type":"code","outputId":"881d0e39-d27c-4c29-ff0e-5e96b9922eca","executionInfo":{"status":"ok","timestamp":1541183365952,"user_tz":-330,"elapsed":3123,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["y_pred = fc1(x)\n","print(y_pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([-0.5580], grad_fn=<ThAddBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"Onf1S8Z4j-0h","colab_type":"text"},"cell_type":"markdown","source":["###Computing loss (predicted vs actual)"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183370329,"user_tz":-330,"elapsed":4238,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"PQM8weRah6Ge","outputId":"3755c524-ceb3-4303-c3e4-704e38398aed","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["loss = criterion(y_pred,y)\n","print(loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.4275, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"rHU6JVs0j5wv","colab_type":"text"},"cell_type":"markdown","source":["###Since loss is a scalar value, we use item() for getting the python number."]},{"metadata":{"id":"xJlxyUIAj6I8","colab_type":"code","outputId":"ca72f8d4-bf6a-4033-8785-8eaf20c0f714","executionInfo":{"status":"ok","timestamp":1541183373258,"user_tz":-330,"elapsed":2490,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(loss.item())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.4275097846984863\n"],"name":"stdout"}]},{"metadata":{"id":"aJNoea7VkF5r","colab_type":"text"},"cell_type":"markdown","source":["###Compute gradients using backpropagation"]},{"metadata":{"colab_type":"code","id":"IZ94l-BdlCyR","colab":{}},"cell_type":"code","source":["loss.backward()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183380094,"user_tz":-330,"elapsed":3140,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"zPqh4je4lKEw","outputId":"e890dac0-9a90-490b-8f8c-678c4048d023","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(fc1.weight.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-2.6357, -2.4555, -2.7029]])\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183382457,"user_tz":-330,"elapsed":1945,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"5-62gnchlL_3","outputId":"05f86b5f-a622-46e6-82fc-17050e6d1919","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["fc1.weight"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.4188, -0.5132, -0.5575]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"qXOjRFdQkWRY","colab_type":"text"},"cell_type":"markdown","source":["###Performing a single step of gradient descent. Note this will change the parameters (weights and bias)"]},{"metadata":{"colab_type":"code","id":"-MzvvfeWlOAi","colab":{}},"cell_type":"code","source":["optimizer.step()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CXiHk6rNke59","colab_type":"text"},"cell_type":"markdown","source":["###Notice the change of values after the step()"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183389090,"user_tz":-330,"elapsed":3500,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"s5OMNsoMlRLy","outputId":"eac150ca-274b-4a31-ae03-51c6ba457795","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["fc1.weight"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.6824, -0.2676, -0.2872]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"Si8HS52Akhkz","colab_type":"text"},"cell_type":"markdown","source":["###Lets do a forward pass again and print the loss. \n","###Note after the weight updation, the loss on the same input should decrease."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1541183393371,"user_tz":-330,"elapsed":3867,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"id":"fwquR9lwmljO","outputId":"976869af-7b36-4f7a-db7f-28fd104b5282","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["y_pred = fc1(x)\n","print(y_pred)\n","loss = criterion(y_pred,y)\n","print(loss.item())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([0.4044], grad_fn=<ThAddBackward>)\n","0.3546842634677887\n"],"name":"stdout"}]},{"metadata":{"id":"F3Pj7lzHlS30","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"metadata":{"id":"Mwm12ECQlRX8","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kLfw2ZUzlYvC","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fIzjg49Ylcbh","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-KIUHYHJlfda","colab_type":"code","cellView":"form","outputId":"b2702b24-2736-466b-920e-ce038e030924","executionInfo":{"status":"ok","timestamp":1541183409164,"user_tz":-330,"elapsed":3123,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Your submission is successful. Ref: 9450\n"],"name":"stdout"}]}]}