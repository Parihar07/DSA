{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_Regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"lBPJEbHmmlS_","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"giKRyJXKmpqL","colab_type":"text"},"cell_type":"markdown","source":["\n","####Regression using MLP with MSE Loss"]},{"metadata":{"id":"3iXTrneDmzN_","colab_type":"text"},"cell_type":"markdown","source":["The objective of this case study is to understand regression i.e., to predict the price of the house using Multilayer perceptron with Cross Entropy Loss.  The package used here is  [PyTorch](https://pytorch.org/). "]},{"metadata":{"id":"v1BKwE3rWi7j","colab_type":"text"},"cell_type":"markdown","source":["#### Setup Steps"]},{"metadata":{"id":"wsULhbIZWh3I","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P18_test\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IziqgHiFWg0B","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"912345678\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LpbyKaH3Wg44","colab_type":"code","cellView":"form","outputId":"1b1af57e-00a7-4fc2-cae7-73fe88b3b0ac","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1548229600300,"user_tz":-330,"elapsed":93974,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M2W6_SUN_CS_2\" #name of the notebook\n","\n","def setup():\n","    #ipython.magic(\"sx wget https://www.dropbox.com/s/vu7xkf6j3v9p5np/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt.zip?dl=1\")\n","   # ipython.magic(\"sx unzip AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt.zip?dl=1\")\n","    ipython.magic(\"sx pip install torch\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(\"Please enter valid Id\")\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful. Ref:\", submission_id)\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"T2vxSk8VpJ0L","colab_type":"code","colab":{}},"cell_type":"code","source":["# Importing required Packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import  torch\n","from torch import nn\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-23T07:34:08.691158Z","start_time":"2018-11-23T07:34:08.685467Z"},"id":"KhJrSQ0wmQDN","colab_type":"code","colab":{}},"cell_type":"code","source":["#The attributes of related House price are stored in \"X\" as features. \n","X = np.array([[3, 2000, 90], [2, 800, 143], [2, 850, 167], [1, 550, 267], [4, 2000, 396]])\n","#The prices of the houses are stored in \"y\" as labels\n","y =  np.array([23.0, 8, 9.0, 9.0 , 25.0])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rUBlUjsLpxQC","colab_type":"code","outputId":"03b36915-0651-4f66-8121-e595f1a887f8","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1548229613054,"user_tz":-330,"elapsed":880,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["#Standard scaling the features \"X\"\n","ss = StandardScaler()\n","ss.fit(X)\n","X = ss.transform(X)\n","X"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([[ 0.58834841,  1.20863526, -1.13296108],\n","       [-0.39223227, -0.6997362 , -0.64318182],\n","       [-0.39223227, -0.62022073, -0.42139498],\n","       [-1.37281295, -1.09731359,  0.50271682],\n","       [ 1.56892908,  1.20863526,  1.69482106]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-23T07:32:33.885729Z","start_time":"2018-11-23T07:32:33.875066Z"},"id":"4NTCCccwmQEp","colab_type":"code","colab":{}},"cell_type":"code","source":["#Defining the model for Linear Regression with MLP using PyTorch's nn.Module\n","class LinearRegressionModel(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim):\n","\n","        super(LinearRegressionModel, self).__init__() \n","        # Calling Super Class's constructor\n","        self.linear1 = nn.Linear(input_dim, 4)\n","        self.sigmoid = nn.Sigmoid()\n","        self.linear2 = nn.Linear(4, output_dim)\n","        # nn.Linear is defined in nn.Module\n","\n","    def forward(self, x):\n","        # Here the forward pass is simply a linear function\n","        #print(x.size())\n","        out = self.sigmoid(self.linear1(x))\n","        out = self.linear2(out)\n","        return out\n","\n","input_dim = 3\n","output_dim = 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ovkoTbXrWE6","colab_type":"code","colab":{}},"cell_type":"code","source":["nn.Sigmoid?"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XntHmRUtn2wJ","colab_type":"text"},"cell_type":"markdown","source":["From the above defined model, we 3 neurons in the input layer, 4 neurons in the hidden layer and 1 neuron in the output layer. \n","\n","![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/MLP_Regression.png)"]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-23T07:33:14.943117Z","start_time":"2018-11-23T07:33:14.931903Z"},"id":"wQa8qRHemQEv","colab_type":"code","colab":{}},"cell_type":"code","source":["model = LinearRegressionModel(input_dim,output_dim)#The LinearRegressionModel() is saved in model\n","criterion = nn.MSELoss()# Mean Squared Loss\n","l_rate = 0.01 #Learning Rate\n","optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n","\n","epochs = 500 #number of epochs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jp4pB7G1odTI","colab_type":"text"},"cell_type":"markdown","source":["Here, we are using MSE Loss for two classes and Stochastic Gradient Descent on the entire batch.\n","\n","\n","It creates a criterion that measures the mean squared error between\n","    `n` elements in the input `x` and target `y`.\n","\n","The loss can be described as:\n","\n","$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n","        l_n = \\left( x_n - y_n \\right)^2$\n","\n"," \n","The losses are averaged across observations for the batch.\n","\n","For more details, can follow this [link](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html). "]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-23T07:37:06.946827Z","start_time":"2018-11-23T07:37:06.921324Z"},"scrolled":false,"id":"Tm7a9HyXmQE1","colab_type":"code","outputId":"12dee2b5-b2e8-4625-a0d8-73086e636210","colab":{"base_uri":"https://localhost:8080/","height":27217},"executionInfo":{"status":"ok","timestamp":1548229749987,"user_tz":-330,"elapsed":2350,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["#Storing the losses in a list for the prescribed epochs\n","losses = []\n","for epoch in range(epochs):\n","#increase the number of epochs by 1 every time\n","    epoch +=1\n","    inputs = torch.from_numpy(X.astype(np.float32))\n","    labels = torch.from_numpy(y.astype(np.float32))\n","    #clear grads as discussed in prev post\n","    optimiser.zero_grad()\n","    #forward to get predicted values\n","    outputs = model.forward(inputs)\n","    #print('outputs: ', outputs.size())\n","    #print('labels: ', labels.size())\n","    loss = criterion(outputs, labels.unsqueeze(1))\n","    loss.backward()# back props\n","    optimiser.step()# update the parameters\n","    print('epoch {}, loss {}'.format(epoch,loss.item()))\n","    losses.append(loss.item())\n","    if (epoch-1)%5 == 0:\n","        for i in model.parameters():\n","            print(i)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["epoch 1, loss 268.8075866699219\n","Parameter containing:\n","tensor([[-0.3333,  0.1166,  0.0509],\n","        [-0.2005,  0.1971,  0.0016],\n","        [-0.5233, -0.1898,  0.4205],\n","        [-0.0365, -0.0681, -0.4328]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3277,  0.1913, -0.1399, -0.0165], requires_grad=True)\n","Parameter containing:\n","tensor([[ 0.4048, -0.0342,  0.1266, -0.0773]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6084], requires_grad=True)\n","epoch 2, loss 252.11549377441406\n","epoch 3, loss 236.7325897216797\n","epoch 4, loss 222.35980224609375\n","epoch 5, loss 208.7726287841797\n","epoch 6, loss 195.80780029296875\n","Parameter containing:\n","tensor([[-0.2132,  0.2468,  0.0923],\n","        [-0.1568,  0.2446,  0.0176],\n","        [-0.4689, -0.1357,  0.4474],\n","        [-0.0122, -0.0403, -0.4269]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.5425,  0.2696, -0.0455,  0.0315], requires_grad=True)\n","Parameter containing:\n","tensor([[1.1550, 0.6786, 0.6468, 0.5226]], requires_grad=True)\n","Parameter containing:\n","tensor([1.8919], requires_grad=True)\n","epoch 7, loss 183.3560791015625\n","epoch 8, loss 171.35630798339844\n","epoch 9, loss 159.78741455078125\n","epoch 10, loss 148.6577911376953\n","epoch 11, loss 137.9938507080078\n","Parameter containing:\n","tensor([[-0.0347,  0.4471,  0.1429],\n","        [-0.0192,  0.3955,  0.0653],\n","        [-0.3331,  0.0052,  0.5068],\n","        [ 0.0989,  0.0811, -0.3869]], requires_grad=True)\n","Parameter containing:\n","tensor([0.8437, 0.4863, 0.1476, 0.2031], requires_grad=True)\n","Parameter containing:\n","tensor([[1.8589, 1.3104, 1.1054, 1.0345]], requires_grad=True)\n","Parameter containing:\n","tensor([2.9091], requires_grad=True)\n","epoch 12, loss 127.83039855957031\n","epoch 13, loss 118.20347595214844\n","epoch 14, loss 109.14566040039062\n","epoch 15, loss 100.68267822265625\n","epoch 16, loss 92.83094787597656\n","Parameter containing:\n","tensor([[ 0.1112,  0.6241,  0.1628],\n","        [ 0.1339,  0.5717,  0.1042],\n","        [-0.1593,  0.2022,  0.5511],\n","        [ 0.2770,  0.2673, -0.3014]], requires_grad=True)\n","Parameter containing:\n","tensor([1.0854, 0.7047, 0.3675, 0.4138], requires_grad=True)\n","Parameter containing:\n","tensor([[2.4760, 1.8742, 1.5313, 1.5006]], requires_grad=True)\n","Parameter containing:\n","tensor([3.6780], requires_grad=True)\n","epoch 17, loss 85.5953369140625\n","epoch 18, loss 78.96800994873047\n","epoch 19, loss 72.92867279052734\n","epoch 20, loss 67.44640350341797\n","epoch 21, loss 62.48265075683594\n","Parameter containing:\n","tensor([[ 2.2544e-01,  7.7223e-01,  1.7162e-01],\n","        [ 2.6236e-01,  7.2999e-01,  1.2485e-01],\n","        [ 4.4153e-04,  4.0647e-01,  5.4939e-01],\n","        [ 4.5200e-01,  4.5345e-01, -2.1668e-01]], requires_grad=True)\n","Parameter containing:\n","tensor([1.2271, 0.8504, 0.5476, 0.5790], requires_grad=True)\n","Parameter containing:\n","tensor([[2.9689, 2.3413, 1.9104, 1.9176]], requires_grad=True)\n","Parameter containing:\n","tensor([4.2338], requires_grad=True)\n","epoch 22, loss 57.99439239501953\n","epoch 23, loss 53.93701171875\n","epoch 24, loss 50.26634979248047\n","epoch 25, loss 46.94035339355469\n","epoch 26, loss 43.91985321044922\n","Parameter containing:\n","tensor([[ 0.3265,  0.9091,  0.1865],\n","        [ 0.3717,  0.8726,  0.1429],\n","        [ 0.1356,  0.5946,  0.5285],\n","        [ 0.5908,  0.6115, -0.1565]], requires_grad=True)\n","Parameter containing:\n","tensor([1.2820, 0.9190, 0.6626, 0.6687], requires_grad=True)\n","Parameter containing:\n","tensor([[3.3495, 2.7129, 2.2319, 2.2672]], requires_grad=True)\n","Parameter containing:\n","tensor([4.6276], requires_grad=True)\n","epoch 27, loss 41.16917419433594\n","epoch 28, loss 38.65629577636719\n","epoch 29, loss 36.352867126464844\n","epoch 30, loss 34.23402786254883\n","epoch 31, loss 32.27815628051758\n","Parameter containing:\n","tensor([[ 0.4199,  1.0410,  0.2123],\n","        [ 0.4678,  1.0047,  0.1667],\n","        [ 0.2490,  0.7606,  0.5128],\n","        [ 0.7015,  0.7479, -0.1091]], requires_grad=True)\n","Parameter containing:\n","tensor([1.2748, 0.9301, 0.7188, 0.6993], requires_grad=True)\n","Parameter containing:\n","tensor([[3.6473, 3.0099, 2.5012, 2.5539]], requires_grad=True)\n","Parameter containing:\n","tensor([4.9099], requires_grad=True)\n","epoch 32, loss 30.466596603393555\n","epoch 33, loss 28.783327102661133\n","epoch 34, loss 27.21462059020996\n","epoch 35, loss 25.748754501342773\n","epoch 36, loss 24.375703811645508\n","Parameter containing:\n","tensor([[ 0.5037,  1.1666,  0.2488],\n","        [ 0.5512,  1.1262,  0.1974],\n","        [ 0.3433,  0.9046,  0.5088],\n","        [ 0.7921,  0.8683, -0.0658]], requires_grad=True)\n","Parameter containing:\n","tensor([1.2296, 0.9057, 0.7345, 0.6927], requires_grad=True)\n","Parameter containing:\n","tensor([[3.8885, 3.2538, 2.7291, 2.7919]], requires_grad=True)\n","Parameter containing:\n","tensor([5.1192], requires_grad=True)\n","epoch 37, loss 23.086912155151367\n","epoch 38, loss 21.875045776367188\n","epoch 39, loss 20.733776092529297\n","epoch 40, loss 19.657634735107422\n","epoch 41, loss 18.64186668395996\n","Parameter containing:\n","tensor([[ 0.5759,  1.2823,  0.2926],\n","        [ 0.6219,  1.2357,  0.2327],\n","        [ 0.4203,  1.0281,  0.5151],\n","        [ 0.8668,  0.9748, -0.0242]], requires_grad=True)\n","Parameter containing:\n","tensor([1.1654, 0.8631, 0.7268, 0.6652], requires_grad=True)\n","Parameter containing:\n","tensor([[4.0905, 3.4594, 2.9248, 2.9933]], requires_grad=True)\n","Parameter containing:\n","tensor([5.2811], requires_grad=True)\n","epoch 42, loss 17.68230628967285\n","epoch 43, loss 16.775243759155273\n","epoch 44, loss 15.917411804199219\n","epoch 45, loss 15.105827331542969\n","epoch 46, loss 14.337834358215332\n","Parameter containing:\n","tensor([[0.6361, 1.3862, 0.3397],\n","        [0.6805, 1.3325, 0.2698],\n","        [0.4821, 1.1332, 0.5284],\n","        [0.9286, 1.0685, 0.0157]], requires_grad=True)\n","Parameter containing:\n","tensor([1.0950, 0.8134, 0.7073, 0.6279], requires_grad=True)\n","Parameter containing:\n","tensor([[4.2636, 3.6358, 3.0945, 3.1662]], requires_grad=True)\n","Parameter containing:\n","tensor([5.4115], requires_grad=True)\n","epoch 47, loss 13.610965728759766\n","epoch 48, loss 12.922958374023438\n","epoch 49, loss 12.271727561950684\n","epoch 50, loss 11.655316352844238\n","epoch 51, loss 11.071901321411133\n","Parameter containing:\n","tensor([[0.6853, 1.4775, 0.3869],\n","        [0.7288, 1.4170, 0.3064],\n","        [0.5312, 1.2223, 0.5460],\n","        [0.9798, 1.1504, 0.0531]], requires_grad=True)\n","Parameter containing:\n","tensor([1.0257, 0.7630, 0.6828, 0.5871], requires_grad=True)\n","Parameter containing:\n","tensor([[4.4136, 3.7885, 3.2424, 3.3157]], requires_grad=True)\n","Parameter containing:\n","tensor([5.5193], requires_grad=True)\n","epoch 52, loss 10.519756317138672\n","epoch 53, loss 9.997251510620117\n","epoch 54, loss 9.502860069274902\n","epoch 55, loss 9.035104751586914\n","epoch 56, loss 8.592605590820312\n","Parameter containing:\n","tensor([[0.7251, 1.5568, 0.4319],\n","        [0.7683, 1.4901, 0.3413],\n","        [0.5700, 1.2977, 0.5657],\n","        [1.0224, 1.2218, 0.0877]], requires_grad=True)\n","Parameter containing:\n","tensor([0.9613, 0.7150, 0.6571, 0.5463], requires_grad=True)\n","Parameter containing:\n","tensor([[4.5442, 3.9209, 3.3711, 3.4453]], requires_grad=True)\n","Parameter containing:\n","tensor([5.6101], requires_grad=True)\n","epoch 57, loss 8.174036979675293\n","epoch 58, loss 7.77815580368042\n","epoch 59, loss 7.403755187988281\n","epoch 60, loss 7.049697399139404\n","epoch 61, loss 6.714905738830566\n","Parameter containing:\n","tensor([[0.7572, 1.6252, 0.4740],\n","        [0.8005, 1.5532, 0.3739],\n","        [0.6005, 1.3615, 0.5865],\n","        [1.0579, 1.2839, 0.1195]], requires_grad=True)\n","Parameter containing:\n","tensor([0.9030, 0.6708, 0.6320, 0.5073], requires_grad=True)\n","Parameter containing:\n","tensor([[4.6578, 4.0358, 3.4831, 3.5577]], requires_grad=True)\n","Parameter containing:\n","tensor([5.6874], requires_grad=True)\n","epoch 62, loss 6.398344039916992\n","epoch 63, loss 6.099031925201416\n","epoch 64, loss 5.816041946411133\n","epoch 65, loss 5.548483848571777\n","epoch 66, loss 5.295513153076172\n","Parameter containing:\n","tensor([[0.7829, 1.6840, 0.5127],\n","        [0.8269, 1.6075, 0.4042],\n","        [0.6242, 1.4155, 0.6077],\n","        [1.0877, 1.3378, 0.1486]], requires_grad=True)\n","Parameter containing:\n","tensor([0.8511, 0.6306, 0.6082, 0.4707], requires_grad=True)\n","Parameter containing:\n","tensor([[4.7565, 4.1354, 3.5804, 3.6551]], requires_grad=True)\n","Parameter containing:\n","tensor([5.7535], requires_grad=True)\n","epoch 67, loss 5.056339263916016\n","epoch 68, loss 4.830193042755127\n","epoch 69, loss 4.616369247436523\n","epoch 70, loss 4.414168834686279\n","epoch 71, loss 4.22295618057251\n","Parameter containing:\n","tensor([[0.8033, 1.7345, 0.5484],\n","        [0.8485, 1.6544, 0.4323],\n","        [0.6425, 1.4612, 0.6288],\n","        [1.1127, 1.3848, 0.1753]], requires_grad=True)\n","Parameter containing:\n","tensor([0.8051, 0.5943, 0.5861, 0.4369], requires_grad=True)\n","Parameter containing:\n","tensor([[4.8421, 4.2216, 3.6646, 3.7393]], requires_grad=True)\n","Parameter containing:\n","tensor([5.8102], requires_grad=True)\n","epoch 72, loss 4.042118072509766\n","epoch 73, loss 3.8710744380950928\n","epoch 74, loss 3.709278106689453\n","epoch 75, loss 3.5562076568603516\n","epoch 76, loss 3.4113688468933105\n","Parameter containing:\n","tensor([[0.8195, 1.7778, 0.5812],\n","        [0.8660, 1.6948, 0.4583],\n","        [0.6563, 1.4999, 0.6497],\n","        [1.1338, 1.4258, 0.1998]], requires_grad=True)\n","Parameter containing:\n","tensor([0.7644, 0.5615, 0.5657, 0.4058], requires_grad=True)\n","Parameter containing:\n","tensor([[4.9163, 4.2962, 3.7376, 3.8120]], requires_grad=True)\n","Parameter containing:\n","tensor([5.8590], requires_grad=True)\n","epoch 77, loss 3.274308204650879\n","epoch 78, loss 3.144580125808716\n","epoch 79, loss 3.02177095413208\n","epoch 80, loss 2.905489444732666\n","epoch 81, loss 2.7953715324401855\n","Parameter containing:\n","tensor([[0.8320, 1.8150, 0.6116],\n","        [0.8802, 1.7298, 0.4826],\n","        [0.6663, 1.5327, 0.6702],\n","        [1.1516, 1.4615, 0.2225]], requires_grad=True)\n","Parameter containing:\n","tensor([0.7282, 0.5319, 0.5470, 0.3771], requires_grad=True)\n","Parameter containing:\n","tensor([[4.9806, 4.3606, 3.8007, 3.8748]], requires_grad=True)\n","Parameter containing:\n","tensor([5.9012], requires_grad=True)\n","epoch 82, loss 2.6910603046417236\n","epoch 83, loss 2.592235565185547\n","epoch 84, loss 2.498579978942871\n","epoch 85, loss 2.409806728363037\n","epoch 86, loss 2.3256354331970215\n","Parameter containing:\n","tensor([[0.8416, 1.8469, 0.6399],\n","        [0.8917, 1.7600, 0.5053],\n","        [0.6732, 1.5603, 0.6904],\n","        [1.1667, 1.4929, 0.2436]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6961, 0.5052, 0.5300, 0.3508], requires_grad=True)\n","Parameter containing:\n","tensor([[5.0362, 4.4162, 3.8554, 3.9289]], requires_grad=True)\n","Parameter containing:\n","tensor([5.9378], requires_grad=True)\n","epoch 87, loss 2.2458081245422363\n","epoch 88, loss 2.1700754165649414\n","epoch 89, loss 2.0982096195220947\n","epoch 90, loss 2.0299901962280273\n","epoch 91, loss 1.965212106704712\n","Parameter containing:\n","tensor([[0.8486, 1.8743, 0.6663],\n","        [0.9008, 1.7862, 0.5267],\n","        [0.6775, 1.5837, 0.7102],\n","        [1.1794, 1.5204, 0.2632]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6674, 0.4809, 0.5145, 0.3264], requires_grad=True)\n","Parameter containing:\n","tensor([[5.0843, 4.4643, 3.9027, 3.9757]], requires_grad=True)\n","Parameter containing:\n","tensor([5.9696], requires_grad=True)\n","epoch 92, loss 1.9036787748336792\n","epoch 93, loss 1.8452122211456299\n","epoch 94, loss 1.7896369695663452\n","epoch 95, loss 1.736788272857666\n","epoch 96, loss 1.6865193843841553\n","Parameter containing:\n","tensor([[0.8534, 1.8978, 0.6912],\n","        [0.9079, 1.8090, 0.5469],\n","        [0.6795, 1.6032, 0.7296],\n","        [1.1902, 1.5446, 0.2816]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6417, 0.4588, 0.5005, 0.3040], requires_grad=True)\n","Parameter containing:\n","tensor([[5.1261, 4.5060, 3.9438, 4.0160]], requires_grad=True)\n","Parameter containing:\n","tensor([5.9976], requires_grad=True)\n","epoch 97, loss 1.6386810541152954\n","epoch 98, loss 1.5931392908096313\n","epoch 99, loss 1.5497640371322632\n","epoch 100, loss 1.5084367990493774\n","epoch 101, loss 1.4690425395965576\n","Parameter containing:\n","tensor([[0.8565, 1.9180, 0.7147],\n","        [0.9134, 1.8288, 0.5660],\n","        [0.6795, 1.6196, 0.7486],\n","        [1.1994, 1.5660, 0.2989]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6187, 0.4386, 0.4877, 0.2832], requires_grad=True)\n","Parameter containing:\n","tensor([[5.1623, 4.5420, 3.9794, 4.0509]], requires_grad=True)\n","Parameter containing:\n","tensor([6.0222], requires_grad=True)\n","epoch 102, loss 1.431475281715393\n","epoch 103, loss 1.3956331014633179\n","epoch 104, loss 1.3614227771759033\n","epoch 105, loss 1.328752040863037\n","epoch 106, loss 1.2975358963012695\n","Parameter containing:\n","tensor([[0.8579, 1.9354, 0.7370],\n","        [0.9174, 1.8460, 0.5842],\n","        [0.6779, 1.6331, 0.7673],\n","        [1.2071, 1.5850, 0.3152]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5979, 0.4202, 0.4762, 0.2640], requires_grad=True)\n","Parameter containing:\n","tensor([[5.1938, 4.5734, 4.0105, 4.0812]], requires_grad=True)\n","Parameter containing:\n","tensor([6.0441], requires_grad=True)\n","epoch 107, loss 1.2676961421966553\n","epoch 108, loss 1.2391573190689087\n","epoch 109, loss 1.2118496894836426\n","epoch 110, loss 1.185705542564392\n","epoch 111, loss 1.1606640815734863\n","Parameter containing:\n","tensor([[0.8581, 1.9502, 0.7582],\n","        [0.9203, 1.8610, 0.6016],\n","        [0.6749, 1.6442, 0.7856],\n","        [1.2137, 1.6019, 0.3306]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5790, 0.4032, 0.4658, 0.2460], requires_grad=True)\n","Parameter containing:\n","tensor([[5.2212, 4.6006, 4.0377, 4.1074]], requires_grad=True)\n","Parameter containing:\n","tensor([6.0636], requires_grad=True)\n","epoch 112, loss 1.136662483215332\n","epoch 113, loss 1.1136469841003418\n","epoch 114, loss 1.0915642976760864\n","epoch 115, loss 1.0703665018081665\n","epoch 116, loss 1.0500038862228394\n","Parameter containing:\n","tensor([[0.8571, 1.9629, 0.7785],\n","        [0.9221, 1.8741, 0.6182],\n","        [0.6706, 1.6532, 0.8036],\n","        [1.2192, 1.6170, 0.3453]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5620, 0.3876, 0.4564, 0.2293], requires_grad=True)\n","Parameter containing:\n","tensor([[5.2452, 4.6243, 4.0615, 4.1301]], requires_grad=True)\n","Parameter containing:\n","tensor([6.0813], requires_grad=True)\n","epoch 117, loss 1.0304334163665771\n","epoch 118, loss 1.0116156339645386\n","epoch 119, loss 0.9935091733932495\n","epoch 120, loss 0.9760783314704895\n","epoch 121, loss 0.9592856168746948\n","Parameter containing:\n","tensor([[0.8552, 1.9738, 0.7980],\n","        [0.9230, 1.8857, 0.6341],\n","        [0.6652, 1.6604, 0.8212],\n","        [1.2239, 1.6306, 0.3592]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5465, 0.3731, 0.4480, 0.2137], requires_grad=True)\n","Parameter containing:\n","tensor([[5.2662, 4.6451, 4.0824, 4.1500]], requires_grad=True)\n","Parameter containing:\n","tensor([6.0973], requires_grad=True)\n","epoch 122, loss 0.9431003928184509\n","epoch 123, loss 0.9274910688400269\n","epoch 124, loss 0.9124289751052856\n","epoch 125, loss 0.8978844881057739\n","epoch 126, loss 0.8838339447975159\n","Parameter containing:\n","tensor([[0.8524, 1.9831, 0.8167],\n","        [0.9232, 1.8958, 0.6493],\n","        [0.6589, 1.6660, 0.8385],\n","        [1.2278, 1.6428, 0.3724]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5324, 0.3598, 0.4405, 0.1990], requires_grad=True)\n","Parameter containing:\n","tensor([[5.2847, 4.6633, 4.1009, 4.1673]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1120], requires_grad=True)\n","epoch 127, loss 0.8702501058578491\n","epoch 128, loss 0.8571118116378784\n","epoch 129, loss 0.844396710395813\n","epoch 130, loss 0.8320848345756531\n","epoch 131, loss 0.8201544284820557\n","Parameter containing:\n","tensor([[0.8489, 1.9911, 0.8347],\n","        [0.9228, 1.9047, 0.6639],\n","        [0.6518, 1.6703, 0.8554],\n","        [1.2312, 1.6539, 0.3850]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5195, 0.3474, 0.4339, 0.1852], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3010, 4.6794, 4.1173, 4.1826]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1256], requires_grad=True)\n","epoch 132, loss 0.8085892200469971\n","epoch 133, loss 0.7973695397377014\n","epoch 134, loss 0.7864807844161987\n","epoch 135, loss 0.7759073376655579\n","epoch 136, loss 0.7656331658363342\n","Parameter containing:\n","tensor([[0.8448, 1.9979, 0.8520],\n","        [0.9219, 1.9127, 0.6780],\n","        [0.6439, 1.6734, 0.8720],\n","        [1.2340, 1.6641, 0.3970]], requires_grad=True)\n","Parameter containing:\n","tensor([0.5077, 0.3359, 0.4280, 0.1723], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3155, 4.6935, 4.1320, 4.1959]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1381], requires_grad=True)\n","epoch 137, loss 0.7556463479995728\n","epoch 138, loss 0.7459319233894348\n","epoch 139, loss 0.7364785075187683\n","epoch 140, loss 0.7272739410400391\n","epoch 141, loss 0.7183077931404114\n","Parameter containing:\n","tensor([[0.8402, 2.0037, 0.8688],\n","        [0.9206, 1.9198, 0.6916],\n","        [0.6354, 1.6755, 0.8883],\n","        [1.2364, 1.6734, 0.4085]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4969, 0.3251, 0.4228, 0.1601], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3284, 4.7061, 4.1451, 4.2077]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1498], requires_grad=True)\n","epoch 142, loss 0.709568202495575\n","epoch 143, loss 0.7010470032691956\n","epoch 144, loss 0.6927337646484375\n","epoch 145, loss 0.6846204400062561\n","epoch 146, loss 0.6766970753669739\n","Parameter containing:\n","tensor([[0.8351, 2.0086, 0.8850],\n","        [0.9189, 1.9262, 0.7046],\n","        [0.6264, 1.6768, 0.9043],\n","        [1.2385, 1.6820, 0.4195]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4870, 0.3151, 0.4184, 0.1485], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3400, 4.7173, 4.1569, 4.2182]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1607], requires_grad=True)\n","epoch 147, loss 0.6689582467079163\n","epoch 148, loss 0.6613945364952087\n","epoch 149, loss 0.653998851776123\n","epoch 150, loss 0.6467655301094055\n","epoch 151, loss 0.6396876573562622\n","Parameter containing:\n","tensor([[0.8298, 2.0129, 0.9006],\n","        [0.9170, 1.9319, 0.7172],\n","        [0.6170, 1.6773, 0.9200],\n","        [1.2402, 1.6901, 0.4300]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4779, 0.3058, 0.4146, 0.1375], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3503, 4.7274, 4.1676, 4.2275]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1710], requires_grad=True)\n","epoch 152, loss 0.6327593922615051\n","epoch 153, loss 0.6259738206863403\n","epoch 154, loss 0.619328498840332\n","epoch 155, loss 0.6128153800964355\n","epoch 156, loss 0.6064314842224121\n","Parameter containing:\n","tensor([[0.8241, 2.0166, 0.9158],\n","        [0.9148, 1.9372, 0.7292],\n","        [0.6072, 1.6773, 0.9353],\n","        [1.2417, 1.6976, 0.4400]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4696, 0.2970, 0.4114, 0.1271], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3597, 4.7364, 4.1774, 4.2359]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1806], requires_grad=True)\n","epoch 157, loss 0.6001699566841125\n","epoch 158, loss 0.5940288305282593\n","epoch 159, loss 0.5880025625228882\n","epoch 160, loss 0.58208829164505\n","epoch 161, loss 0.5762813687324524\n","Parameter containing:\n","tensor([[0.8183, 2.0198, 0.9304],\n","        [0.9125, 1.9421, 0.7409],\n","        [0.5970, 1.6767, 0.9503],\n","        [1.2431, 1.7048, 0.4495]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4619, 0.2887, 0.4088, 0.1173], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3682, 4.7446, 4.1863, 4.2434]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1898], requires_grad=True)\n","epoch 162, loss 0.5705791115760803\n","epoch 163, loss 0.5649772882461548\n","epoch 164, loss 0.559472918510437\n","epoch 165, loss 0.554063081741333\n","epoch 166, loss 0.5487448573112488\n","Parameter containing:\n","tensor([[0.8123, 2.0226, 0.9446],\n","        [0.9100, 1.9467, 0.7520],\n","        [0.5866, 1.6757, 0.9649],\n","        [1.2443, 1.7116, 0.4587]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4548, 0.2810, 0.4067, 0.1078], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3760, 4.7520, 4.1945, 4.2502]], requires_grad=True)\n","Parameter containing:\n","tensor([6.1984], requires_grad=True)\n","epoch 167, loss 0.5435156226158142\n","epoch 168, loss 0.5383729338645935\n","epoch 169, loss 0.5333139896392822\n","epoch 170, loss 0.5283361077308655\n","epoch 171, loss 0.5234375\n","Parameter containing:\n","tensor([[0.8061, 2.0251, 0.9583],\n","        [0.9075, 1.9510, 0.7628],\n","        [0.5759, 1.6744, 0.9792],\n","        [1.2454, 1.7181, 0.4674]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4484, 0.2737, 0.4051, 0.0989], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3832, 4.7588, 4.2021, 4.2563]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2066], requires_grad=True)\n","epoch 172, loss 0.5186166167259216\n","epoch 173, loss 0.5138702392578125\n","epoch 174, loss 0.5091965198516846\n","epoch 175, loss 0.5045946836471558\n","epoch 176, loss 0.5000613927841187\n","Parameter containing:\n","tensor([[0.7999, 2.0274, 0.9715],\n","        [0.9049, 1.9551, 0.7731],\n","        [0.5651, 1.6729, 0.9932],\n","        [1.2464, 1.7244, 0.4757]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4424, 0.2668, 0.4040, 0.0903], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3897, 4.7650, 4.2092, 4.2619]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2143], requires_grad=True)\n","epoch 177, loss 0.4955960810184479\n","epoch 178, loss 0.4911969304084778\n","epoch 179, loss 0.48686206340789795\n","epoch 180, loss 0.48259082436561584\n","epoch 181, loss 0.47837984561920166\n","Parameter containing:\n","tensor([[0.7936, 2.0295, 0.9843],\n","        [0.9023, 1.9591, 0.7831],\n","        [0.5542, 1.6712, 1.0069],\n","        [1.2473, 1.7306, 0.4836]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4370, 0.2604, 0.4033, 0.0821], requires_grad=True)\n","Parameter containing:\n","tensor([[5.3958, 4.7708, 4.2158, 4.2670]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2216], requires_grad=True)\n","epoch 182, loss 0.47422972321510315\n","epoch 183, loss 0.47013816237449646\n","epoch 184, loss 0.4661039710044861\n","epoch 185, loss 0.4621264934539795\n","epoch 186, loss 0.45820438861846924\n","Parameter containing:\n","tensor([[0.7874, 2.0315, 0.9967],\n","        [0.8998, 1.9629, 0.7926],\n","        [0.5431, 1.6694, 1.0202],\n","        [1.2483, 1.7366, 0.4912]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4319, 0.2543, 0.4031, 0.0743], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4015, 4.7761, 4.2220, 4.2717]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2285], requires_grad=True)\n","epoch 187, loss 0.45433521270751953\n","epoch 188, loss 0.45051994919776917\n","epoch 189, loss 0.4467558264732361\n","epoch 190, loss 0.4430422782897949\n","epoch 191, loss 0.4393790662288666\n","Parameter containing:\n","tensor([[0.7811, 2.0334, 1.0087],\n","        [0.8972, 1.9668, 0.8018],\n","        [0.5320, 1.6674, 1.0332],\n","        [1.2492, 1.7424, 0.4984]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4273, 0.2485, 0.4033, 0.0667], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4068, 4.7810, 4.2278, 4.2761]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2351], requires_grad=True)\n","epoch 192, loss 0.43576428294181824\n","epoch 193, loss 0.432197630405426\n","epoch 194, loss 0.4286780059337616\n","epoch 195, loss 0.4252040982246399\n","epoch 196, loss 0.4217758774757385\n","Parameter containing:\n","tensor([[0.7749, 2.0353, 1.0202],\n","        [0.8948, 1.9706, 0.8106],\n","        [0.5209, 1.6655, 1.0458],\n","        [1.2501, 1.7482, 0.5053]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4231, 0.2430, 0.4038, 0.0595], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4117, 4.7855, 4.2333, 4.2801]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2412], requires_grad=True)\n","epoch 197, loss 0.4183921813964844\n","epoch 198, loss 0.41505250334739685\n","epoch 199, loss 0.41175511479377747\n","epoch 200, loss 0.4085005223751068\n","epoch 201, loss 0.40528663992881775\n","Parameter containing:\n","tensor([[0.7688, 2.0371, 1.0314],\n","        [0.8924, 1.9744, 0.8190],\n","        [0.5098, 1.6636, 1.0581],\n","        [1.2510, 1.7540, 0.5118]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4192, 0.2378, 0.4047, 0.0526], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4164, 4.7898, 4.2385, 4.2839]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2470], requires_grad=True)\n","epoch 202, loss 0.40211421251296997\n","epoch 203, loss 0.3989814817905426\n","epoch 204, loss 0.3958882689476013\n","epoch 205, loss 0.39283376932144165\n","epoch 206, loss 0.3898172974586487\n","Parameter containing:\n","tensor([[0.7628, 2.0390, 1.0421],\n","        [0.8901, 1.9782, 0.8270],\n","        [0.4987, 1.6616, 1.0701],\n","        [1.2520, 1.7597, 0.5181]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4157, 0.2329, 0.4059, 0.0460], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4208, 4.7938, 4.2434, 4.2873]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2524], requires_grad=True)\n","epoch 207, loss 0.38683876395225525\n","epoch 208, loss 0.3838963210582733\n","epoch 209, loss 0.38099047541618347\n","epoch 210, loss 0.37812021374702454\n","epoch 211, loss 0.3752850294113159\n","Parameter containing:\n","tensor([[0.7568, 2.0410, 1.0525],\n","        [0.8879, 1.9820, 0.8348],\n","        [0.4877, 1.6598, 1.0817],\n","        [1.2530, 1.7654, 0.5240]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4125, 0.2282, 0.4075, 0.0396], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4249, 4.7976, 4.2481, 4.2906]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2574], requires_grad=True)\n","epoch 212, loss 0.37248414754867554\n","epoch 213, loss 0.36971744894981384\n","epoch 214, loss 0.3669840395450592\n","epoch 215, loss 0.3642832040786743\n","epoch 216, loss 0.3616148829460144\n","Parameter containing:\n","tensor([[0.7510, 2.0430, 1.0625],\n","        [0.8858, 1.9859, 0.8421],\n","        [0.4767, 1.6580, 1.0930],\n","        [1.2540, 1.7711, 0.5296]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4096, 0.2238, 0.4093, 0.0334], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4288, 4.8011, 4.2525, 4.2937]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2622], requires_grad=True)\n","epoch 217, loss 0.3589785695075989\n","epoch 218, loss 0.35637331008911133\n","epoch 219, loss 0.3537989556789398\n","epoch 220, loss 0.35125502943992615\n","epoch 221, loss 0.3487410545349121\n","Parameter containing:\n","tensor([[0.7453, 2.0451, 1.0721],\n","        [0.8838, 1.9899, 0.8492],\n","        [0.4658, 1.6564, 1.1040],\n","        [1.2551, 1.7767, 0.5349]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4069, 0.2196, 0.4114, 0.0275], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4325, 4.8045, 4.2567, 4.2966]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2665], requires_grad=True)\n","epoch 222, loss 0.3462565243244171\n","epoch 223, loss 0.34380072355270386\n","epoch 224, loss 0.3413735628128052\n","epoch 225, loss 0.33897387981414795\n","epoch 226, loss 0.3366018533706665\n","Parameter containing:\n","tensor([[0.7398, 2.0474, 1.0814],\n","        [0.8819, 1.9939, 0.8559],\n","        [0.4549, 1.6549, 1.1146],\n","        [1.2562, 1.7824, 0.5400]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4045, 0.2156, 0.4138, 0.0217], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4360, 4.8076, 4.2608, 4.2993]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2706], requires_grad=True)\n","epoch 227, loss 0.3342575132846832\n","epoch 228, loss 0.33193936944007874\n","epoch 229, loss 0.3296477198600769\n","epoch 230, loss 0.32738158106803894\n","epoch 231, loss 0.32514140009880066\n","Parameter containing:\n","tensor([[0.7343, 2.0497, 1.0903],\n","        [0.8801, 1.9981, 0.8624],\n","        [0.4442, 1.6534, 1.1250],\n","        [1.2574, 1.7880, 0.5448]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4023, 0.2118, 0.4164, 0.0162], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4393, 4.8106, 4.2646, 4.3018]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2743], requires_grad=True)\n","epoch 232, loss 0.32292595505714417\n","epoch 233, loss 0.3207351565361023\n","epoch 234, loss 0.3185689449310303\n","epoch 235, loss 0.31642594933509827\n","epoch 236, loss 0.3143070340156555\n","Parameter containing:\n","tensor([[0.7291, 2.0521, 1.0989],\n","        [0.8785, 2.0023, 0.8685],\n","        [0.4336, 1.6522, 1.1350],\n","        [1.2586, 1.7937, 0.5494]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4003, 0.2081, 0.4192, 0.0109], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4425, 4.8134, 4.2683, 4.3042]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2778], requires_grad=True)\n","epoch 237, loss 0.31221088767051697\n","epoch 238, loss 0.3101380467414856\n","epoch 239, loss 0.3080874979496002\n","epoch 240, loss 0.3060583472251892\n","epoch 241, loss 0.3040508031845093\n","Parameter containing:\n","tensor([[0.7240, 2.0547, 1.1072],\n","        [0.8770, 2.0065, 0.8744],\n","        [0.4231, 1.6511, 1.1448],\n","        [1.2599, 1.7994, 0.5537]], requires_grad=True)\n","Parameter containing:\n","tensor([0.3986, 0.2047, 0.4222, 0.0058], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4454, 4.8160, 4.2718, 4.3065]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2809], requires_grad=True)\n","epoch 242, loss 0.3020648658275604\n","epoch 243, loss 0.3001003563404083\n","epoch 244, loss 0.29815608263015747\n","epoch 245, loss 0.296232134103775\n","epoch 246, loss 0.2943284511566162\n","Parameter containing:\n","tensor([[0.7190, 2.0573, 1.1152],\n","        [0.8756, 2.0109, 0.8800],\n","        [0.4128, 1.6501, 1.1542],\n","        [1.2612, 1.8051, 0.5578]], requires_grad=True)\n","Parameter containing:\n","tensor([0.3970, 0.2013, 0.4254, 0.0008], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4483, 4.8185, 4.2751, 4.3086]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2838], requires_grad=True)\n","epoch 247, loss 0.2924436330795288\n","epoch 248, loss 0.2905789613723755\n","epoch 249, loss 0.2887327969074249\n","epoch 250, loss 0.2869059145450592\n","epoch 251, loss 0.2850973904132843\n","Parameter containing:\n","tensor([[0.7142, 2.0601, 1.1228],\n","        [0.8743, 2.0153, 0.8853],\n","        [0.4025, 1.6492, 1.1634],\n","        [1.2626, 1.8108, 0.5617]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3956,  0.1982,  0.4288, -0.0040], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4510, 4.8209, 4.2783, 4.3107]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2863], requires_grad=True)\n","epoch 252, loss 0.2833065986633301\n","epoch 253, loss 0.2815338671207428\n","epoch 254, loss 0.27977898716926575\n","epoch 255, loss 0.2780410051345825\n","epoch 256, loss 0.2763200104236603\n","Parameter containing:\n","tensor([[0.7095, 2.0630, 1.1302],\n","        [0.8732, 2.0198, 0.8904],\n","        [0.3925, 1.6486, 1.1723],\n","        [1.2640, 1.8165, 0.5654]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3943,  0.1952,  0.4323, -0.0086], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4535, 4.8232, 4.2814, 4.3126]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2886], requires_grad=True)\n","epoch 257, loss 0.27461567521095276\n","epoch 258, loss 0.2729282081127167\n","epoch 259, loss 0.27125686407089233\n","epoch 260, loss 0.26960110664367676\n","epoch 261, loss 0.2679610550403595\n","Parameter containing:\n","tensor([[0.7050, 2.0661, 1.1373],\n","        [0.8721, 2.0243, 0.8952],\n","        [0.3825, 1.6480, 1.1809],\n","        [1.2655, 1.8222, 0.5689]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3933,  0.1923,  0.4360, -0.0131], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4560, 4.8253, 4.2842, 4.3144]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2907], requires_grad=True)\n","epoch 262, loss 0.2663366198539734\n","epoch 263, loss 0.2647274136543274\n","epoch 264, loss 0.26313328742980957\n","epoch 265, loss 0.2615537643432617\n","epoch 266, loss 0.25998812913894653\n","Parameter containing:\n","tensor([[0.7007, 2.0692, 1.1441],\n","        [0.8712, 2.0290, 0.8999],\n","        [0.3727, 1.6476, 1.1893],\n","        [1.2670, 1.8279, 0.5722]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3923,  0.1895,  0.4398, -0.0175], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4583, 4.8273, 4.2870, 4.3161]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2925], requires_grad=True)\n","epoch 267, loss 0.2584371864795685\n","epoch 268, loss 0.2569003701210022\n","epoch 269, loss 0.2553772032260895\n","epoch 270, loss 0.25386741757392883\n","epoch 271, loss 0.2523711919784546\n","Parameter containing:\n","tensor([[0.6965, 2.0724, 1.1506],\n","        [0.8704, 2.0336, 0.9043],\n","        [0.3630, 1.6474, 1.1975],\n","        [1.2685, 1.8335, 0.5753]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3915,  0.1869,  0.4437, -0.0217], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4605, 4.8292, 4.2896, 4.3177]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2940], requires_grad=True)\n","epoch 272, loss 0.25088828802108765\n","epoch 273, loss 0.24941784143447876\n","epoch 274, loss 0.24796025454998016\n","epoch 275, loss 0.2465156465768814\n","epoch 276, loss 0.24508313834667206\n","Parameter containing:\n","tensor([[0.6924, 2.0757, 1.1569],\n","        [0.8697, 2.0384, 0.9084],\n","        [0.3535, 1.6473, 1.2054],\n","        [1.2701, 1.8392, 0.5782]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3908,  0.1843,  0.4477, -0.0258], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4625, 4.8310, 4.2921, 4.3192]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2954], requires_grad=True)\n","epoch 277, loss 0.24366270005702972\n","epoch 278, loss 0.24225397408008575\n","epoch 279, loss 0.24085746705532074\n","epoch 280, loss 0.2394721955060959\n","epoch 281, loss 0.2380989044904709\n","Parameter containing:\n","tensor([[0.6885, 2.0791, 1.1630],\n","        [0.8690, 2.0432, 0.9124],\n","        [0.3441, 1.6473, 1.2131],\n","        [1.2717, 1.8449, 0.5810]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3902,  0.1819,  0.4519, -0.0297], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4645, 4.8327, 4.2945, 4.3206]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2965], requires_grad=True)\n","epoch 282, loss 0.2367362529039383\n","epoch 283, loss 0.23538482189178467\n","epoch 284, loss 0.23404450714588165\n","epoch 285, loss 0.23271456360816956\n","epoch 286, loss 0.23139554262161255\n","Parameter containing:\n","tensor([[0.6847, 2.0826, 1.1688],\n","        [0.8685, 2.0480, 0.9162],\n","        [0.3349, 1.6474, 1.2205],\n","        [1.2733, 1.8505, 0.5837]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3898,  0.1796,  0.4561, -0.0335], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4663, 4.8342, 4.2968, 4.3220]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2974], requires_grad=True)\n","epoch 287, loss 0.2300865203142166\n","epoch 288, loss 0.22878849506378174\n","epoch 289, loss 0.22749973833560944\n","epoch 290, loss 0.22622190415859222\n","epoch 291, loss 0.22495299577713013\n","Parameter containing:\n","tensor([[0.6810, 2.0862, 1.1744],\n","        [0.8681, 2.0528, 0.9199],\n","        [0.3257, 1.6477, 1.2278],\n","        [1.2749, 1.8562, 0.5862]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3894,  0.1773,  0.4604, -0.0372], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4681, 4.8357, 4.2989, 4.3232]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2981], requires_grad=True)\n","epoch 292, loss 0.22369420528411865\n","epoch 293, loss 0.22244498133659363\n","epoch 294, loss 0.22120468318462372\n","epoch 295, loss 0.2199740707874298\n","epoch 296, loss 0.2187526673078537\n","Parameter containing:\n","tensor([[0.6775, 2.0899, 1.1798],\n","        [0.8677, 2.0577, 0.9233],\n","        [0.3168, 1.6480, 1.2348],\n","        [1.2766, 1.8618, 0.5885]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3891,  0.1752,  0.4647, -0.0408], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4698, 4.8371, 4.3010, 4.3244]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2986], requires_grad=True)\n","epoch 297, loss 0.2175402194261551\n","epoch 298, loss 0.21633632481098175\n","epoch 299, loss 0.2151411771774292\n","epoch 300, loss 0.21395501494407654\n","epoch 301, loss 0.21277748048305511\n","Parameter containing:\n","tensor([[0.6741, 2.0936, 1.1850],\n","        [0.8674, 2.0626, 0.9266],\n","        [0.3079, 1.6485, 1.2417],\n","        [1.2783, 1.8674, 0.5908]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3889,  0.1731,  0.4692, -0.0443], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4713, 4.8385, 4.3030, 4.3255]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2990], requires_grad=True)\n","epoch 302, loss 0.21160784363746643\n","epoch 303, loss 0.21044686436653137\n","epoch 304, loss 0.209293931722641\n","epoch 305, loss 0.2081495225429535\n","epoch 306, loss 0.2070123255252838\n","Parameter containing:\n","tensor([[0.6708, 2.0974, 1.1900],\n","        [0.8672, 2.0676, 0.9297],\n","        [0.2992, 1.6491, 1.2483],\n","        [1.2800, 1.8729, 0.5929]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3888,  0.1711,  0.4736, -0.0477], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4728, 4.8397, 4.3048, 4.3265]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2992], requires_grad=True)\n","epoch 307, loss 0.20588335394859314\n","epoch 308, loss 0.20476213097572327\n","epoch 309, loss 0.20364831387996674\n","epoch 310, loss 0.20254205167293549\n","epoch 311, loss 0.2014434039592743\n","Parameter containing:\n","tensor([[0.6676, 2.1012, 1.1948],\n","        [0.8671, 2.0725, 0.9327],\n","        [0.2907, 1.6498, 1.2548],\n","        [1.2817, 1.8784, 0.5949]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3888,  0.1692,  0.4782, -0.0510], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4742, 4.8409, 4.3066, 4.3275]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2993], requires_grad=True)\n","epoch 312, loss 0.2003520280122757\n","epoch 313, loss 0.19926795363426208\n","epoch 314, loss 0.19819147884845734\n","epoch 315, loss 0.197121724486351\n","epoch 316, loss 0.1960587501525879\n","Parameter containing:\n","tensor([[0.6646, 2.1051, 1.1994],\n","        [0.8670, 2.0775, 0.9356],\n","        [0.2822, 1.6505, 1.2612],\n","        [1.2835, 1.8839, 0.5968]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3888,  0.1674,  0.4827, -0.0542], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4755, 4.8419, 4.3082, 4.3284]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2992], requires_grad=True)\n","epoch 317, loss 0.1950027346611023\n","epoch 318, loss 0.19395385682582855\n","epoch 319, loss 0.19291150569915771\n","epoch 320, loss 0.1918759047985077\n","epoch 321, loss 0.19084659218788147\n","Parameter containing:\n","tensor([[0.6616, 2.1090, 1.2039],\n","        [0.8670, 2.0824, 0.9383],\n","        [0.2739, 1.6514, 1.2673],\n","        [1.2852, 1.8894, 0.5986]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3889,  0.1656,  0.4874, -0.0573], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4767, 4.8430, 4.3098, 4.3293]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2989], requires_grad=True)\n","epoch 322, loss 0.1898244321346283\n","epoch 323, loss 0.18880832195281982\n","epoch 324, loss 0.18779882788658142\n","epoch 325, loss 0.18679538369178772\n","epoch 326, loss 0.1857980638742447\n","Parameter containing:\n","tensor([[0.6588, 2.1130, 1.2082],\n","        [0.8671, 2.0874, 0.9409],\n","        [0.2657, 1.6523, 1.2733],\n","        [1.2869, 1.8948, 0.6003]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3890,  0.1639,  0.4920, -0.0603], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4779, 4.8439, 4.3113, 4.3301]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2985], requires_grad=True)\n","epoch 327, loss 0.18480703234672546\n","epoch 328, loss 0.18382218480110168\n","epoch 329, loss 0.1828434318304062\n","epoch 330, loss 0.18187028169631958\n","epoch 331, loss 0.18090315163135529\n","Parameter containing:\n","tensor([[0.6560, 2.1170, 1.2123],\n","        [0.8672, 2.0923, 0.9433],\n","        [0.2577, 1.6533, 1.2792],\n","        [1.2887, 1.9001, 0.6019]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3892,  0.1622,  0.4966, -0.0633], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4790, 4.8448, 4.3128, 4.3308]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2981], requires_grad=True)\n","epoch 332, loss 0.17994217574596405\n","epoch 333, loss 0.17898665368556976\n","epoch 334, loss 0.1780366599559784\n","epoch 335, loss 0.17709249258041382\n","epoch 336, loss 0.1761539727449417\n","Parameter containing:\n","tensor([[0.6534, 2.1210, 1.2163],\n","        [0.8673, 2.0973, 0.9457],\n","        [0.2498, 1.6544, 1.2848],\n","        [1.2905, 1.9055, 0.6034]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3895,  0.1607,  0.5013, -0.0661], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4800, 4.8456, 4.3141, 4.3315]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2975], requires_grad=True)\n","epoch 337, loss 0.17522111535072327\n","epoch 338, loss 0.1742933839559555\n","epoch 339, loss 0.17337143421173096\n","epoch 340, loss 0.17245475947856903\n","epoch 341, loss 0.17154353857040405\n","Parameter containing:\n","tensor([[0.6508, 2.1250, 1.2202],\n","        [0.8676, 2.1022, 0.9479],\n","        [0.2419, 1.6555, 1.2904],\n","        [1.2922, 1.9107, 0.6048]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3898,  0.1591,  0.5060, -0.0689], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4809, 4.8464, 4.3154, 4.3321]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2967], requires_grad=True)\n","epoch 342, loss 0.1706376075744629\n","epoch 343, loss 0.16973654925823212\n","epoch 344, loss 0.16884073615074158\n","epoch 345, loss 0.16795048117637634\n","epoch 346, loss 0.16706497967243195\n","Parameter containing:\n","tensor([[0.6484, 2.1291, 1.2239],\n","        [0.8678, 2.1071, 0.9500],\n","        [0.2342, 1.6567, 1.2958],\n","        [1.2940, 1.9160, 0.6061]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3901,  0.1576,  0.5107, -0.0716], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4818, 4.8471, 4.3166, 4.3327]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2959], requires_grad=True)\n","epoch 347, loss 0.16618426144123077\n","epoch 348, loss 0.16530880331993103\n","epoch 349, loss 0.16443844139575958\n","epoch 350, loss 0.16357280313968658\n","epoch 351, loss 0.16271208226680756\n","Parameter containing:\n","tensor([[0.6460, 2.1331, 1.2275],\n","        [0.8681, 2.1120, 0.9521],\n","        [0.2267, 1.6580, 1.3011],\n","        [1.2958, 1.9211, 0.6074]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3905,  0.1562,  0.5154, -0.0742], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4827, 4.8478, 4.3178, 4.3333]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2950], requires_grad=True)\n","epoch 352, loss 0.16185584664344788\n","epoch 353, loss 0.16100497543811798\n","epoch 354, loss 0.16015823185443878\n","epoch 355, loss 0.15931697189807892\n","epoch 356, loss 0.1584799587726593\n","Parameter containing:\n","tensor([[0.6437, 2.1372, 1.2310],\n","        [0.8685, 2.1169, 0.9540],\n","        [0.2192, 1.6593, 1.3063],\n","        [1.2975, 1.9263, 0.6086]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3909,  0.1548,  0.5201, -0.0768], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4834, 4.8484, 4.3189, 4.3338]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2940], requires_grad=True)\n","epoch 357, loss 0.15764714777469635\n","epoch 358, loss 0.156819686293602\n","epoch 359, loss 0.15599612891674042\n","epoch 360, loss 0.15517762303352356\n","epoch 361, loss 0.1543629914522171\n","Parameter containing:\n","tensor([[0.6414, 2.1413, 1.2343],\n","        [0.8689, 2.1217, 0.9559],\n","        [0.2118, 1.6606, 1.3113],\n","        [1.2993, 1.9313, 0.6098]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3913,  0.1535,  0.5248, -0.0793], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4842, 4.8490, 4.3199, 4.3343]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2930], requires_grad=True)\n","epoch 362, loss 0.1535530388355255\n","epoch 363, loss 0.15274816751480103\n","epoch 364, loss 0.15194718539714813\n","epoch 365, loss 0.15115022659301758\n","epoch 366, loss 0.15035775303840637\n","Parameter containing:\n","tensor([[0.6393, 2.1454, 1.2375],\n","        [0.8693, 2.1265, 0.9576],\n","        [0.2046, 1.6620, 1.3163],\n","        [1.3010, 1.9363, 0.6108]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3918,  0.1522,  0.5295, -0.0817], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4848, 4.8495, 4.3209, 4.3347]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2918], requires_grad=True)\n","epoch 367, loss 0.14956948161125183\n","epoch 368, loss 0.14878574013710022\n","epoch 369, loss 0.14800605177879333\n","epoch 370, loss 0.14723047614097595\n","epoch 371, loss 0.14645928144454956\n","Parameter containing:\n","tensor([[0.6372, 2.1495, 1.2407],\n","        [0.8697, 2.1313, 0.9593],\n","        [0.1975, 1.6635, 1.3211],\n","        [1.3028, 1.9413, 0.6119]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3923,  0.1509,  0.5342, -0.0840], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4855, 4.8500, 4.3219, 4.3351]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2906], requires_grad=True)\n","epoch 372, loss 0.14569181203842163\n","epoch 373, loss 0.14492881298065186\n","epoch 374, loss 0.14416955411434174\n","epoch 375, loss 0.14341500401496887\n","epoch 376, loss 0.14266365766525269\n","Parameter containing:\n","tensor([[0.6352, 2.1536, 1.2437],\n","        [0.8702, 2.1361, 0.9609],\n","        [0.1904, 1.6650, 1.3258],\n","        [1.3045, 1.9462, 0.6128]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3928,  0.1497,  0.5388, -0.0863], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4861, 4.8504, 4.3227, 4.3354]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2894], requires_grad=True)\n","epoch 377, loss 0.141916885972023\n","epoch 378, loss 0.14117325842380524\n","epoch 379, loss 0.14043472707271576\n","epoch 380, loss 0.1396988481283188\n","epoch 381, loss 0.13896796107292175\n","Parameter containing:\n","tensor([[0.6333, 2.1576, 1.2466],\n","        [0.8707, 2.1408, 0.9625],\n","        [0.1835, 1.6665, 1.3304],\n","        [1.3062, 1.9511, 0.6137]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3933,  0.1485,  0.5435, -0.0886], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4866, 4.8508, 4.3236, 4.3358]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2880], requires_grad=True)\n","epoch 382, loss 0.1382407248020172\n","epoch 383, loss 0.13751664757728577\n","epoch 384, loss 0.13679686188697815\n","epoch 385, loss 0.13608086109161377\n","epoch 386, loss 0.13536851108074188\n","Parameter containing:\n","tensor([[0.6314, 2.1617, 1.2495],\n","        [0.8712, 2.1455, 0.9639],\n","        [0.1767, 1.6680, 1.3349],\n","        [1.3079, 1.9559, 0.6146]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3939,  0.1474,  0.5481, -0.0908], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4871, 4.8512, 4.3244, 4.3361]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2866], requires_grad=True)\n","epoch 387, loss 0.13466008007526398\n","epoch 388, loss 0.13395515084266663\n","epoch 389, loss 0.13325382769107819\n","epoch 390, loss 0.1325569599866867\n","epoch 391, loss 0.13186296820640564\n","Parameter containing:\n","tensor([[0.6296, 2.1657, 1.2522],\n","        [0.8718, 2.1502, 0.9653],\n","        [0.1700, 1.6696, 1.3393],\n","        [1.3096, 1.9606, 0.6154]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3945,  0.1462,  0.5527, -0.0929], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4876, 4.8516, 4.3252, 4.3363]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2852], requires_grad=True)\n","epoch 392, loss 0.13117262721061707\n","epoch 393, loss 0.1304861605167389\n","epoch 394, loss 0.12980352342128754\n","epoch 395, loss 0.1291234791278839\n","epoch 396, loss 0.12844786047935486\n","Parameter containing:\n","tensor([[0.6278, 2.1698, 1.2549],\n","        [0.8724, 2.1548, 0.9667],\n","        [0.1634, 1.6712, 1.3437],\n","        [1.3113, 1.9653, 0.6162]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3951,  0.1452,  0.5573, -0.0950], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4880, 4.8519, 4.3259, 4.3366]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2837], requires_grad=True)\n","epoch 397, loss 0.1277751326560974\n","epoch 398, loss 0.1271064430475235\n","epoch 399, loss 0.12644076347351074\n","epoch 400, loss 0.12577912211418152\n","epoch 401, loss 0.125120609998703\n","Parameter containing:\n","tensor([[0.6261, 2.1738, 1.2574],\n","        [0.8730, 2.1594, 0.9680],\n","        [0.1569, 1.6728, 1.3479],\n","        [1.3130, 1.9699, 0.6169]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3957,  0.1441,  0.5619, -0.0970], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4884, 4.8521, 4.3266, 4.3368]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2822], requires_grad=True)\n","epoch 402, loss 0.12446528673171997\n","epoch 403, loss 0.123813696205616\n","epoch 404, loss 0.12316544353961945\n","epoch 405, loss 0.1225203201174736\n","epoch 406, loss 0.12187878042459488\n","Parameter containing:\n","tensor([[0.6245, 2.1778, 1.2599],\n","        [0.8736, 2.1639, 0.9692],\n","        [0.1505, 1.6745, 1.3520],\n","        [1.3147, 1.9745, 0.6176]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3963,  0.1431,  0.5664, -0.0990], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4888, 4.8524, 4.3272, 4.3370]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2806], requires_grad=True)\n","epoch 407, loss 0.12124056369066238\n","epoch 408, loss 0.12060558050870895\n","epoch 409, loss 0.11997398734092712\n","epoch 410, loss 0.11934560537338257\n","epoch 411, loss 0.11872062087059021\n","Parameter containing:\n","tensor([[0.6229, 2.1817, 1.2623],\n","        [0.8742, 2.1684, 0.9703],\n","        [0.1442, 1.6762, 1.3561],\n","        [1.3163, 1.9790, 0.6182]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3970,  0.1421,  0.5709, -0.1009], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4891, 4.8526, 4.3279, 4.3372]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2790], requires_grad=True)\n","epoch 412, loss 0.1180989146232605\n","epoch 413, loss 0.11748013645410538\n","epoch 414, loss 0.1168644055724144\n","epoch 415, loss 0.11625202745199203\n","epoch 416, loss 0.1156429573893547\n","Parameter containing:\n","tensor([[0.6213, 2.1857, 1.2647],\n","        [0.8749, 2.1729, 0.9715],\n","        [0.1380, 1.6778, 1.3600],\n","        [1.3180, 1.9835, 0.6188]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3976,  0.1411,  0.5754, -0.1028], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4894, 4.8528, 4.3284, 4.3374]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2774], requires_grad=True)\n","epoch 417, loss 0.11503728479146957\n","epoch 418, loss 0.11443409323692322\n","epoch 419, loss 0.11383410543203354\n","epoch 420, loss 0.11323776841163635\n","epoch 421, loss 0.11264447122812271\n","Parameter containing:\n","tensor([[0.6199, 2.1896, 1.2670],\n","        [0.8756, 2.1773, 0.9725],\n","        [0.1318, 1.6795, 1.3639],\n","        [1.3196, 1.9879, 0.6194]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3983,  0.1402,  0.5798, -0.1046], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4897, 4.8530, 4.3290, 4.3375]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2757], requires_grad=True)\n","epoch 422, loss 0.11205363273620605\n","epoch 423, loss 0.11146645992994308\n","epoch 424, loss 0.11088204383850098\n","epoch 425, loss 0.1103009581565857\n","epoch 426, loss 0.10972239822149277\n","Parameter containing:\n","tensor([[0.6184, 2.1935, 1.2692],\n","        [0.8762, 2.1817, 0.9736],\n","        [0.1258, 1.6813, 1.3677],\n","        [1.3212, 1.9922, 0.6199]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3989,  0.1393,  0.5843, -0.1064], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4900, 4.8532, 4.3295, 4.3376]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2740], requires_grad=True)\n","epoch 427, loss 0.10914725065231323\n","epoch 428, loss 0.10857506096363068\n","epoch 429, loss 0.1080055683851242\n","epoch 430, loss 0.10743924230337143\n","epoch 431, loss 0.10687566548585892\n","Parameter containing:\n","tensor([[0.6170, 2.1974, 1.2713],\n","        [0.8769, 2.1860, 0.9745],\n","        [0.1199, 1.6830, 1.3715],\n","        [1.3228, 1.9965, 0.6204]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.3996,  0.1384,  0.5886, -0.1081], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4902, 4.8533, 4.3301, 4.3377]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2723], requires_grad=True)\n","epoch 432, loss 0.10631491988897324\n","epoch 433, loss 0.10575731098651886\n","epoch 434, loss 0.10520247370004654\n","epoch 435, loss 0.10465101152658463\n","epoch 436, loss 0.10410208255052567\n","Parameter containing:\n","tensor([[0.6157, 2.2012, 1.2734],\n","        [0.8776, 2.1903, 0.9755],\n","        [0.1140, 1.6847, 1.3751],\n","        [1.3244, 2.0007, 0.6209]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4003,  0.1376,  0.5930, -0.1099], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4904, 4.8535, 4.3305, 4.3378]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2706], requires_grad=True)\n","epoch 437, loss 0.10355554521083832\n","epoch 438, loss 0.1030125692486763\n","epoch 439, loss 0.1024717390537262\n","epoch 440, loss 0.10193413496017456\n","epoch 441, loss 0.10139930993318558\n","Parameter containing:\n","tensor([[0.6144, 2.2050, 1.2754],\n","        [0.8784, 2.1945, 0.9764],\n","        [0.1083, 1.6865, 1.3787],\n","        [1.3260, 2.0049, 0.6213]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4009,  0.1367,  0.5973, -0.1115], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4906, 4.8536, 4.3310, 4.3379]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2689], requires_grad=True)\n","epoch 442, loss 0.10086693614721298\n","epoch 443, loss 0.10033740103244781\n","epoch 444, loss 0.09981096535921097\n","epoch 445, loss 0.09928717464208603\n","epoch 446, loss 0.09876615554094315\n","Parameter containing:\n","tensor([[0.6131, 2.2088, 1.2773],\n","        [0.8791, 2.1987, 0.9772],\n","        [0.1026, 1.6883, 1.3822],\n","        [1.3276, 2.0090, 0.6218]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4016,  0.1359,  0.6016, -0.1131], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4908, 4.8537, 4.3314, 4.3379]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2671], requires_grad=True)\n","epoch 447, loss 0.09824718534946442\n","epoch 448, loss 0.09773173928260803\n","epoch 449, loss 0.09721856564283371\n","epoch 450, loss 0.0967085212469101\n","epoch 451, loss 0.0962003767490387\n","Parameter containing:\n","tensor([[0.6119, 2.2126, 1.2792],\n","        [0.8798, 2.2028, 0.9780],\n","        [0.0970, 1.6900, 1.3857],\n","        [1.3291, 2.0131, 0.6222]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4023,  0.1351,  0.6058, -0.1147], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4909, 4.8537, 4.3319, 4.3380]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2653], requires_grad=True)\n","epoch 452, loss 0.09569524228572845\n","epoch 453, loss 0.09519266337156296\n","epoch 454, loss 0.0946931466460228\n","epoch 455, loss 0.094195656478405\n","epoch 456, loss 0.09370119124650955\n","Parameter containing:\n","tensor([[0.6107, 2.2163, 1.2811],\n","        [0.8805, 2.2069, 0.9788],\n","        [0.0915, 1.6918, 1.3891],\n","        [1.3306, 2.0171, 0.6225]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4030,  0.1343,  0.6100, -0.1163], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4911, 4.8538, 4.3323, 4.3380]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2635], requires_grad=True)\n","epoch 457, loss 0.0932091623544693\n","epoch 458, loss 0.09271970391273499\n","epoch 459, loss 0.0922325998544693\n","epoch 460, loss 0.09174849092960358\n","epoch 461, loss 0.09126665443181992\n","Parameter containing:\n","tensor([[0.6095, 2.2200, 1.2828],\n","        [0.8813, 2.2110, 0.9795],\n","        [0.0861, 1.6936, 1.3924],\n","        [1.3321, 2.0211, 0.6229]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4037,  0.1336,  0.6142, -0.1178], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4912, 4.8538, 4.3327, 4.3381]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2617], requires_grad=True)\n","epoch 462, loss 0.09078729897737503\n","epoch 463, loss 0.09031043946743011\n","epoch 464, loss 0.0898360162973404\n","epoch 465, loss 0.08936423808336258\n","epoch 466, loss 0.08889483660459518\n","Parameter containing:\n","tensor([[0.6084, 2.2237, 1.2846],\n","        [0.8820, 2.2150, 0.9803],\n","        [0.0808, 1.6954, 1.3956],\n","        [1.3336, 2.0250, 0.6232]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4044,  0.1329,  0.6183, -0.1193], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4913, 4.8539, 4.3330, 4.3381]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2599], requires_grad=True)\n","epoch 467, loss 0.08842796087265015\n","epoch 468, loss 0.08796345442533493\n","epoch 469, loss 0.08750126510858536\n","epoch 470, loss 0.08704161643981934\n","epoch 471, loss 0.0865841880440712\n","Parameter containing:\n","tensor([[0.6073, 2.2273, 1.2863],\n","        [0.8828, 2.2189, 0.9809],\n","        [0.0755, 1.6972, 1.3988],\n","        [1.3351, 2.0288, 0.6235]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4051,  0.1321,  0.6224, -0.1207], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4914, 4.8539, 4.3334, 4.3381]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2581], requires_grad=True)\n","epoch 472, loss 0.08612988144159317\n","epoch 473, loss 0.08567705750465393\n","epoch 474, loss 0.08522729575634003\n","epoch 475, loss 0.08477937430143356\n","epoch 476, loss 0.08433424681425095\n","Parameter containing:\n","tensor([[0.6062, 2.2309, 1.2879],\n","        [0.8835, 2.2229, 0.9816],\n","        [0.0704, 1.6989, 1.4020],\n","        [1.3366, 2.0326, 0.6238]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4058,  0.1314,  0.6264, -0.1222], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4914, 4.8539, 4.3337, 4.3381]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2563], requires_grad=True)\n","epoch 477, loss 0.08389125019311905\n","epoch 478, loss 0.08345063030719757\n","epoch 479, loss 0.08301226794719696\n","epoch 480, loss 0.08257613331079483\n","epoch 481, loss 0.0821421667933464\n","Parameter containing:\n","tensor([[0.6052, 2.2344, 1.2895],\n","        [0.8843, 2.2267, 0.9822],\n","        [0.0653, 1.7007, 1.4051],\n","        [1.3380, 2.0363, 0.6241]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4065,  0.1308,  0.6304, -0.1235], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4915, 4.8539, 4.3340, 4.3381]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2545], requires_grad=True)\n","epoch 482, loss 0.081711046397686\n","epoch 483, loss 0.08128170669078827\n","epoch 484, loss 0.08085467666387558\n","epoch 485, loss 0.08043049275875092\n","epoch 486, loss 0.08000756800174713\n","Parameter containing:\n","tensor([[0.6042, 2.2379, 1.2911],\n","        [0.8850, 2.2305, 0.9828],\n","        [0.0603, 1.7025, 1.4081],\n","        [1.3394, 2.0400, 0.6243]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4072,  0.1301,  0.6344, -0.1249], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4915, 4.8539, 4.3343, 4.3380]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2527], requires_grad=True)\n","epoch 487, loss 0.07958756387233734\n","epoch 488, loss 0.0791698545217514\n","epoch 489, loss 0.07875383645296097\n","epoch 490, loss 0.07834018021821976\n","epoch 491, loss 0.07792884111404419\n","Parameter containing:\n","tensor([[0.6033, 2.2414, 1.2926],\n","        [0.8858, 2.2343, 0.9834],\n","        [0.0553, 1.7043, 1.4111],\n","        [1.3408, 2.0437, 0.6246]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4079,  0.1295,  0.6383, -0.1262], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4916, 4.8539, 4.3346, 4.3380]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2508], requires_grad=True)\n","epoch 492, loss 0.07751971483230591\n","epoch 493, loss 0.07711247354745865\n","epoch 494, loss 0.07670751214027405\n","epoch 495, loss 0.07630479335784912\n","epoch 496, loss 0.07590404152870178\n","Parameter containing:\n","tensor([[0.6023, 2.2449, 1.2941],\n","        [0.8865, 2.2380, 0.9839],\n","        [0.0505, 1.7061, 1.4140],\n","        [1.3422, 2.0472, 0.6248]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.4086,  0.1288,  0.6422, -0.1275], requires_grad=True)\n","Parameter containing:\n","tensor([[5.4916, 4.8538, 4.3349, 4.3380]], requires_grad=True)\n","Parameter containing:\n","tensor([6.2490], requires_grad=True)\n","epoch 497, loss 0.07550565153360367\n","epoch 498, loss 0.07510931044816971\n","epoch 499, loss 0.07471498101949692\n","epoch 500, loss 0.07432276010513306\n"],"name":"stdout"}]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-15T15:21:26.445260Z","start_time":"2018-10-15T15:21:26.282006Z"},"id":"hu6RrNJsmQE6","colab_type":"code","outputId":"f6e6bc72-11a6-4a2b-e50c-76e16bb71c73","colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"status":"ok","timestamp":1548229750508,"user_tz":-330,"elapsed":1071,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["#Plotting Loss vs Epochs\n","fig,ax = plt.subplots(1)\n","plt.title('Loss vs Epochs')\n","ax.plot(losses)\n","#ax.set_xticklabels([])\n","#ax.set_yticklabels([])\n","ax.set_ylabel('Loss')\n","ax.set_xlabel('Epoch')\n","plt.savefig('Loss_vs_Epoch.png')"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfQBbAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"WyMnFF4DXWHb","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"Scz_UtI5XWmI","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x1k_ksslXakG","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qrRpg5xAXegY","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"No\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HrjdzK1eXhEe","colab_type":"code","cellView":"form","outputId":"fd963da9-0a88-446c-d1d2-39a5c052a670","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Your submission is successful. Ref: 14869\n"],"name":"stdout"}]},{"metadata":{"id":"1uRpzh_dXoHn","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}