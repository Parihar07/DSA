{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pNDAGCxqiu1a"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n"]},{"cell_type":"markdown","metadata":{"id":"sbXsFYquiu1d"},"source":["The objective of this experiment is to understand MLP and Back Propagation."]},{"cell_type":"markdown","metadata":{"id":"F9erKg4niu1e"},"source":["In this experiment we will be using MNIST database. The MNIST database is a dataset of handwritten digits. It has 60,000 training samples, and 10,000 test samples. Each image is represented by 28 x 28 pixels, each containing a value 0 - 255 with its gray scale value.\n","\n","It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n","\n","It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."]},{"cell_type":"markdown","metadata":{"id":"wx-g0e1Giu1f"},"source":["#### Training a MLP"]},{"cell_type":"markdown","metadata":{"id":"IiVNpulviu1g"},"source":["\n","\n","Here, we will build a 3-layer neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data. Similarly, the number of nodes in the output layer is determined by the number of classes we have.\n"]},{"cell_type":"markdown","metadata":{"id":"mLxDcpqeiu1h"},"source":["#### How our network makes predictions\n"]},{"cell_type":"markdown","metadata":{"id":"trbEjRajiu1i"},"source":["Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the $N$-dimensional input to our network then we calculate our prediction $\\hat{y}$ (of lets say dimension $C$) as follows:\n","\n","$$\n","\\begin{aligned}\n","z_1 & = xW_1 + b_1 \\\\\n","a_1 & = \\tanh(z_1) \\\\\n","z_2 & = a_1W_2 + b_2 \\\\\n","a_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n","\\end{aligned}\n","$$\n","\n","$z_i$ is the weighted sum of inputs of layer $i$ (bias included) and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices. If we use 100 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{N\\times100}$, $b_1 \\in \\mathbb{R}^{100}$, $W_2 \\in \\mathbb{R}^{100\\times C}$, $b_2 \\in \\mathbb{R}^{C}$. Now you see why we have more parameters if we increase the size of the hidden layer.\n"]},{"cell_type":"markdown","metadata":{"id":"hMJW49Ywiu1j"},"source":["#### Learning the Parameters/Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"Mw-Tg2UYiu1l"},"source":["Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the cross-entropy loss. If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n","\n","$$\n","\\begin{aligned}\n","L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n","\\end{aligned}\n","$$\n","\n","The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. \n","\n","Remember that our goal is to find the parameters that minimize our loss function. We can use gradient descent to find its minimum. Here, we implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice but we are not going into that in this experiment.\n","\n","As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *back propagation algorithm*, which is a way to efficiently calculate the gradients starting from the output.\n","\n","Applying the back propagation formula using chain rule we find the following:\n","\n","$$\n","\\begin{aligned}\n","& \\delta_3 = \\frac{\\partial{L}}{\\partial{z_2}} = \\frac{\\partial{L}}{\\partial{a_2}}\\times\\frac{\\partial{a_2}}{\\partial{z_2}} = -(y - \\hat{y})\\\\\n","\\end{aligned}\n","$$\n","where $a_2$ is $\\hat{y}$\n","$$\n","\\begin{aligned}\n","& \\frac{\\partial{L}}{\\partial{W_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n","& \\frac{\\partial{L}}{\\partial{b_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{b_2}} = \\delta_3\\\\\n","& \\delta_2 = \\frac{\\partial{L}}{\\partial{z_1}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{a_1}}\\times\\frac{\\partial{a_1}}{\\partial{z_1}} = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n","& \\frac{\\partial{L}}{\\partial{W_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{W_1}} = x^T \\delta_2\\\\\n","& \\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{b_1}} = \\delta_2 \\\\\n","\\end{aligned}\n","$$\n","\n","$\\delta_3 = $ derivative of cross-entropy loss with Softmax as Activation [We will not go into its derivation]\n"]},{"cell_type":"markdown","metadata":{"id":"LRfTjZhvaa1s"},"source":["### Keywords"]},{"cell_type":"markdown","metadata":{"id":"9E9E7QK2aenH"},"source":["MLP\n","\n","BackPropagation\n","\n","Chain Rule\n","\n","Softmax\n","\n","Activation Function\n","\n","Solvers\n","\n","Gradient descent"]},{"cell_type":"code","metadata":{"id":"EXHXczB_gy-4","cellView":"form","executionInfo":{"status":"ok","timestamp":1544864576827,"user_tz":-330,"elapsed":1034,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"outputId":"79505210-4367-4469-d7db-cb937c8fd757","colab":{"base_uri":"https://localhost:8080/","height":260}},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_blr_b6/cfus/week_6/module_2_week_6_experment_1.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_blr_b6/cfus/week_6/module_2_week_6_experment_1.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"D45Og15HkFdC"},"source":["### Setup Steps"]},{"cell_type":"code","metadata":{"id":"HgrjC9OQkHNl"},"source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P19A06E_test\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJK_oO17kJKz"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"981234567\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF0fDzYlkLi5","cellView":"form","executionInfo":{"status":"ok","timestamp":1544072298620,"user_tz":-330,"elapsed":1307,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"outputId":"22290b71-34b7-4035-808e-607faca273a7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M2W6_SAT_EXP_1\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx pip3 install torch\")\n","   \n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful. Ref:\", submission_id)\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jQzrNGJjiu1m"},"source":["We will perform the experiment in 3 steps :"]},{"cell_type":"markdown","metadata":{"id":"xPr3RBukiu1n"},"source":["#### 1. Loading the dataset"]},{"cell_type":"code","metadata":{"id":"itdGOLOgiu1o"},"source":["# Importing Required Packages\n","import numpy as np\n","from scipy import ndimage \n","from matplotlib import pyplot as plt\n","from sklearn import manifold, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMKiT9zpiu1u","executionInfo":{"status":"ok","timestamp":1544072305089,"user_tz":-330,"elapsed":1309,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"outputId":"f1309253-4565-40b0-f845-4c3bcdbb9725","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Loading the data from sklearn datasets package\n","\n","#Load MNIST dataset \n","digits = datasets.load_digits(n_class=10)\n","# Create our X and y data\n","X = digits.data\n","Y = digits.target\n","print(X.shape, Y.shape)\n","num_examples = X.shape[0]      ## training set size\n","nn_input_dim = X.shape[1]      ## input layer dimensionality\n","print(nn_input_dim)\n","nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n","print(nn_output_dim)\n","\n","params = {\n","    \"lr\":1e-5,        ## learning_rate\n","    \"max_iter\":1000,\n","    \"h_dimn\":40,     ## hidden_layer_size\n","}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1797, 64) (1797,)\n","64\n","10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T9TiAV9diu11"},"source":["#### 2. Writing helper functions for the MLP"]},{"cell_type":"markdown","metadata":{"id":"vF49f_Zxghd8"},"source":["Let us define the skeleton of the model which is a 3 layer neural network with one input layer, one hidden layer and one output layer."]},{"cell_type":"code","metadata":{"id":"k0FKcj0vbFPN"},"source":["def build_model():\n","    hdim = params[\"h_dimn\"]\n","    # Initialize the parameters to random values.\n","    np.random.seed(0)\n","    W1 = np.random.randn(nn_input_dim, hdim) / np.sqrt(nn_input_dim)\n","    b1 = np.zeros((1, hdim))\n","    W2 = np.random.randn(hdim, nn_output_dim) / np.sqrt(hdim)\n","    b2 = np.zeros((1, nn_output_dim))\n","\n","    # This is what we return at the end\n","    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QPM03Pbb3og"},"source":["The softmax function is often used in the final layer of a neural network-based classifier. The main advantage of using Softmax is that it outputs probability. The range of the probability will be from 0 to 1, and the sum of all the probabilities in the output layer will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class is the one with the highest probability.\n","\n","Now let us define a function to calculate the softmax value:"]},{"cell_type":"code","metadata":{"id":"XTDeMu7GbIHs"},"source":["def softmax(x):\n","    exp_scores = np.exp(x)\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","    return probs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeJD44HPh4Uy"},"source":["Let us define a function for forward propagation."]},{"cell_type":"code","metadata":{"id":"e3LTvG_bbLPx"},"source":["def feedforward(model, x):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    z1 = x.dot(W1) + b1\n","    a1 = np.tanh(z1)\n","    z2 = a1.dot(W2) + b2\n","    probs = softmax(z2)\n","    return a1, probs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOyUPft5iBOw"},"source":["Let us define a function for backpropagation"]},{"cell_type":"code","metadata":{"id":"FkhZf5SjbN8R"},"source":["def backpropagation(model, x, y, a1, probs):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    \n","    delta3 = probs\n","    delta3[range(y.shape[0]), y] -= 1\n","    dW2 = (a1.T).dot(delta3)\n","    db2 = np.sum(delta3, axis=0, keepdims=True)\n","    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n","    dW1 = np.dot(x.T, delta2)\n","    db1 = np.sum(delta2, axis=0)\n","    return dW2, db2, dW1, db1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQny-gqTeGGl"},"source":["Now let us write a function to calculate loss."]},{"cell_type":"code","metadata":{"id":"ap2eV_qYbQ0G"},"source":["def calculate_loss(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","   \n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    \n","    # Calculating the cross entropy loss\n","    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n","    data_loss = np.sum(corect_logprobs)\n","    \n","    return 1./y.shape[0] * data_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUx9rDdOc6sm"},"source":["Now let us define a function to calculate the predictions  by forward propagation"]},{"cell_type":"code","metadata":{"id":"LxnLQL9-bT2A"},"source":["def test(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    preds = np.argmax(probs, axis=1)\n","    return np.count_nonzero(y==preds)/y.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PJd-MKKedKG1"},"source":["Now let us define a function to train the model. First we will perform forward propagation then backpropagation to update the gradient descent parameters then assign updated paramters to the model."]},{"cell_type":"code","metadata":{"id":"JneN3n5kiu12"},"source":["def train(model, X_train, X_test, Y_train, Y_test, print_loss=True):\n","    # Gradient descent. For each batch...\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    for i in range(0, params[\"max_iter\"]):\n","\n","        # Forward propagation\n","        a1, probs = feedforward(model, X_train)\n","\n","        # Backpropagation\n","        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n","\n","        # Gradient descent parameter update\n","        W1 += -params[\"lr\"] * dW1\n","        b1 += -params[\"lr\"] * db1\n","        W2 += -params[\"lr\"] * dW2\n","        b2 += -params[\"lr\"] * db2\n","        \n","        # Assign new parameters to the model\n","        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","        if print_loss and i % 50 == 0:\n","            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n","                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0MI7Wvdiu16"},"source":["#### 3. Training the model"]},{"cell_type":"code","metadata":{"id":"x955E_rOiu17","executionInfo":{"status":"ok","timestamp":1544072375741,"user_tz":-330,"elapsed":5393,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"outputId":"dfdfc9d6-0db6-4f79-aadc-9c2b31c97408","colab":{"base_uri":"https://localhost:8080/","height":711}},"source":["model = build_model()\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n","\n","model = train(model, X_train, X_test, Y_train, Y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss after iteration 0: 2.492749 , Test accuracy: 0.13236929922135707 \n","\n","Loss after iteration 50: 1.712895 , Test accuracy: 0.45050055617352613 \n","\n","Loss after iteration 100: 1.311046 , Test accuracy: 0.6596218020022246 \n","\n","Loss after iteration 150: 1.043388 , Test accuracy: 0.7686318131256952 \n","\n","Loss after iteration 200: 0.858650 , Test accuracy: 0.8131256952169077 \n","\n","Loss after iteration 250: 0.725068 , Test accuracy: 0.8498331479421579 \n","\n","Loss after iteration 300: 0.625338 , Test accuracy: 0.8654060066740823 \n","\n","Loss after iteration 350: 0.548757 , Test accuracy: 0.882091212458287 \n","\n","Loss after iteration 400: 0.486890 , Test accuracy: 0.8921023359288098 \n","\n","Loss after iteration 450: 0.436110 , Test accuracy: 0.8987764182424917 \n","\n","Loss after iteration 500: 0.392825 , Test accuracy: 0.9087875417130145 \n","\n","Loss after iteration 550: 0.355599 , Test accuracy: 0.917686318131257 \n","\n","Loss after iteration 600: 0.324155 , Test accuracy: 0.9210233592880979 \n","\n","Loss after iteration 650: 0.297017 , Test accuracy: 0.9221357063403782 \n","\n","Loss after iteration 700: 0.273805 , Test accuracy: 0.9254727474972191 \n","\n","Loss after iteration 750: 0.253553 , Test accuracy: 0.92880978865406 \n","\n","Loss after iteration 800: 0.235535 , Test accuracy: 0.92880978865406 \n","\n","Loss after iteration 850: 0.219625 , Test accuracy: 0.9299221357063404 \n","\n","Loss after iteration 900: 0.205759 , Test accuracy: 0.932146829810901 \n","\n","Loss after iteration 950: 0.193485 , Test accuracy: 0.9354838709677419 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CSGc4tMlkYKY"},"source":["### Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"0dguxf1Liu2B"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Too Simple, I am wasting time\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBmeI-rgkbfG"},"source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"test\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20mhCAEFkdWE"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3o516AOkfkp","cellView":"form","executionInfo":{"status":"ok","timestamp":1542352714406,"user_tz":-330,"elapsed":1432,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"outputId":"b9597b59-828b-45a9-9c68-4a4494ce7788","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your submission is successful. Ref: 2807\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wNUf0YWAiRZF"},"source":[""],"execution_count":null,"outputs":[]}]}