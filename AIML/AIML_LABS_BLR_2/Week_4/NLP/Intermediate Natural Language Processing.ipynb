{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intermediate Natural Language Processing.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"9thwECUTm4PV","colab_type":"text"},"cell_type":"markdown","source":["## Setup\n","\n","This guide was written in Python 3.6.\n","\n","### Libraries\n","\n","We'll be working with the re library for regular expressions and nltk for natural language processing techniques, so make sure to install them! To install these libraries, enter the following commands into your terminal: \n","\n","``` \n","pip3 install nltk\n","pip3 install spacy\n","pip3 install pandas\n","pip3 install scikit-learn\n","```\n","\n","### Other\n","\n","Sentence boundary detection requires the dependency parse, which requires data to be installed, so enter the following command in your terminal. \n","\n","```\n","python3 -m spacy.en.download all\n","```"]},{"metadata":{"id":"e8Q-7QUtm4PY","colab_type":"text"},"cell_type":"markdown","source":["## Background\n","\n","### Polarity Flippers\n","\n","Polarity flippers are words that change positive expressions into negative ones or vice versa. \n","\n","#### Negation \n","\n","Negations directly change an expression's sentiment by preceding the word before it. An example would be\n","\n","```\n","The cat is not nice.\n","```\n","\n","#### Constructive Discourse Connectives\n","\n","Constructive Discourse Connectives are words which indirectly change an expression's meaning with words like \"but\". An example would be \n","\n","``` \n","I usually like cats, but this cat is evil.\n","```\n","\n","### Multiword Expressions\n","\n","Multiword expressions are important because, depending on the context, can be considered positive or negative. For example, \n","\n","``` \n","This song is shit.\n","```\n","is definitely considered negative. Whereas\n","\n","``` \n","This song is the shit.\n","```\n","is actually considered positive, simply because of the addition of 'the' before the word 'shit'."]},{"metadata":{"id":"Q_rzrEfXm4Pd","colab_type":"text"},"cell_type":"markdown","source":["### WordNet\n","\n","WordNet is an English lexical database with emphasis on synonymy - sort of like a thesaurus. Specifically, nouns, verbs, adjectives and adjectives are grouped into synonym sets. \n","\n","#### Synsets\n","\n","nltk has a built-in WordNet that we can use to find synonyms. We import it as such:"]},{"metadata":{"id":"9ACKu4ilm4Ph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"86b5df3f-d917-4cc8-84bd-565c264d87b1","executionInfo":{"status":"ok","timestamp":1538817564079,"user_tz":-330,"elapsed":1960,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')"],"execution_count":54,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"id":"HDt55IJam4Pv","colab_type":"text"},"cell_type":"markdown","source":["If we feed a word to the synsets() method, the return value will be the class to which belongs. For example, if we call the method on good,  \n"]},{"metadata":{"id":"INEtkGR2m4Pw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"cfed0a7c-bb61-4462-a346-daf149f88fbf","executionInfo":{"status":"ok","timestamp":1538817569256,"user_tz":-330,"elapsed":3280,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["print(wn.synsets('good'))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n"],"name":"stdout"}]},{"metadata":{"id":"ESSJj2kUm4QC","colab_type":"text"},"cell_type":"markdown","source":["if we want to take it a step further, we can. We've previously learned what lemmas are - if you want to obtain the lemmas for a given synonym set, you can use the following method:\n"]},{"metadata":{"id":"xrf1nv7Qm4QG","colab_type":"code","colab":{},"outputId":"36bea7eb-c6e5-4dc3-8d92-629a83f20dda"},"cell_type":"code","source":["print(wn.synset('car.n.01').lemma_names())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['car', 'auto', 'automobile', 'machine', 'motorcar']\n"],"name":"stdout"}]},{"metadata":{"id":"VSB4fu8Pm4QU","colab_type":"text"},"cell_type":"markdown","source":["Even more, you can do things like get the definition of a word: \n"]},{"metadata":{"id":"2fAV2g_Pm4QZ","colab_type":"code","colab":{},"outputId":"61a2df07-ed1d-4f70-e8ce-6700b3308809"},"cell_type":"code","source":["print(wn.synset('car.n.01').definition())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"],"name":"stdout"}]},{"metadata":{"id":"xb7KSGzgm4Ql","colab_type":"text"},"cell_type":"markdown","source":["#### Negation\n","\n","With WordNet, we can easily detect negations. This is great because it's not only fast, but it requires no training data and has a fairly good predictive accuracy. On the other hand, it's not able to handle context well or work with multiple word phrases. \n","\n","\n","### SentiWordNet\n","\n","Based on WordNet synsets, SentiWordNet is a lexical resource for opinion mining, where each synset is assigned three sentiment scores: positivity, negativity, and objectivity."]},{"metadata":{"id":"jteSLp-Lm4Qn","colab_type":"code","colab":{}},"cell_type":"code","source":["from nltk.corpus import sentiwordnet as swn\n","cat = swn.senti_synset('cat.n.03')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6ahQtOKGm4Q3","colab_type":"code","colab":{},"outputId":"699713d3-923b-4ad0-b524-53f84e03419b"},"cell_type":"code","source":["cat.pos_score()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"YQMbjuBRm4RG","colab_type":"code","colab":{},"outputId":"ee7198ed-482c-4f49-9bb1-fbd3bdf8bb26"},"cell_type":"code","source":["cat.neg_score()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.125"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"5t7AC0ZCm4RU","colab_type":"code","colab":{},"outputId":"04d2816d-f4d7-4b70-c6ab-1501e4b0852a"},"cell_type":"code","source":["cat.obj_score()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.875"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"eIFFtRHWm4Re","colab_type":"text"},"cell_type":"markdown","source":["### Stop Words\n","\n","Stop words are extremely common words that would be of little value in our analysis are often excluded from the vocabulary entirely. Some common examples are determiners like the, a, an, another, but your list of stop words (or <b>stop list</b>) depends on the context of the problem you're working on. \n"]},{"metadata":{"id":"n4_bs_GZm4Rf","colab_type":"text"},"cell_type":"markdown","source":["## Information Extraction\n","\n","Information Extraction is the process of acquiring meaning from text in a computational manner. \n","\n","### Data Forms\n","\n","#### Structured Data\n","\n","Structured Data is when there is a regular and predictable organization of entities and relationships.\n","\n","#### Unstructured Data\n","\n","Unstructured data, as the name suggests, assumes no organization. This is the case with most written textual data. \n","\n","### What is Information Extraction?\n","\n","With that said, information extraction is the means by which you acquire structured data from a given unstructured dataset. There are a number of ways in which this can be done, but generally, information extraction consists of searching for specific types of entities and relationships between those entities. \n","\n","An example is being given the following text, \n","\n","```\n","Martin received a 98% on his math exam, whereas Jacob received a 84%. Eli, who also took the same test, received an 89%. Lastly, Ojas received a 72%.\n","```\n","This is clearly unstructured. It requires reading for any logical relationships to be extracted. Through the use of information extraction techniques, however, we could output structured data such as the following: \n","\n","```\n","Name     Grade\n","Martin   98\n","Jacob    84\n","Eli      89\n","Ojas     72\n","```"]},{"metadata":{"id":"IjOqMotXm4Rh","colab_type":"text"},"cell_type":"markdown","source":["## Named Entity Extraction\n","\n","Named entities are noun phrases that refer to specific types of individuals, such as organizations, people, dates, etc. Therefore, the purpose of a named entity recognition (NER) system is to identify all textual mentions of the named entities.\n","\n","### spaCy\n","\n","In the following exercise, we'll build our own named entity recognition system with the Python module `spaCy`, a Python module commonly used for Natural Language Processing in industry. "]},{"metadata":{"id":"RAOJmUMom4Rj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"outputId":"171944ac-6150-4d0e-821b-bc57ab685b94","executionInfo":{"status":"ok","timestamp":1538816489290,"user_tz":-330,"elapsed":3863,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["!pip3 install -U spacy"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n","Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n","Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n","Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n","Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n","Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n","Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n","Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n","Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n","Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n","Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n","Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n","Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n","Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n","Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n","Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n","Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n"],"name":"stdout"}]},{"metadata":{"id":"h8h6I8RRm4Rx","colab_type":"code","colab":{}},"cell_type":"code","source":["import spacy\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a-33QwA0ndZ3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":270},"outputId":"d0c15742-7054-487c-b19d-b0246dcd7803","executionInfo":{"status":"ok","timestamp":1538816615702,"user_tz":-330,"elapsed":16440,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["!python -m spacy download en"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n","\u001b[K    100% |████████████████████████████████| 37.4MB 59.9MB/s \n","\u001b[?25hInstalling collected packages: en-core-web-sm\n","  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n","\n","    You can now load the model via spacy.load('en')\n","\n"],"name":"stdout"}]},{"metadata":{"id":"XWQ38MREm4R9","colab_type":"text"},"cell_type":"markdown","source":["Using spaCy, we'll load the built-in English tokenizer, tagger, parser, NER and word vectors. We indicate this with the parameter `'en'`:"]},{"metadata":{"id":"T3by7fWym4SA","colab_type":"code","colab":{}},"cell_type":"code","source":["nlp = spacy.load('en')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hgQD9Liwm4SJ","colab_type":"text"},"cell_type":"markdown","source":["We need an example to actually process, so below is some text from Columbia's website. With this example in mind, we feed it into the tokenizer."]},{"metadata":{"id":"YrPTjSC4m4SK","colab_type":"code","colab":{}},"cell_type":"code","source":["review = \"Macbeth is most guilty of his own destruction and evil, but other characters played a significant part in his reasoning behind the crimes he committed. The Three Witches gave Macbeth a path to follow of how to obtain the goal that he had wanted for a long time, to become king. His wife, Lady Macbeth, was a huge incentive to commit the crimes he committed. She manipulated Macbeth in many ways. Even considering all of that, Macbeth is most guilty because he and only he can control his actions.The Three Witches tell Macbeth that he will become Thane of Cawdor and King of Scotland. They also predicted that Banquo's sons will end up being kings, but that Banquo would never become king. Because of their predictions, Macbeth murders many people. They also help Hecate concoct a potion that puts a curse on Macbeth. They definitely helped Macbeth along his evil path. Lady Macbeth is Macbeth's wife. She urges him to kill King Duncan so that he can be King. She later loses her nerve and starts sleepwalking because of the stress of killing Duncan. It gets so bad that she ends up committing suicide, but before that she did everything in her power to convince him to kill Duncan. She accused him of being weak like a woman. She knew that insulting him would motivate him. Macbeth is a nobleman of Scotland. Early on he is known as Thane of Glamis, but later becomes Thane of Cawdor after the original Thane of Cawdor is killed for treason. Macbeth is an extremely ambitious and power hungry man, and is always looking for a newer better title. Macbeth kills Duncan to become king, kills Banquo because his family was destined to become rulers over Scotland, and kills all of Macduff's family. The whole story seems to be about Macbeth and all of his efforts to get and keep the throne. Macbeth is definitely the guiltiest person in that whole ordeal\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"--hGbeOXm4SR","colab_type":"code","colab":{}},"cell_type":"code","source":["doc = nlp(review) # entities"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0y8LDC9fm4Sf","colab_type":"text"},"cell_type":"markdown","source":["Going along the process of named entity extraction, we begin by segmenting the text, i.e. splitting it into a list of sentences. "]},{"metadata":{"id":"Q1in8R-Em4Sj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"168c588a-b4a1-46b1-c17c-3cb8a4e0ee63","executionInfo":{"status":"ok","timestamp":1538816965887,"user_tz":-330,"elapsed":1076,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["sentences = [sentence.orth_ for sentence in doc.sents] # list of sentences\n","print(\"There were {} sentences found.\".format(len(sentences)))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["There were 22 sentences found.\n"],"name":"stdout"}]},{"metadata":{"id":"TTdcuek0m4S4","colab_type":"text"},"cell_type":"markdown","source":["Now, we go a step further, and count the number of nounphrases by taking advantage of chunk properties."]},{"metadata":{"id":"3Ook9iVbm4S8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b06eb403-3a68-43c5-be5e-9f70b7982bd3","executionInfo":{"status":"ok","timestamp":1538816969645,"user_tz":-330,"elapsed":861,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["nounphrases = [[np.orth_, np.root.head.orth_] for np in doc.noun_chunks]\n","print(nounphrases[0])\n","#print(\"There were {} noun phrases found.\".format(len(nounphrases)))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["['Macbeth', 'is']\n"],"name":"stdout"}]},{"metadata":{"id":"NJBxtI09m4TM","colab_type":"text"},"cell_type":"markdown","source":["Lastly, we achieve our final goal: entity extraction. "]},{"metadata":{"id":"q3IiX5oZm4TQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3f2914eb-4277-43d9-8884-d45e04abe4fe","executionInfo":{"status":"ok","timestamp":1538816974167,"user_tz":-330,"elapsed":890,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["entities = list(doc.ents) # converts entities into a list\n","print(\"There were {} entities found\".format(len(entities)))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["There were 34 entities found\n"],"name":"stdout"}]},{"metadata":{"id":"CcUm-vufm4Tk","colab_type":"text"},"cell_type":"markdown","source":["So now, we can turn this into a DataFrame for better visualization: "]},{"metadata":{"id":"63DF-Sunm4Tm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":896},"outputId":"68a3b8b8-041f-4c43-ed8e-698c83a49b29","executionInfo":{"status":"ok","timestamp":1538816979454,"user_tz":-330,"elapsed":1147,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["orgs_and_people = [entity.orth_ for entity in entities if entity.label_ in ['ORG','PERSON']]\n","pd.DataFrame(orgs_and_people)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lady Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Thane of Cawdor</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Banquo</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Banquo</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Hecate</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Lady Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Duncan</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Duncan</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Duncan</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Thane of Cawdor</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Thane of Cawdor</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Duncan</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Banquo</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Macduff</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Macbeth</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Macbeth</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  0\n","0           Macbeth\n","1      Lady Macbeth\n","2           Macbeth\n","3           Macbeth\n","4           Macbeth\n","5   Thane of Cawdor\n","6            Banquo\n","7            Banquo\n","8           Macbeth\n","9            Hecate\n","10          Macbeth\n","11          Macbeth\n","12     Lady Macbeth\n","13          Macbeth\n","14           Duncan\n","15           Duncan\n","16           Duncan\n","17          Macbeth\n","18  Thane of Cawdor\n","19  Thane of Cawdor\n","20          Macbeth\n","21          Macbeth\n","22           Duncan\n","23           Banquo\n","24          Macduff\n","25          Macbeth\n","26          Macbeth"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"dxt1GebPm4Tu","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","> Indented block\n","\n","\n","\n","In summary, named entity extraction typically follows the process of sentence segmentation, noun phrase chunking, and, finally, entity extraction. "]},{"metadata":{"id":"90djdZizm4Ty","colab_type":"text"},"cell_type":"markdown","source":["### nltk\n","\n","Next, we'll work through a similar example as before, this time using the nltk module to extract the named entities through the use of chunk parsing. As always, we begin by importing our needed modules and example: "]},{"metadata":{"id":"C3-hdCfcm4T0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"outputId":"986b8e58-2aae-4e9e-c55f-5c3498503023","executionInfo":{"status":"ok","timestamp":1538817356848,"user_tz":-330,"elapsed":1990,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('ieer')\n","import re\n","content = \"Starbucks has not been doing well lately\""],"execution_count":43,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package ieer to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/ieer.zip.\n"],"name":"stdout"}]},{"metadata":{"id":"DSumkiF4m4T6","colab_type":"text"},"cell_type":"markdown","source":["Then, as always, we tokenize the sentence and follow up with parts-of-speech tagging. "]},{"metadata":{"id":"1mUTUqKKm4T9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"54059855-d91f-4247-d56d-b61609a037a2","executionInfo":{"status":"ok","timestamp":1538817144103,"user_tz":-330,"elapsed":1211,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["tokenized = nltk.word_tokenize(content)\n","tagged = nltk.pos_tag(tokenized)\n","print(tagged)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[('Starbucks', 'NNP'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('doing', 'VBG'), ('well', 'RB'), ('lately', 'RB')]\n"],"name":"stdout"}]},{"metadata":{"id":"L_hlPo3Nm4UO","colab_type":"text"},"cell_type":"markdown","source":["Great, now we've got something to work with! \n","\n","``` \n","[('Starbucks', 'NNP'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('doing', 'VBG'), ('well', 'RB'), ('lately', 'RB')]\n","```"]},{"metadata":{"id":"I6v-3x0nm4Ud","colab_type":"text"},"cell_type":"markdown","source":["Now, if you wanted to simply get the named entities from the namedEnt object we created, how do you think you would go about doing so?"]},{"metadata":{"id":"Usit7Z5Tm4Uf","colab_type":"text"},"cell_type":"markdown","source":["## Chunking\n","\n","Chunking is used for entity recognition and segments and labels multitoken sequences. This typically involves segmenting multi-token sequences and labeling them with entity types, such as 'person', 'organization', or 'time'. \n","\n","### Noun Phrase Chunking\n","\n","Noun Phrase Chunking, or NP-Chunking, is where we search for chunks corresponding to individual noun phrases.\n","\n","We can use nltk, as is the case most of the time, to create a chunk parser. We begin with importing nltk and defining a sentence with its parts-of-speeches tagged (which we covered in the previous tutorial). \n"]},{"metadata":{"id":"JRgZtkyam4Uh","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk \n","sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R11LFj-nm4Uu","colab_type":"text"},"cell_type":"markdown","source":["Next, we define the tag pattern of an NP chunk. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, e.g. `<DT>?<JJ>*<NN>`. This is how the parse tree for a given sentence is acquired.  \n"]},{"metadata":{"id":"UozCBQXfm4Uw","colab_type":"code","colab":{}},"cell_type":"code","source":["pattern = \"NP: {<DT>?<JJ>*<NN>}\" "],"execution_count":0,"outputs":[]},{"metadata":{"id":"DmML9TOym4U6","colab_type":"text"},"cell_type":"markdown","source":["Finally we create the chunk parser with the nltk `RegexpParser()` class. "]},{"metadata":{"id":"0cJHZmbkm4U8","colab_type":"code","colab":{}},"cell_type":"code","source":["NPChunker = nltk.RegexpParser(pattern) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"9Bv0So5dm4VD","colab_type":"text"},"cell_type":"markdown","source":["And lastly, we actually parse the example sentence and display its parse tree. \n"]},{"metadata":{"id":"U2la8vgjm4VF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"8e8916ee-3cf5-488c-ef33-7406be3fbf1e","executionInfo":{"status":"ok","timestamp":1538817331458,"user_tz":-330,"elapsed":2811,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["result = NPChunker.parse(sentence) \n","print(result)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["(S\n","  (NP the/DT little/JJ yellow/JJ dog/NN)\n","  barked/VBD\n","  at/IN\n","  (NP the/DT cat/NN))\n"],"name":"stdout"}]},{"metadata":{"id":"kMuXYT5am4VP","colab_type":"text"},"cell_type":"markdown","source":["## Relation Extraction \n","\n","Once we have identified named entities in a text, we then want to analyze for the relations that exist between them. This can be performed using either rule-based systems, which typically look for specific patterns in the text that connect entities and the intervening words, or using machine learning systems that typically attempt to learn such patterns automatically from a training corpus.\n","\n","### Rule-Based Systems\n","\n","In the rule-based systems approach, we look for all triples of the form (X, a, Y), where X and Y are named entities and a is the string of words that indicates the relationship between X and Y. Using regular expressions, we can pull out those instances of a that express the relation that we are looking for. \n","\n","In the following code, we search for strings that contain the word \"in\". The special regular expression `(?!\\b.+ing\\b)` allows us to disregard strings such as `success in supervising the transition of`, where \"in\" is followed by a gerund. "]},{"metadata":{"id":"PL-wxmSYm4VQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":270},"outputId":"63c636d3-4fee-415d-a5b0-b2c2f4498701","executionInfo":{"status":"ok","timestamp":1538817362814,"user_tz":-330,"elapsed":1017,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n","for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n","    for rel in nltk.sem.relextract.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n","         print (nltk.sem.relextract.rtuple(rel))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n","[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n","[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n","[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n","[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n","[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n","[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n","[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n","[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n","[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n","[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n","[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n","[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"],"name":"stdout"}]},{"metadata":{"id":"-0Jd5wrHm4VW","colab_type":"text"},"cell_type":"markdown","source":["Note that the X and Y named entitities types all match with one another! Object type matching is an important and required part of this process. "]},{"metadata":{"id":"aGGwbtJ-m4Vn","colab_type":"text"},"cell_type":"markdown","source":["So then we'll define the test and training data URLs to variables, as well as filenames for each of those datasets."]},{"metadata":{"id":"OAmcUC7Km4Wl","colab_type":"text"},"cell_type":"markdown","source":["### Preparing the Data\n","\n","To implement our bag-of-words linear classifier, we need our data in a format that allows us to feed it in to the classifer. Using sklearn.feature_extraction.text.CountVectorizer in the Python scikit learn module, we can convert the text documents to a matrix of token counts. So first, we import all the needed modules: "]},{"metadata":{"id":"dbWVODjrm4Wn","colab_type":"code","colab":{}},"cell_type":"code","source":["import re\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer        \n","from nltk.stem.porter import PorterStemmer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GgTEcX0dm4Wy","colab_type":"text"},"cell_type":"markdown","source":["We need to remove punctuations, lowercase, remove stop words, and stem words. All these steps can be directly performed by CountVectorizer if we pass the right parameter values. We can do this as follows. \n","\n","We first create a stemmer, using the Porter Stemmer implementation."]},{"metadata":{"id":"SjNbKN0nm4Wz","colab_type":"code","colab":{}},"cell_type":"code","source":["stemmer = PorterStemmer()\n","def stem_tokens(tokens, stemmer):\n","    stemmed = [stemmer.stem(item) for item in tokens]\n","    return(stemmed)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0Z_xefMJm4W5","colab_type":"text"},"cell_type":"markdown","source":["Here, we have our tokenizer, which removes non-letters and stems:"]},{"metadata":{"id":"B_DQX44km4W7","colab_type":"code","colab":{}},"cell_type":"code","source":["def tokenize(text):\n","    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n","    tokens = nltk.word_tokenize(text)\n","    stems = stem_tokens(tokens, stemmer)\n","    return(stems)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SS8Aozyom4XC","colab_type":"text"},"cell_type":"markdown","source":["Here we init the vectoriser with the CountVectorizer class, making sure to pass our tokenizer and stemmers as parameters, remove stop words, and lowercase all characters."]},{"metadata":{"id":"z7uKDMHfm4XE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"1c002cb9-db74-48d2-c3d6-82d8f940beb6","executionInfo":{"status":"ok","timestamp":1538817488790,"user_tz":-330,"elapsed":1731,"user":{"displayName":"Raghava kumar","photoUrl":"","userId":"12233731786455370504"}}},"cell_type":"code","source":["vectorizer = CountVectorizer(\n","    analyzer = 'word',\n","    tokenizer = tokenize,\n","    lowercase = True,\n","    stop_words = 'english',\n","    max_features = 85\n",")\n","\n","print(vectorizer)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=85, min_df=1,\n","        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n","        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","        tokenizer=<function tokenize at 0x7f778dd8de18>, vocabulary=None)\n"],"name":"stdout"}]},{"metadata":{"id":"7PqtDCRdm4XR","colab_type":"text"},"cell_type":"markdown","source":["Next, we use the `fit_transform()` method to transform our corpus data into feature vectors. Since the input needed is a list of strings, we concatenate all of our training and test data. "]}]}