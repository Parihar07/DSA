{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0SI_AEBcqs_S","colab_type":"text"},"cell_type":"markdown","source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"kGv9cnKpqs_U","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to understand Quantization."]},{"metadata":{"id":"QqZUYFdZNSBg","colab_type":"text"},"cell_type":"markdown","source":["Neural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model. Because they're all slightly different floating point numbers, simple compression formats like zip don't compress them well.\n","\n","Training neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work. Taking a pre-trained model and running inference is very different. If you think about recognizing an object in a photo you've just taken, the network has to ignore all the noise, lighting changes, and other non-essential differences between it and the training examples it's seen before, and focus on the important similarities instead. This ability means that they seem to treat low-precision calculations as just another source of noise, and still produce accurate results even with numerical formats that hold less information.\n","\n","\n","\n"]},{"metadata":{"id":"TCMtDHSHqs_W","colab_type":"text"},"cell_type":"markdown","source":["##### Keywords\n","\n","* Uniform quantization\n","* Non-uniform quantization\n","* K-means clustering\n"]},{"metadata":{"id":"u2vKGcIvqs_Y","colab_type":"text"},"cell_type":"markdown","source":["##### Expected time to complete the experiment is : 90 min"]},{"metadata":{"id":"P8C30f_7qy9u","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"tcCyG_oJq0oi","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P19A06E_test\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KxGcskFTq28t","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"981234567\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5m_tdbiBv1DD","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"95d8c450-1461-4bef-d72e-3a98c1d95424","executionInfo":{"status":"ok","timestamp":1549542459556,"user_tz":-330,"elapsed":9214,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M3W13_SAT_EXP_4\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install torchvision\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", datetime.datetime.now().date().strftime(\"%d %b %Y\"))\n","      print(\"Time of submission: \", datetime.datetime.now().time().strftime(\"%H:%M:%S\"))\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"LX6ALkaHqs_Y","colab_type":"text"},"cell_type":"markdown","source":["Once again we do our regular imports."]},{"metadata":{"id":"Mk4-H3VnNSBi","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","np.random.seed(1337)  # for reproducibility\n","from sklearn.cluster import KMeans\n","import torch \n","import torchvision\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-dE4fUykNSBm","colab_type":"text"},"cell_type":"markdown","source":["### Hyperparameters"]},{"metadata":{"id":"FKpyM4zzNSBm","colab_type":"code","colab":{}},"cell_type":"code","source":["num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","use_reg = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x9E-00Slqs_k","colab_type":"code","colab":{}},"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mJH3yYJKNSBp","colab_type":"text"},"cell_type":"markdown","source":["### Downloading the MNIST dataset"]},{"metadata":{"id":"3inDH-alNSBq","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dataset = dsets.MNIST(root='../data/',\n","                            train=True, \n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='../data/',\n","                           train=False, \n","                           transform=transforms.ToTensor())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3cIqGefQNSBw","colab_type":"text"},"cell_type":"markdown","source":["### Dataloader"]},{"metadata":{"id":"_3Nxm0bXNSBw","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MrX-dz4fNSB1","colab_type":"text"},"cell_type":"markdown","source":["### Define the network"]},{"metadata":{"id":"DYXjWlJsNSB2","colab_type":"code","colab":{}},"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.fc1 = nn.Linear(7*7*32, 300)\n","        self.fc2 = nn.Linear(300, 10)\n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ne2DrrXiNSB5","colab_type":"text"},"cell_type":"markdown","source":["<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "]},{"metadata":{"id":"7ui3ChcWNSB6","colab_type":"code","colab":{}},"cell_type":"code","source":["def reset_model():\n","    net = Net()\n","    net = net.to(device)\n","\n","    # Loss and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    return net,criterion,optimizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EfBSn8rJNSB8","colab_type":"text"},"cell_type":"markdown","source":["### Initializing the model"]},{"metadata":{"id":"7vLzLbbnNSB9","colab_type":"code","colab":{}},"cell_type":"code","source":["net, criterion, optimizer = reset_model()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZyXY7Oq6NSCB","colab_type":"text"},"cell_type":"markdown","source":["### Defining a L1 Regularizer"]},{"metadata":{"id":"zMLxnmDkNSCC","colab_type":"code","colab":{}},"cell_type":"code","source":["def l1_regularizer(net, loss, beta):\n","    l1_crit = nn.L1Loss(size_average=False)\n","    reg_loss = 0\n","    for param in net.parameters():\n","        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n","        reg_loss += l1_crit(param, target)\n","        \n","    loss += beta * reg_loss\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3NogS9pZNSCF","colab_type":"text"},"cell_type":"markdown","source":["### Training function"]},{"metadata":{"id":"6hUX_GntNSCG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Train the Model\n","\n","def training(net, reset = True):\n","    if reset == True:\n","        net, criterion, optimizer = reset_model()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    \n","    net.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        accuracy = []\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            temp_labels = labels\n","          \n","\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","\n","            if use_reg == True :\n","                loss = l1_regularizer(net,loss,beta=0.001)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == temp_labels).sum().item()\n","            accuracy.append(correct/float(batch_size))\n","\n","        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n","    \n","    return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_5fha93LNSCI","colab_type":"text"},"cell_type":"markdown","source":["### Testing function"]},{"metadata":{"id":"Kj1AAWuBNSCL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Test the Model\n","def testing(net):\n","    net.eval() \n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","       \n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4WRXM1YNSCP","colab_type":"text"},"cell_type":"markdown","source":["### Training and testing the network"]},{"metadata":{"id":"qSfX2-vFNSCP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"c8602788-1327-4e8a-fa73-c29d7fab4c0f","executionInfo":{"status":"ok","timestamp":1549542647370,"user_tz":-330,"elapsed":90869,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["reset = False\n","net = training(net, reset)\n","testing(net)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Loss: 565.9312, Accuracy: 0.9565\n","Epoch: 2, Loss: 235.6102, Accuracy: 0.9769\n","Epoch: 3, Loss: 195.3265, Accuracy: 0.9791\n","Epoch: 4, Loss: 177.5104, Accuracy: 0.9802\n","Epoch: 5, Loss: 165.8923, Accuracy: 0.9811\n","Test Accuracy of the network on the 10000 test images: 98.18 %\n"],"name":"stdout"}]},{"metadata":{"id":"b8kc_CbbNSCR","colab_type":"text"},"cell_type":"markdown","source":["### Uniform Quantization\n","\n","The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range.\n","\n","In the function below we send 8 bits as input which ressembles that the weights of the network should be represented with only 8 bits while storing to disk. In other words we use only 2^8 or 256 clusters. Hence each weight is represented as a 8-bit integer between 0-255.\n","\n","Thus before using the weights during test time they need to be projected into the original weight space by using the following equation:\n","\n","$$\n","W_{i} = min + \\dfrac{max-min}{255}*W_{index}\n","$$"]},{"metadata":{"id":"bkD7PGOCNSCT","colab_type":"code","colab":{}},"cell_type":"code","source":["def uniform_quantize(weight, bits):\n","    print('-------------------------LAYER---------------------------')\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weight))))\n","    n_clusters = 2**bits\n","    \n","    maxim = np.amax(weight)\n","    minim = np.amin(weight)\n","    step= (maxim-minim)/(n_clusters - 1)\n","\n","    clusters=[]\n","\n","    for i in range(0,n_clusters):\n","        clusters.append(minim)\n","        minim+=step\n","\n","    for i in range(0,len(weight)):\n","        dist= (clusters-weight[i])**2     \n","        weight[i]=clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weight))))\n","    \n","    return weight  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"NWEIuI9wNSCU","colab_type":"text"},"cell_type":"markdown","source":["### Uniform Quantization\n","\n","Different number of bits can be used for representing the weights and biases. The exact number of bits to use is a design choice and may depend on the complexity of the task at hand since using too less number of bits can result in poor performance. Here, we use 8 bits for quantizing the weights and 1 bit for the biases."]},{"metadata":{"id":"JpJ1C75qNSCU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":847},"outputId":"27d81c0e-e18a-4e7e-92bc-58991cfa1acc","executionInfo":{"status":"ok","timestamp":1549542674788,"user_tz":-330,"elapsed":15122,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = uniform_quantize(temp_weight, 8)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = uniform_quantize(temp_bias, 1)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 400\n","Number of unique parameters after quantization: 111\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 12\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 2304\n","Number of unique parameters after quantization: 131\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 13\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 4608\n","Number of unique parameters after quantization: 114\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 17\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 468995\n","Number of unique parameters after quantization: 158\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 300\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 3000\n","Number of unique parameters after quantization: 111\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 10\n","Number of unique parameters after quantization: 2\n"],"name":"stdout"}]},{"metadata":{"id":"lQsNHoNyNSCW","colab_type":"text"},"cell_type":"markdown","source":["Now that we have replaced the weight matrix with the approximated weight of the nearest cluster, we can test the network with the modified weights."]},{"metadata":{"id":"3qYOc_42NSCX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"41dda13a-b3ea-4bb7-d54b-2885f4ef1286","executionInfo":{"status":"ok","timestamp":1549542682252,"user_tz":-330,"elapsed":2620,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["testing(net)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Test Accuracy of the network on the 10000 test images: 98.38 %\n"],"name":"stdout"}]},{"metadata":{"id":"-sCB4SNmNSCa","colab_type":"text"},"cell_type":"markdown","source":["## Non-uniform quantization\n","\n","We have seen in the previous method that we divide the weight space into equally partitioned cluster heads. However, instead of forcing the cluster heads to be equally spaced it would make more sense to learn them. A common and obvious practice is to learn the weight space as a distribution of cluseter centers using k-means clustering. Here, we define a function to perform k-means to the weight values.\n","\n","$$\n","min\\sum_{i}^{mn}\\sum_{j}^{k}||w_{i}-c_{j}||_{2}^{2}\n","$$"]},{"metadata":{"id":"GUDAcctdNSCb","colab_type":"code","colab":{}},"cell_type":"code","source":["num_clusters = 8\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0,  max_iter=500, precompute_distances='auto', verbose=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r2wWh5lbNSCe","colab_type":"code","colab":{}},"cell_type":"code","source":["def non_uniform_quantize(weights):\n","    print(\"---------------------------Layer--------------------------------\")\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weights))))\n","    weights = np.reshape(weights,[weights.shape[0],1])\n","    print(weights.shape)\n","    kmeans_fit = kmeans.fit(weights)\n","    clusters = kmeans_fit.cluster_centers_\n","    \n","    for i in range(0,len(weights)):\n","        dist= (clusters-weights[i])**2     \n","        weights[i]=clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weights))))\n","    \n","    return weights  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"luzXZINVNSCg","colab_type":"text"},"cell_type":"markdown","source":["We reset the model and train the network since we had earlier done uniform quantization on the weight already."]},{"metadata":{"id":"IYdxVUsvNSCj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"f103805f-a028-455b-92cf-4dd3bd705d98","executionInfo":{"status":"ok","timestamp":1549542783059,"user_tz":-330,"elapsed":91767,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["reset = True\n","net = training(net, reset)\n","testing(net)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Loss: 546.8267, Accuracy: 0.9508\n","Epoch: 2, Loss: 237.4946, Accuracy: 0.9736\n","Epoch: 3, Loss: 195.0379, Accuracy: 0.9769\n","Epoch: 4, Loss: 174.0726, Accuracy: 0.9791\n","Epoch: 5, Loss: 164.0048, Accuracy: 0.9804\n","Test Accuracy of the network on the 10000 test images: 97.98 %\n"],"name":"stdout"}]},{"metadata":{"id":"WXhYLriUNSCm","colab_type":"text"},"cell_type":"markdown","source":["Uniform quantization on the weights and biases"]},{"metadata":{"id":"ygTmNfWsNSCm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1123},"outputId":"cc3ea4db-1a98-492c-ccf2-08e693d5034f","executionInfo":{"status":"ok","timestamp":1549542829683,"user_tz":-330,"elapsed":10356,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = non_uniform_quantize(temp_weight)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = non_uniform_quantize(temp_bias)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":24,"outputs":[{"output_type":"stream","text":["---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 400\n","(400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 2304\n","(2304, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 4608\n","(4608, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 468920\n","(470400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 300\n","(300, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 3000\n","(3000, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 10\n","(10, 1)\n","Number of unique parameters after quantization: 8\n"],"name":"stdout"}]},{"metadata":{"id":"XAZkRYg9NSCp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f999822b-dbc4-48c0-f450-da8456e261e6","executionInfo":{"status":"ok","timestamp":1549542858352,"user_tz":-330,"elapsed":3202,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["testing(net)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Test Accuracy of the network on the 10000 test images: 96.90 %\n"],"name":"stdout"}]},{"metadata":{"id":"AZME62IFNSCr","colab_type":"text"},"cell_type":"markdown","source":["### Retraining the network\n","\n","Here we see that 8 clusters are too less in order to maintain the network at the same accuracy since we see almost a 3% drop in performance. One of the solutions is to retrain the network. This helps the other weights to compensate for those weights which on being rounded off to the nearest cluster center have resulted in a drop in performance. Accuracy can be recovered significantly on retraining the network and then non-uniformly quantizing the weights again."]},{"metadata":{"id":"hDGjonRNNSCt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"a7ba4a5a-fe34-4ecb-d7d8-505c6e1bad42","executionInfo":{"status":"ok","timestamp":1549543087218,"user_tz":-330,"elapsed":94219,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}}},"cell_type":"code","source":["reset = False\n","net = training(net, reset)\n","#perform non-uniform quantization\n","testing(net)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Loss: 158.4530, Accuracy: 0.9812\n","Epoch: 2, Loss: 150.5025, Accuracy: 0.9824\n","Epoch: 3, Loss: 144.6509, Accuracy: 0.9831\n","Epoch: 4, Loss: 141.3243, Accuracy: 0.9831\n","Epoch: 5, Loss: 139.0651, Accuracy: 0.9835\n","Test Accuracy of the network on the 10000 test images: 98.50 %\n"],"name":"stdout"}]},{"metadata":{"id":"wNL2AuOeNSCx","colab_type":"text"},"cell_type":"markdown","source":["### References\n","\n","1. https://arxiv.org/pdf/1412.6115.pdf"]},{"metadata":{"id":"do82jrSqrJ3R","colab_type":"text"},"cell_type":"markdown","source":["#### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"CPcT0jDuNSCx","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e5EaiqxJrOjw","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NiMFoU9NrQM9","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yrDuolf4rSZC","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DFYZ95VYxoRZ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}