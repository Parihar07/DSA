{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_5.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"UtjqKzW7l1ql","colab_type":"text"},"cell_type":"markdown","source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"ZvRVBNVkl1qm","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is understand a modern Back Propagation Implementation "]},{"metadata":{"id":"oCBV1ishl1qn","colab_type":"text"},"cell_type":"markdown","source":["In this experiment we will be using MNIST database. The MNIST database is a dataset of handwritten digits. It has 60,000 training samples, and 10,000 test samples. Each image is represented by 28 x 28 pixels, each containing a value 0 - 255 with its gray scale value.\n","\n","It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n","\n","It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."]},{"metadata":{"id":"KTfTydasYCkh","colab_type":"text"},"cell_type":"markdown","source":["### Keywords"]},{"metadata":{"id":"g97IA0QGYEil","colab_type":"text"},"cell_type":"markdown","source":["MLP\n","\n","BackPropagation\n","\n","Chain Rule\n","\n","Softmax\n","\n","Activation Function\n","\n","Solvers\n","\n","Gradient descent\n","\n","Zero Weight Initialization\n","\n","Xavier Weight Initialization\n","\n","Hyper Parameters"]},{"metadata":{"id":"cTajDqcbmB9z","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"GFoaL3gqmD77","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P19A06E_test\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lyz43e8HmHGn","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"981234567\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xeMHRA3wwEwg","colab_type":"code","cellView":"form","outputId":"4d1ad8e5-8d41-42e0-fdbf-f8b35494b1f7","executionInfo":{"status":"ok","timestamp":1549543232637,"user_tz":-330,"elapsed":1859,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M3W13_SAT_EXP_5\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","   \n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", datetime.datetime.now().date().strftime(\"%d %b %Y\"))\n","      print(\"Time of submission: \", datetime.datetime.now().time().strftime(\"%H:%M:%S\"))\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"wquX0aVEl1qp","colab_type":"code","colab":{}},"cell_type":"code","source":["# Importing Required Packages\n","import numpy as np\n","from scipy import ndimage\n","from matplotlib import pyplot as plt\n","from sklearn import manifold, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import fetch_mldata\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neural_network import MLPClassifier\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BmcfjBiml1qt","colab_type":"text"},"cell_type":"markdown","source":["Loading the dataset from sklearn dataset"]},{"metadata":{"id":"hNnNJLhPl1qv","colab_type":"code","outputId":"cd823fa9-d420-4edf-8a76-620d3b6971ad","executionInfo":{"status":"ok","timestamp":1549543276593,"user_tz":-330,"elapsed":38732,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Load MNIST datset \n","from sklearn.datasets import fetch_openml \n","mnist = fetch_openml('mnist_784') \n","X, Y = mnist.data, mnist.target\n","Y = Y.astype(int)\n","\n","X = X[::10, :]     ## taking the whole data will take a lot of processing time\n","Y = Y[::10]\n","# digits = datasets.load_digits(n_class=10)\n","# # Create our X and y data\n","# X = digits.data\n","# Y = digits.target\n","print(X.shape, Y.shape)\n","num_examples = X.shape[0]      ## training set size\n","nn_input_dim = X.shape[1]      ## input layer dimensionality\n","nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n","\n","params = {\n","    \"lr\":0.0001,        ## learning_rate\n","    \"max_iter\":500,\n","    \"weight_init\":\"xavier\",\n","    \"h_dimn\":100,     ## hidden_layer_size\n","}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7000, 784) (7000,)\n"],"name":"stdout"}]},{"metadata":{"id":"qM3lXGTvl1q0","colab_type":"text"},"cell_type":"markdown","source":["#### Weight Initializations"]},{"metadata":{"id":"gIiuLhpyl1q2","colab_type":"text"},"cell_type":"markdown","source":["\n","Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative."]},{"metadata":{"id":"XADD1eyFl1q3","colab_type":"text"},"cell_type":"markdown","source":["Zero Weight Initialization: This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n","\n"]},{"metadata":{"id":"CaovVT6ll1q4","colab_type":"text"},"cell_type":"markdown","source":["As a solution, it is common to initialize the weights of the neurons to small numbers (random or unique) and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. Instead of using random initializations, it is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n","\n","It is worth mentioning that if you do not know which technique should be chosen as weight initilalizaion method, Xaiver is often choosen as a initial try.\n","\n"]},{"metadata":{"id":"rLfLDLSSl1q6","colab_type":"code","colab":{}},"cell_type":"code","source":["def xavier_init(fan_in, fan_out):\n","    ## using FanAvg variation\n","    n = (fan_in+fan_out)/2\n","    limit = np.sqrt(3.0 * 1 / n)\n","    return np.random.uniform(size = (fan_in, fan_out), low = -limit, high = +limit)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C8O9NELEl1rA","colab_type":"code","colab":{}},"cell_type":"code","source":["def weight_initialization(params):\n","    hdim = params[\"h_dimn\"]\n","    winit = params[\"weight_init\"]\n","    if winit == \"random\":\n","        np.random.seed(0)\n","        W1 = np.random.randn(nn_input_dim, hdim)\n","        b1 = np.random.randn(1, hdim)\n","        W2 = np.random.randn(hdim, nn_output_dim)\n","        b2 = np.random.randn(1, nn_output_dim)\n","    elif winit == \"zeros\":\n","        W1 = np.zeros((nn_input_dim, hdim))\n","        b1 = np.zeros((1, hdim))\n","        W2 = np.zeros((hdim, nn_output_dim))\n","        b2 = np.zeros((1, nn_output_dim))\n","    elif winit == \"xavier\":\n","        W1 = xavier_init(nn_input_dim, hdim)\n","        b1 = xavier_init(1, hdim)\n","        W2 = xavier_init(hdim, nn_output_dim)\n","        b2 = xavier_init(1, nn_output_dim)\n","    elif winit == \"uniform\":\n","        W1 = np.random.uniform(size=(nn_input_dim, hdim), low=-1, high=1)/np.sqrt(nn_input_dim)\n","        b1 = np.random.uniform(size=(1, hdim), low=-1, high=1)\n","        W2 = np.random.uniform(size=(hdim, nn_output_dim), low=-1, high=1)/np.sqrt(hdim)\n","        b2 = np.random.uniform(size=(1, nn_output_dim), low=-1, high=1)\n","    elif winit == \"normal\":\n","        W1 = np.random.normal(loc = 0, scale = 0.5, size = (nn_input_dim, hdim))\n","        b1 = np.random.normal(loc = 0, scale = 0.5, size=(1, hdim))\n","        W2 = np.random.normal(loc = 0, scale = 0.5, size = (hdim, nn_output_dim))\n","        b2 = np.random.normal(loc = 0, scale = 0.5, size=(1, nn_output_dim))\n","    return W1, b1, W2, b2 \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RBEj75tKl1rF","colab_type":"code","colab":{}},"cell_type":"code","source":["def softmax(x):\n","    exp_scores = np.exp(x)\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","    return probs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x-Pi55bxl1rK","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_model():\n","    W1, b1, W2, b2 = weight_initialization(params)\n","    # This is what we return at the end\n","    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zkLAHNUAl1rR","colab_type":"code","colab":{}},"cell_type":"code","source":["def feedforward(model, x):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    z1 = x.dot(W1) + b1\n","    a1 = np.tanh(z1)\n","    z2 = a1.dot(W2) + b2\n","    probs = softmax(z2)\n","    return a1, probs\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yL59jqZnl1rX","colab_type":"code","colab":{}},"cell_type":"code","source":["def backpropagation(model, x, y, a1, probs):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    \n","    delta3 = probs\n","    delta3[range(y.shape[0]), y] -= 1\n","    dW2 = (a1.T).dot(delta3)\n","    db2 = np.sum(delta3, axis=0, keepdims=True)\n","    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n","    dW1 = np.dot(x.T, delta2)\n","    db1 = np.sum(delta2, axis=0)\n","    return dW2, db2, dW1, db1\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5yIzLCj9l1ra","colab_type":"code","colab":{}},"cell_type":"code","source":["def calculate_loss(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    \n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    \n","    # Calculating the cross entropy loss\n","    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n","    data_loss = np.sum(corect_logprobs)\n","    \n","    return 1./y.shape[0] * data_loss\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MoWjIR3El1rd","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    preds = np.argmax(probs, axis=1)\n","    return np.count_nonzero(y==preds)/y.shape[0]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vKtjrIvNl1rh","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(model, X_train, X_test, Y_train, Y_test, verbose=True):\n","    # Gradient descent. For each batch...\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    for i in range(0, params[\"max_iter\"]):\n","\n","        # Forward propagation\n","        a1, probs = feedforward(model, X_train)\n","\n","        # Backpropagation\n","        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n","\n","        # Gradient descent parameter update\n","        W1 += -params[\"lr\"] * dW1\n","        b1 += -params[\"lr\"] * db1\n","        W2 += -params[\"lr\"] * dW2\n","        b2 += -params[\"lr\"] * db2\n","        \n","        # Assign new parameters to the model\n","        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","        if verbose and i % 50 == 0:\n","            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n","                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tup_cU3kl1rp","colab_type":"text"},"cell_type":"markdown","source":["#### Experimenting with different Weight Initializations and evaluate the corresponding test accuracies"]},{"metadata":{"id":"bpwb08Jxl1rs","colab_type":"code","outputId":"dacc27f5-e9a7-45ae-85bd-fe3188c450a3","executionInfo":{"status":"ok","timestamp":1549543512843,"user_tz":-330,"elapsed":210792,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4)\n","t = [\"xavier\",\"uniform\",\"normal\",\"zeros\",\"random\"]\n","\n","for i in range(5):\n","    params[\"weight_init\"] = t[i]\n","    model = build_model()\n","    model = train(model, X_train, X_test, Y_train, Y_test, verbose=False)\n","    print(params, \"TestAccuracy=\", test(model,X_test, Y_test))\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'xavier', 'h_dimn': 100} TestAccuracy= 0.8989285714285714\n","{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'uniform', 'h_dimn': 100} TestAccuracy= 0.9010714285714285\n","{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'normal', 'h_dimn': 100} TestAccuracy= 0.7753571428571429\n","{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'zeros', 'h_dimn': 100} TestAccuracy= 0.0925\n","{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'random', 'h_dimn': 100} TestAccuracy= 0.7146428571428571\n"],"name":"stdout"}]},{"metadata":{"id":"UrXsWGvTl1rz","colab_type":"text"},"cell_type":"markdown","source":["####  Selecting Hyperparameters\n","\n","scikit-learn provides a function: GridSearchCV to optimize your neural network's hyper-parameters automatically. We just provide the range or possible value of hyperparameters as the parameter"]},{"metadata":{"id":"ZY130_0el1r2","colab_type":"code","outputId":"e39f7ab9-c8b0-4d43-8d78-1aea535c0aaa","executionInfo":{"status":"ok","timestamp":1549544577594,"user_tz":-330,"elapsed":501,"user":{"displayName":"Vidyadhar Rao","photoUrl":"https://lh5.googleusercontent.com/-um6SAcmQKBA/AAAAAAAAAAI/AAAAAAAAC7o/q07oNDn7ftg/s64/photo.jpg","userId":"01257137440126415573"}},"colab":{"base_uri":"https://localhost:8080/","height":2698}},"cell_type":"code","source":["parameters = {'activation' : [\"tanh\", \"relu\"],\n","            'learning_rate_init' : [0.0001, 0.001],\n","            'hidden_layer_sizes' : [(300,), (300, 100), (100, 50)],\n","            'solver' : [\"adam\",\"sgd\"]\n","             }\n","clf = MLPClassifier()\n","clf = GridSearchCV(estimator=clf, param_grid=parameters, verbose=2, cv=2)\n","clf.fit(X_train, Y_train)   ## might take about 10 minutes depending on number of total parameters"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Fitting 2 folds for each of 24 candidates, totalling 48 fits\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n","[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.5s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam, total=  39.4s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam, total=  39.6s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd, total=  35.4s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd, total=  35.3s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam, total=  38.7s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam, total=  27.1s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd, total=  34.7s\n","[CV] activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd, total=  35.4s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam, total=  46.7s\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam, total=  46.6s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd, total=  42.3s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd, total=  42.1s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam, total=  10.8s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam, total=   9.5s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd, total=  42.1s\n","[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd, total=  41.9s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam, total=  18.0s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam, total=  17.5s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd, total=  15.9s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd, total=  15.6s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam, total=   8.1s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam \n","[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam, total=   5.9s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd, total=  15.7s\n","[CV] activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=tanh, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd, total=  15.6s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam, total=  10.5s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=adam, total=  12.8s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd, total=   4.4s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.0001, solver=sgd, total=   4.7s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam, total=   5.6s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=adam, total=   5.9s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd, total=  21.3s\n","[CV] activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300,), learning_rate_init=0.001, solver=sgd, total=  30.8s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam, total=  13.5s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam, total=  12.2s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd, total=   9.3s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd, total=   5.8s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam, total=   5.8s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam, total=   5.9s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd, total=  37.6s\n","[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd, total=  39.0s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam, total=  13.9s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=adam, total=  11.5s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd, total=   6.3s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.0001, solver=sgd, total=   7.2s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam, total=   7.7s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=adam, total=   6.3s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd, total=  14.1s\n","[CV] activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd \n","[CV]  activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, solver=sgd, total=   7.9s\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 16.4min finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=2, error_score='raise-deprecating',\n","       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(100,), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n","       validation_fraction=0.1, verbose=False, warm_start=False),\n","       fit_params=None, iid='warn', n_jobs=None,\n","       param_grid={'activation': ['tanh', 'relu'], 'learning_rate_init': [0.0001, 0.001], 'hidden_layer_sizes': [(300,), (300, 100), (100, 50)], 'solver': ['adam', 'sgd']},\n","       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n","       scoring=None, verbose=2)"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"RcevnbP6l1r9","colab_type":"code","colab":{}},"cell_type":"code","source":["print(clf.best_estimator_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"piziFhO9mPx-","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"klvDQDcql1sC","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NU4PhcZUmUA3","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r8i3OP1cmVuA","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nFAQUp7lmXc1","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]}]}