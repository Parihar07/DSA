{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_6.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sY8HfxCh_CSO","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"HcLZbnv4Ba3a","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to understand Siamese Network."]},{"metadata":{"id":"IuObTdeFBiGY","colab_type":"text"},"cell_type":"markdown","source":["Tons of data area available on the web (wikipedia, Google, Twitter, YouTube) that could be used to train an ML model.\n","One such source is Google Images. You enter a text query and Google Images shows thousands of related images based on the query and text that are present on the web page with the related image."]},{"metadata":{"id":"f5Gw25muBlKU","colab_type":"text"},"cell_type":"markdown","source":["In this experiment we would crawl images from Google Images and try to use this as data for training."]},{"metadata":{"id":"GHi5xGcCBnA-","colab_type":"text"},"cell_type":"markdown","source":["1. Your task is to search for face images for 'AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha'\n","\n","2. Refine your search to faces (Google Images -> enter query -> Tools -> Type -> Face). You could also use movies', ads' names as additional query (e.g., \"Aamir 3 idiots\", \"Boman Irani Khosla Ka Ghosla\", \"Katrina Slice ad\" etc.). The results are noisy but they are useful, and moreover, they are avaible in abundance and for free!\n","\n","    a. Example: https://www.google.co.in/search?client=firefox-b-ab&dcr=0&biw=1366&bih=628&tbs=itp%3Aface&tbm=isch&sa=1&ei=5gbIWtCjN4n2vgSCoqzYBw&q=biswa+kalyan+rath\n","\n","3. Then use a browser extensions to download all the results into a directory. In this way you, would get around 300-600 images for each class. Overall, you should collect atleast 10000 images.\n","    \n","    a. Firefox: https://addons.mozilla.org/en-US/firefox/addon/google-images-downloader/\n","    \n","    b. Chrome: https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm/related?hl=en\n","    \n","4. **Without cleaning** use these images as your training data. Test you results on IMFDB test set.\n","\n"]},{"metadata":{"id":"xbKgGvN8BtWG","colab_type":"text"},"cell_type":"markdown","source":["#### Run the Notebook on GPU"]},{"metadata":{"id":"lhoMm0ENB04C","colab_type":"text"},"cell_type":"markdown","source":["#### Setup Steps"]},{"metadata":{"id":"hFM_mHXUr0m5","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P19A06E_test\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z-UzlZNwr9vs","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"981234567\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d9JR0vFayDgq","colab_type":"code","cellView":"form","outputId":"ac3d53e6-24bf-4166-ea21-a00f4bea109e","executionInfo":{"status":"ok","timestamp":1549604441749,"user_tz":-330,"elapsed":70866,"user":{"displayName":"Priyanka Gaddam","photoUrl":"","userId":"07316718693164986599"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M3W13_SAT_EXP_6\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip install torchvision\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx unzip  One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/data_loader.py\")\n","    ipython.magic(\"sx cd One_shot_Face_recognition\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", datetime.datetime.now().date().strftime(\"%d %b %Y\"))\n","      print(\"Time of submission: \", datetime.datetime.now().time().strftime(\"%H:%M:%S\"))\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"jKYrrgDzgqdS","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aq9clcuMCRVy","colab_type":"text"},"cell_type":"markdown","source":["#### Importing the Required Packages"]},{"metadata":{"id":"PEONXHPEewqd","colab_type":"code","colab":{}},"cell_type":"code","source":["from data_loader import custom_data_loader\n","# Importing pytorch packages\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BTauj20mAO0p","colab_type":"code","colab":{}},"cell_type":"code","source":["%cd One_shot_Face_recognition"],"execution_count":0,"outputs":[]},{"metadata":{"id":"APSGlS_s_CSU","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# Importing config.py file\n","import config as cf\n","from utils import *\n","from light_cnn import LightCNN_9Layers #, LightCNN_29Layers, LightCNN_29Layers_v2\n","#from resnet import resnet18\n","from siamese_data_loader import *\n","from contrastive import *   ### implementation of contrastive loss\n","## Importing python packages\n","import os\n","import sys\n","import time\n","import datetime\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oia-EoIHCjo0","colab_type":"text"},"cell_type":"markdown","source":["#### Loading the data "]},{"metadata":{"id":"LWG1XpBV_CSi","colab_type":"code","colab":{}},"cell_type":"code","source":["img_root = cf.data_dir+'IMFDB_final/'\n","\n","train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n","val_list_file = cf.data_dirimg_root = cf.data_dir+'IMFDB_final/'\n","\n","train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n","val_list_file = cf.data_dir+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n","\n","\n","train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n","val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n","\n","print(len(train_image_list), len(val_image_list))\n","\n","### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n","### see siamese_data_loader.py for details\n","\n","trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n","                                                             resize = True, resize_shape=[128,128]), \n","batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n","\n","testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n","\n","\n","#classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n","\n","\n","train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n","val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n","\n","print(len(train_image_list), len(val_image_list))\n","\n","### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n","### see siamese_data_loader.py for details\n","\n","trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n","                                                             resize = True, resize_shape=[128,128]), \n","batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n","\n","testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n","\n","\n","classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jlVJMEWneLx4","colab_type":"code","colab":{}},"cell_type":"code","source":["img_root"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u2Aee8StCokK","colab_type":"text"},"cell_type":"markdown","source":["#### Command to check whether GPU is enabled or not\n"]},{"metadata":{"id":"e0_YCzNqeLyB","colab_type":"code","colab":{}},"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VvlieheJ_CSy","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","#Intilizaing the loss value as high value\n","best_loss = 99999999\n","\n","num_classes = 16"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y4NzPR-weLyH","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchvision import models "],"execution_count":0,"outputs":[]},{"metadata":{"id":"40tFVFoWeLyM","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_net = LightCNN_9Layers()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C2kay4SoG6dd","colab_type":"code","colab":{}},"cell_type":"code","source":["print(cf.data_dir)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ort2nuVUeLyS","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zlkvrFIeeLyW","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yUJKM-V5_CS-","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_net = LightCNN_9Layers()   ### creates an object of this network architecture\n","feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')\n","\n","\n","layers_to_remove = ['fc2']\n","for layers_ in layers_to_remove:        \n","    del(feature_net._modules[layers_])\n","    \n","classifier = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n","                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n","                           nn.Linear(32, num_classes))\n","\n","feature_net.fc2 = nn.Sequential(nn.Linear(256, 16))\n","feature_net = feature_net.to(device)\n","classifier =  classifier.to(device)\n","    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dvh9E5z0_CTG","colab_type":"code","colab":{}},"cell_type":"code","source":["### Intiliazing the loss\n","criterion = nn.CrossEntropyLoss()\n","siamese_loss = contrastive_loss()   ### Notice a new loss. contrastive.py shows how to compute contrastive loss."],"execution_count":0,"outputs":[]},{"metadata":{"id":"JFloT0WAeLyw","colab_type":"code","colab":{}},"cell_type":"code","source":["criterion = criterion.to(device)\n","siamese_loss = siamese_loss.to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lOmg3Ypb_CTO","colab_type":"text"},"cell_type":"markdown","source":["#### Lets train the siamese networks. The objective is images from same class (+ pair, label = 0) should have similar feature and images from different classes (- pair, label = 1) should have different features. Instead of having two physical networks sharing the weights, in implementation we have only one network and first pass image_1 (to get its feature) and then pass image_2 (to get its feature) through the same network. We then compute the contrastive loss on these feature pairs from input image pairs. This saves a lot of memory."]},{"metadata":{"id":"isK--8bS_CTS","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    feature_net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 1\n","    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(trainloader):\n","        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n","        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n","        \n","        loss = siamese_loss(features_1, features_2, targets.float())   ### compute the contrastive loss, computes the similarity between the features.\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        #print(1)\n","        \n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f '% (train_loss/(batch_idx+1)))\n","        \n","    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n","        #print(1)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qj7vLS23C_SS","colab_type":"text"},"cell_type":"markdown","source":["#### Function to test\n","\n"]},{"metadata":{"id":"zMwHtczA_CTc","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(epoch):\n","    global best_loss\n","    feature_net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 1\n","    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n","        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs_1, inputs_2, targets = Variable(inputs_1), Variable(inputs_2), Variable(targets)\n","        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2      \n","        \n","        loss = siamese_loss(features_1, features_2, targets.float())\n","        test_loss += loss.item()\n","        \n","        progress_bar(batch_idx, len(testloader), 'Loss: %.3f '\n","                         % (test_loss/(batch_idx+1)))\n","        \n","    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n","\n","    # Save checkpoint.\n","    losss = test_loss/len(testloader)\n","    if  losss < best_loss:   ### save model with the best loss so far\n","        print('Saving..') \n","        state = {\n","            'net': feature_net\n","        }\n","        if not os.path.isdir(cf.data_dir+'checkpoint'):\n","            os.mkdir(cf.data_dir+'checkpoint')\n","        torch.save(state, cf.data_dir+'checkpoint/siamese_ckpt.t7')\n","        best_loss = losss\n","    \n","    return test_loss/len(testloader)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3k0q1BOVDD86","colab_type":"text"},"cell_type":"markdown","source":["#### Creating the files to store train and validation data loss values"]},{"metadata":{"id":"KhnUSGoC_CTk","colab_type":"code","colab":{}},"cell_type":"code","source":["experiment = 'siamese_IMFDB/'\n","train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"w\")\n","val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"w\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xKSAHBYR_CT0","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_net = feature_net.to(device)\n","optimizer = optim.Adam(feature_net.parameters(), lr=1e-3)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n","for epoch in range(0, 10):\n","    train(epoch)\n","    test_loss = test(epoch)\n","    scheduler.step(test_loss)\n","    print(\"Test Loss: \", test_loss)\n","train_loss_file.close()\n","val_loss_file.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xYMrwcsA_CUG","colab_type":"code","colab":{}},"cell_type":"code","source":["### After training we load the model that performed the best on validation data (avoid picking overfitted model)\n","### we will use the base pre-trained network for feature extraction only. This feature is used to train an MLP classifier.\n","\n","feature_net = torch.load(cf.data_dir+'checkpoint/siamese_ckpt.t7')['net'].eval()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"48su1H5b_CUO","colab_type":"text"},"cell_type":"markdown","source":["#### Lets see how well does the siamese detect an imposter. We check whether image_2 is same individual as image_1 or an imposter. We do this by computing dissimilarity score between features."]},{"metadata":{"id":"fSo3BA0a_CUQ","colab_type":"code","outputId":"57126fc7-e38a-456a-d038-408bcfa969a1","executionInfo":{"status":"ok","timestamp":1549606306049,"user_tz":-330,"elapsed":65163,"user":{"displayName":"Priyanka Gaddam","photoUrl":"","userId":"07316718693164986599"}},"colab":{"base_uri":"https://localhost:8080/","height":50093,"output_embedded_package_id":"1EDax_gMlqptYhm5dAaXaJn60MMg9gxlR"}},"cell_type":"code","source":["testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n","\n","lab = ['same', 'imposter']\n","with torch.no_grad():\n","  for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n","      if batch_idx%10 == 0 or int(targets)==0:      ### show every tenth image or if its the same individual\n","\n","          inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","          optimizer.zero_grad()\n","          #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n","          features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","          features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n","\n","          dissimilarity = torch.nn.functional.cosine_similarity(features_1, features_2).item()\n","          img = np.concatenate((inputs_1.data.cpu().numpy()[0][0], inputs_2.data.cpu().numpy()[0][0]), axis = 1)\n","          plt.imshow(img, cmap='gray')\n","          plt.text(100,20,str(dissimilarity), fontsize=24, color='r')     ### similarity score\n","          plt.text(100,40,lab[int(targets.data[0])], fontsize=24, color='r')   ### ground truth\n","          plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"-9di4sNM_CUc","colab_type":"text"},"cell_type":"markdown","source":["### Now we use this network for feature extraction and train an MLP classifier. Feature_net is not updated/train/tweak after this. We only train the MLP classifier."]},{"metadata":{"id":"FTC3qFNJ_CUe","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","train_list_file = cf.data_dir+'IMFDB_train.txt'   #### 5000 images for training\n","val_list_file = cf.data_dir+'IMFDB_test.txt'      #### 1095 images for validation\n","\n","\n","train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n","val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n","\n","print(len(train_image_list), len(val_image_list))\n","\n","trainloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n","                                                             resize = True, resize_shape=[128,128]), \n","                                          batch_size=32, num_workers=16, shuffle = True, pin_memory=False)\n","\n","testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                         batch_size=10, num_workers=5, shuffle = False, pin_memory=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-tfWqQZB_CUm","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_classifier(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    classifier.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs, targets = Variable(inputs), Variable(targets)\n","        features = feature_net(inputs)[1]      \n","        \n","        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        loss = criterion(outputs_, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        \n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        \n","    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n","    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"srGe92K5_CUu","colab_type":"code","colab":{}},"cell_type":"code","source":["def test_classifier(epoch):\n","    global best_acc\n","    classifier.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        #if device:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","        features = feature_net(inputs)[1]      \n","        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        loss = criterion(outputs_, targets)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        \n","        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        \n","    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': classifier,\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(cf.data_dir+'checkpoint'):\n","            os.mkdir(cf.data_dir+'checkpoint')\n","        torch.save(state, cf.data_dir+'checkpoint/checkpoint_ckpt.t7')\n","        best_acc = acc\n","    \n","    return test_loss/len(testloader)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bO-t7kPP_CU0","colab_type":"code","colab":{}},"cell_type":"code","source":["best_acc = 0\n","experiment = 'siamese_IMFDB'\n","train_loss_file = open(cf.data_dir+experiment+\"train_loss2.txt\", \"w\")\n","val_loss_file = open(cf.data_dir+experiment+\"val_loss2.txt\", \"w\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_e3YpfZz_CU4","colab_type":"code","colab":{}},"cell_type":"code","source":["%%capture\n","optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n","for epoch in range(0, 30):\n","    train_classifier(epoch)\n","    test_loss = test_classifier(epoch)\n","    scheduler.step(test_loss)\n","    \n","train_loss_file.close()\n","val_loss_file.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_zOf7cwQ_CVG","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval():\n","    feature_net.eval()\n","    classifier.eval()\n","    \n","    testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n","    correct = 0\n","    total = 0\n","    conf_mat = np.zeros((num_classes, num_classes))\n","    total_ = 1e-12+np.zeros((num_classes))\n","    wrong_predictions = []\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        #if use_cuda:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","        features = feature_net.features(inputs).detach()\n","        features = features.view(1,-1)\n","        #print(features.size())        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        prediction = predicted.cpu().numpy()[0]\n","        targets = targets.data.cpu().numpy()[0]\n","        total_[targets] +=1\n","        conf_mat[predicted, targets] +=1\n","        \n","        if prediction != targets:\n","            wrong_predictions += [[inputs, prediction, targets]]\n","        \n","    for k in range(num_classes):\n","        conf_mat[:,k] /= total_[k]\n","    return conf_mat, 100.*correct/total, wrong_predictions\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"xu5-2sTarNfA","colab_type":"code","colab":{}},"cell_type":"code","source":["classifier = nn.Sequential(nn.Linear(8192, 64), nn.BatchNorm1d(64), nn.ReLU(),\n","                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n","                           nn.Linear(32, num_classes))\n","classifier = classifier.to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DjLZ6ZXq_CVQ","colab_type":"code","colab":{}},"cell_type":"code","source":["%%capture\n","conf, acc, wrong_predictions = eval()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rVIMarONIZCU","colab_type":"code","colab":{}},"cell_type":"code","source":["print(acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RPb3-WEb_CVg","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.imshow(conf, cmap='jet', vmin=0, vmax = 1)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FzV2QqgE_CVo","colab_type":"code","outputId":"e66bdadc-8d28-4eed-ff42-5631e1b617b7","executionInfo":{"status":"ok","timestamp":1549609895853,"user_tz":-330,"elapsed":54952,"user":{"displayName":"Priyanka Gaddam","photoUrl":"","userId":"07316718693164986599"}},"colab":{"base_uri":"https://localhost:8080/","height":36809,"output_embedded_package_id":"1fh6j_K-Mnm4dCDS8Jq5zd4rn9vCnvS5F"}},"cell_type":"code","source":["for w in wrong_predictions[::10]:\n","    print(classes[w[2]], 'confused with', classes[w[1]])\n","    plt.imshow(w[0][0][0].data.cpu().numpy(), cmap='gray')\n","    plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"0JZfPfrSshl3","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"lJ9ZOEPor9_s","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8IGUTKc5snRU","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"test\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R5G8VEQXsosU","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bJ72nhqosqhp","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g-VVMPKK9CG6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}