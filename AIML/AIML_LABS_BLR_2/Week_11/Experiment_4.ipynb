{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"NdCnz388ED4A","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"TjFWY7W8ED4C","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to understand how to visualize CNNs "]},{"metadata":{"id":"13b4b8u0ED4D","colab_type":"text"},"cell_type":"markdown","source":["\n","In this experiment, we will first follow the approach used by [Matthew D Zeiler, Rob Fergus](https://arxiv.org/abs/1311.2901) to visualize CNNs. We will visualize a VGG16 model trained on the Imagenet dataset.\n","\n","We also follow the approach used by [Mahendran and Vedaldi](https://arxiv.org/abs/1412.0035) to understand deep image representations. [Section 6 onwards]"]},{"metadata":{"id":"6M4mXl8YED4E","colab_type":"text"},"cell_type":"markdown","source":["We will be performing following actions in this experiment:\n","    \n","    1. Load the model and see its architecture\n","    2. Load and preprocess an image to pass as input to the network\n","    3. Visualize the kernel weights at each layer\n","    4. Visualizing the image as it passes through the network\n","    5. Visualizing output of each filter at a given layer\n","    6. Understanding Deep Image Representations by Inverting Them [Mahendran, Vedaldi]"]},{"metadata":{"id":"tdnQ5yg8ED4G","colab_type":"text"},"cell_type":"markdown","source":["### Keywords\n","\n","* CNN\n","* Minibatch\n","* Pooling\n","* Convolutions\n","* Filters\n","* Padding\n","* ReLU\n","* Softmax\n","* CrossEntorpyloss\n","* Adam\n","* Kernels/Filters\n","* Visualization of filters\n","* Visualization of learned representation"]},{"metadata":{"id":"otifP1F9ED4H","colab_type":"text"},"cell_type":"markdown","source":["### Expected time to complete this experiment is : 90min"]},{"metadata":{"id":"jll8vCv2sV5Y","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"800\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_blr_b6/cfus/week_11/module_3_week_11_experiment_4.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tHlXETLsEh1b","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"06pvO77lEklO","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qtFs2uMZEny1","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eEvWFL-PIy_H","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M3W11_SAT_EXP_4\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install torchvision\")\n","    ipython.magic(\"sx pip install Pillow==4.0.0\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp4/dog.jpg\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp4/cnn_utils.py\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", datetime.datetime.now().date().strftime(\"%d %b %Y\"))\n","      print(\"Time of submission: \", datetime.datetime.now().time().strftime(\"%H:%M:%S\"))\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KmPeN1sZED4I","colab_type":"text"},"cell_type":"markdown","source":["#### 1. Load the model and see its architecture"]},{"metadata":{"id":"S9s-pePFED4K","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torchvision import models\n","from torchvision import transforms, utils\n","\n","import numpy as np\n","import scipy.misc\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from PIL import Image\n","\n","model = models.vgg16(pretrained=True)\n","print(model.features)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7TzExHEzED4T","colab_type":"text"},"cell_type":"markdown","source":["#### We will load all the module details in a list "]},{"metadata":{"id":"u1wEeMK5ED4U","colab_type":"code","colab":{}},"cell_type":"code","source":["modules = list(model.features.modules())\n","modules = modules[1:]\n","print(modules,\"\\n\\n\")\n","print(\"third module = \", modules[2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xaJRjWkTED4a","colab_type":"text"},"cell_type":"markdown","source":["### 2. Load and preprocess an image to pass as input to the network"]},{"metadata":{"id":"v8kcBiQDED4b","colab_type":"code","colab":{}},"cell_type":"code","source":["def normalize(image):\n","    normalize = transforms.Normalize(\n","    mean=[0.485, 0.456, 0.406],\n","    std=[0.229, 0.224, 0.225]\n","    )\n","    preprocess = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    normalize\n","    ])\n","    image = Variable(preprocess(image).unsqueeze(0))\n","    return image\n","\n","img_raw = Image.open(\"dog.jpg\")\n","plt.imshow(img_raw)\n","plt.title(\"Image loaded successfully\")\n","\n","img = normalize(img_raw)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PqIuGuwNED4h","colab_type":"text"},"cell_type":"markdown","source":["### 3. Visualize the kernel weights at each layer"]},{"metadata":{"id":"8djbmV1UED4i","colab_type":"code","colab":{}},"cell_type":"code","source":["def visualize_weights(image, layer):\n","    weight_used = []\n","    \n","    ## Gather all Convolution layers and append their corresponding filters in a list\n","    for w in model.features.children():\n","        if isinstance(w, torch.nn.modules.conv.Conv2d):\n","            weight_used.append(w.weight.data)\n","\n","    print(\"(#filters, i/p depth, size of filter) === \",weight_used[layer].shape)\n","    print(\"No. of filters: \", weight_used[layer].shape[0])\n","    filters = []\n","    for i in range(weight_used[layer].shape[0]):\n","        filters.append(weight_used[layer][i,:,:,:].sum(dim=0))    ##summing across input depth(3 in the first layer)\n","        filters[i].div(weight_used[layer].shape[1])\n","        \n","    fig = plt.figure()\n","    plt.rcParams[\"figure.figsize\"] = (10, 10)\n","    for i in range(int(np.sqrt(weight_used[layer].shape[0])) * int(np.sqrt(weight_used[layer].shape[0]))):\n","        a = fig.add_subplot(np.sqrt(weight_used[layer].shape[0]),np.sqrt(weight_used[layer].shape[0]),i+1)\n","        imgplot = plt.imshow(filters[i])\n","        plt.axis('off')\n","\n","visualize_weights(img, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"838kbLzfED4n","colab_type":"text"},"cell_type":"markdown","source":["The filters are usually small $(3X3)$ and hence the visualization of filter weights usually doesn't give us a clear understanding of the what the filters learn.\n"," \n","Therefore, we will visualize how the the input image looks as it is passed through the various layers in the network.\n","\n","### 4. Visualizing the image as it passes through the network"]},{"metadata":{"id":"7H01mcx-ED4o","colab_type":"code","colab":{}},"cell_type":"code","source":["def to_grayscale(image):\n","    image = torch.sum(image, dim=0)\n","    image = torch.div(image, image.shape[0])\n","    return image\n","\n","def layer_outputs(image):\n","    outputs = []\n","    names = []\n","    \n","    ## feed forward the image through the network and store the outputs\n","    for layer in modules:\n","        image = layer(image) \n","        outputs.append(image)\n","        names.append(str(layer))\n","    \n","    ## for visualization purposes, convert the output into a 2D image by averaging across the filters.\n","    output_im = []\n","    for i in outputs:\n","        i = i.squeeze(0)\n","        temp = to_grayscale(i)  ## convert say 64x112x112 to 112x112\n","        output_im.append(temp.data.numpy())\n","        \n","    fig = plt.figure()\n","    plt.rcParams[\"figure.figsize\"] = (30, 40)\n","\n","\n","    for i in range(len(output_im)):\n","        a = fig.add_subplot(8,4,i+1)\n","        imgplot = plt.imshow(output_im[i])\n","        plt.axis('off')\n","        a.set_title(str(i+1)+\". \"+names[i].partition('(')[0], fontsize=15)\n","\n","#     ##save the resulting visualization\n","#     plt.savefig('layer_outputs.jpg', bbox_inches='tight')\n","\n","##\n","layer_outputs(img)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IAoqS-B2ED4t","colab_type":"text"},"cell_type":"markdown","source":["Through the above visualization, it is clearly visible how the CNN responds to an image at each layer and in the final layer, the pixels of the image which produce the highest activation values are visible as well. However, we still haven't looked at how the filters at each layer respond differently to features present in the input.\n","\n","### 5. Visualizing output of each filter at a given layer "]},{"metadata":{"id":"Ixef3LvtED4u","colab_type":"code","colab":{}},"cell_type":"code","source":["def filter_outputs(image, layer_to_visualize, num_filters=64):\n","    if layer_to_visualize < 0:\n","        layer_to_visualize += 31\n","    output = None\n","    name = None\n","    \n","    ## get outputs corresponding to the mentioned layer\n","    for count, layer in enumerate(modules):\n","        image = layer(image)\n","        if count == layer_to_visualize: \n","            output = image\n","            name = str(layer)\n","    \n","    filters = []\n","    output = output.data.squeeze()\n","\n","    ## if num_filters==-1, visualize all the filters\n","    num_filters = min(num_filters, output.shape[0])\n","    if num_filters==-1:\n","        num_filters = output.shape[0]\n","\n","    for i in range(num_filters):\n","        filters.append(output[i,:,:])\n","        \n","    fig = plt.figure()\n","    plt.rcParams[\"figure.figsize\"] = (10, 10)\n","\n","    for i in range(int(np.sqrt(len(filters))) * int(np.sqrt(len(filters)))):\n","        fig.add_subplot(np.sqrt(len(filters)), np.sqrt(len(filters)),i+1)\n","        imgplot = plt.imshow(filters[i])\n","        plt.axis('off')\n","\n","## if num_filters==-1, visualize all the filters\n","filter_outputs(img,0,16)    #visualize the outputs of first 16 filters of the 1st layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"45imeFMQED41","colab_type":"text"},"cell_type":"markdown","source":["The above visualization shows that each filter responds differently to an input which implies that each filter learns and extracts different features from an input."]},{"metadata":{"id":"8N_hwnQNED42","colab_type":"text"},"cell_type":"markdown","source":["### 6. Understanding Deep Image Representations by Inverting Them [Mahendran, Vedaldi]\n","\n","Like Zeiler and Fergus, their method starts from a specific input image. They record the network’s representation of that specific image and then reconstruct an image that produces a similar code. Thus, their method provides insight into what the activation of a whole layer represent, not what an individual neuron represents.\n","\n","They show what each neuron “wants to see”, and thus what each neuron has learned to look for.\n","\n","To visualize the function of a specific unit in a neural network, we $synthesize$ inputs that cause that unit to have high activation. To synthesize such a “preferred input example”, we start with a random image, meaning we randomly choose a color for each pixel. The image will initially look like colored TV static."]},{"metadata":{"id":"oFKA0TDVED43","colab_type":"code","colab":{}},"cell_type":"code","source":["random_noise_img = Variable(1e-1 * torch.randn(1, 3, 224, 224), requires_grad=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9a83K-4sED47","colab_type":"text"},"cell_type":"markdown","source":["Now we take an image $X$ whose representation $X_0$ at some layer $\"target\\_layer\"$ we want to learn. Our aim is to reconstruct the noise image to get this representation $X_0$. The principle behind this is that the noise image will be so reconstructed such that it will represent what the particular layer for which it is trained against wants to see."]},{"metadata":{"id":"N4NFoCSBED49","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_output_at_nth_layer(inp, layer):\n","    for i in range(layer):\n","        inp = modules[i](inp)\n","    return inp[0]\n","\n","## dont forget that the system is 0 indexed\n","target_layer = 18    ## which is this layer Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","inp_img = normalize(Image.open(\"./dog.jpg\"))\n","inp_img_representation = get_output_at_nth_layer(inp_img, target_layer)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ogtvsBaXED5B","colab_type":"text"},"cell_type":"markdown","source":["Some functions of code (euclidean_loss, alpha_norm, total_variation_norm, recreate_image) are present in utils.py. If you truly want to understand how this is implemented, it is recommended that you readthe second and third page of this [paper](https://arxiv.org/abs/1412.0035), specifically, the regularization part, before asking questions on that. The aim of this code is to mainly understand the deep representations."]},{"metadata":{"id":"i3nk6EiKED5B","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.optim import SGD\n","import cnn_utils\n","\n","#define optimizers for learning the representation of the noise input image\n","optimizer = SGD([random_noise_img], lr=1e4, momentum=0.9)\n","alpha_reg_alpha = 6\n","alpha_reg_lambda = 1e-7\n","tv_reg_beta = 2\n","tv_reg_lambda = 1e-8\n","## Put model into evaluation state\n","model.eval"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HudW3gu0ED5J","colab_type":"code","colab":{}},"cell_type":"code","source":["imgs=[]\n","for i in range(161):\n","        optimizer.zero_grad()\n","        \n","        ## get output at the target layer (not the final layer)\n","        output = get_output_at_nth_layer(random_noise_img,target_layer)\n","        \n","        # Calculate euclidian loss between output image and the target image\n","        euc_loss = 1e-1 * cnn_utils.euclidian_loss(inp_img_representation.detach(), output)\n","        \n","        # regularization\n","        reg_alpha = alpha_reg_lambda * cnn_utils.alpha_norm(random_noise_img, alpha_reg_alpha)\n","        reg_total_variation = tv_reg_lambda * cnn_utils.total_variation_norm(random_noise_img,tv_reg_beta)\n","        \n","        loss = euc_loss + reg_alpha + reg_total_variation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Generate image every 5 iterations\n","        if i % 10 == 0:\n","            print('Iteration:', str(i), 'Loss:', loss.item())\n","            x = cnn_utils.recreate_image(random_noise_img)\n","            imgs.append(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qsuqgehUED5Q","colab_type":"text"},"cell_type":"markdown","source":["### Visualize the learned representations for the layer \"target_layer\""]},{"metadata":{"id":"dd42p_g-ED5Q","colab_type":"code","colab":{}},"cell_type":"code","source":["fig = plt.figure()\n","plt.rcParams[\"figure.figsize\"] = (10, 10)\n","for i in range(int(np.sqrt(len(imgs))) * int(np.sqrt(len(imgs)))):\n","    a = fig.add_subplot(np.sqrt(len(imgs)), np.sqrt(len(imgs)),i+1)\n","    imgplot = plt.imshow(imgs[i])\n","    a.set_title(\"iter = \"+ str((i+1)*10))\n","    plt.axis('off')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Z5yCWHhED5U","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise: Above we visualized the inverted representations for the 18th layer, find similar representations for different layers in the network and thus visualize what the network learns at each of those layer. Try for different target images as well"]},{"metadata":{"id":"GRs9EActQf0p","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"metadata":{"id":"0sUP_aJeED5W","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R01P_KPAQkbV","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dsgoMbn7QmLt","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0WFhZX_TQoEr","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w_vTUzteJIBp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}