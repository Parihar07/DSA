{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"-Va8_ShN-EKe","colab_type":"text"},"cell_type":"markdown","source":["# Foundations of Artificial Intelligence and Machine Learning\n","## A Program by IIIT-H and TalentSprint\n","#### To be done in the Lab"]},{"metadata":{"id":"Je_N7TtA-EKg","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to perform non-linear dimensionality reduction using an Autoencoder. "]},{"metadata":{"id":"Fpeqoffx-EKh","colab_type":"text"},"cell_type":"markdown","source":["In this experiment we  will use the 20 news classification text dataset previously used.\n","\n","#### Data Source\n","\n","http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups."]},{"metadata":{"id":"BR20AjQ--EKi","colab_type":"text"},"cell_type":"markdown","source":["#### Keywords\n","\n","* Autoencoders\n","* Dimensionality reduction\n","* Reconstruction\n","* PCA\n","* MSEloss\n","* Word2vec"]},{"metadata":{"id":"D8AwdKDJ-EKj","colab_type":"text"},"cell_type":"markdown","source":["#### Expected time to complete the experiment is : 60 min"]},{"metadata":{"id":"38LlmdD-r2n2","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"800\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_blr_b6/cfus/week_11/module_3_week_11_experiment_2.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MTIkJjOW-P5R","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"Q4KZVPNS-Rk_","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ngfTquO_-VBs","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kUONBjgBHPio","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M3W11_SAT_EXP_2\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip install torchvision\")\n","    ipython.magic(\"sx pip install Pillow==4.0.0\")\n","    ipython.magic(\"sx pip install gensim\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp2/AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip\")\n","    ipython.magic(\"sx unzip AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp2/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", datetime.datetime.now().date().strftime(\"%d %b %Y\"))\n","      print(\"Time of submission: \", datetime.datetime.now().time().strftime(\"%H:%M:%S\"))\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SZUjWvk_-EKk","colab_type":"text"},"cell_type":"markdown","source":["#### Importing required packages"]},{"metadata":{"id":"lkmPNGdN-EKn","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","### importing pytorch packages\n","import torch\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import torch.autograd as autograd\n","from sklearn import manifold"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BQU75ihE-EKs","colab_type":"text"},"cell_type":"markdown","source":["#### Loading the dataset"]},{"metadata":{"id":"SNAgKctO-EKt","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","dataset = pickle.load(open(\"AIML_DS_NEWSGROUPS_PICKELFILE.pkl\",'rb'))\n","print(dataset.keys())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gJPdVgh1-EK1","colab_type":"text"},"cell_type":"markdown","source":["#### Dividing the dataset into train and test\n","\n","We will use 950 samples from each class in the training set, and the remaining 50 (Are you sure?)  in the test set. "]},{"metadata":{"id":"b7Q4P0BB-EK3","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = {}\n","test_set = {}\n","\n","# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n","for key in dataset:\n","    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n","    \n","# Break dataset into 95-5 split for training and testing\n","n_train = 0\n","n_test = 0\n","for k in dataset:\n","    split = int(0.95*len(dataset[k]))\n","    train_set[k] = dataset[k][0:split]\n","    test_set[k] = dataset[k][split:-1]\n","    n_train += len(train_set[k])\n","    n_test += len(test_set[k])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z5C924mf-EK6","colab_type":"text"},"cell_type":"markdown","source":["#### Loading the predefined word2vec file"]},{"metadata":{"id":"q-Brrxn8-EK7","colab_type":"code","colab":{}},"cell_type":"code","source":["import gensim\n","model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gg4kcLVI-ELA","colab_type":"text"},"cell_type":"markdown","source":["#### Calculating the frequency of the words"]},{"metadata":{"id":"3tmv8TOH-ELB","colab_type":"code","colab":{}},"cell_type":"code","source":["import collections\n","import re\n","import operator\n","frequency = collections.defaultdict(int)\n","    \n","for key in train_set:\n","    for f in train_set[key]:\n","        \n","        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n","        # We ignore all special characters such as !.$ and words containing numbers\n","        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n","    \n","        for word in words:\n","            frequency[word] += 1\n","\n","sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iIQA8ipd-ELF","colab_type":"text"},"cell_type":"markdown","source":["#### Preprocessing the data"]},{"metadata":{"id":"zPFI5rVV-ELG","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","valid_words = collections.defaultdict(int)\n","\n","print('Number of words before preprocessing:', len(sorted_words))\n","\n","# Ignore the 25 most frequent words, and the words which appear less than 100 times\n","ignore_most_frequent = 25\n","freq_thresh = 100\n","feature_number = 0\n","for word, word_frequency in sorted_words[ignore_most_frequent:]:\n","    if word_frequency > freq_thresh:\n","        valid_words[word] = feature_number\n","        feature_number += 1\n","        \n","print('Number of words after preprocessing:', len(valid_words))\n","\n","word_vector_size = len(valid_words)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"werGndNl-ELM","colab_type":"text"},"cell_type":"markdown","source":["#### Function for word2vec"]},{"metadata":{"id":"ICLzdosJc6yS","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","word2vec_vector_size = 300\n","\n","'''\n"," This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency \n"," threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid \n"," vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time,\n"," we store the same noise vector for training and test time in substitute_word_vecs variable.\n","'''\n","def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n","    d = {}\n","    labels = np.zeros((number_of_documents, 1))\n","    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n","    \n","    # Iterate over the dataset and split into words\n","    i = 0\n","    for label, class_name in enumerate(dataset):\n","        for f in dataset[class_name]:\n","            text = ' '.join(f).split(' ')\n","            valid_count = 1\n","            for word in text:\n","                \n","                # Check if word is valid or not according to original dataset pruning\n","                if word in valid_words:\n","                    try:\n","                        w2v_rep[i] += model[word]\n","                        d[word] = model[word]\n","                    except:\n","                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n","                         to account for this. We store the random vector we assigned to the word, and reuse \n","                         the same vector during test time to ensure consistency.'''\n","                        \n","                        if word not in substitute_word_vecs.keys():\n","                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n","                            \n","                        w2v_rep[i] += substitute_word_vecs[word]\n","                    \n","                    valid_count += 1\n","            \n","            # Average\n","            w2v_rep[i] = w2v_rep[i] / valid_count\n","            \n","            # Save label\n","            labels[i] = label\n","            \n","            i += 1\n","\n","    return d, w2v_rep, labels, substitute_word_vecs\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sHA2QArx-ELR","colab_type":"text"},"cell_type":"markdown","source":["#### Convert the train and test datasets into their word2vec representations"]},{"metadata":{"id":"OfC2CyID-ELT","colab_type":"code","colab":{}},"cell_type":"code","source":["d_train,train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n","d_test,test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VtZVJgg1-ELW","colab_type":"text"},"cell_type":"markdown","source":["#### Let us build a autoencoder model. The autoencoder consists of two parts: Encoding, which performs non-linear dimenssionality reduction to give us a compressed representation and decoding, which converts the compressed representation back to original."]},{"metadata":{"id":"ocL1vHQ_-ELX","colab_type":"code","colab":{}},"cell_type":"code","source":["class autoencoder(nn.Module):\n","    def __init__(self):\n","        super(autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(300,100),\n","            nn.ReLU(True),\n","            nn.Linear(100,30),\n","            nn.ReLU(True))\n","        self.decoder = nn.Sequential(\n","            nn.Linear(30,100),\n","            nn.ReLU(True),\n","            nn.Linear(100,300))\n","\n","    def forward(self, x):\n","        y = self.encoder(x)\n","        z = self.decoder(y)\n","        return y,z\n","    \n","\n","\n","model = autoencoder()\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_jx5Dgub7TF","colab_type":"code","colab":{}},"cell_type":"code","source":["EPOCH=10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HYxuYqbxakMP","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(EPOCH):\n","    for i, (word, w2v) in enumerate(d_train.items()):\n","        w2v = torch.FloatTensor(np.array(w2v))\n","        #b_x = Variable(w2v)   # batch x, shape (1, 300)\n","        b_x = w2v\n","        encoded, decoded = model(b_x)\n","        loss = criterion(decoded, b_x)      # mean square error\n","        \n","        optimizer.zero_grad()               # clear gradients for this training step\n","        loss.backward()                     # backpropagation, compute gradients\n","        optimizer.step()                    # apply gradients\n","    print(\"Loss:\",float(loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FhsGPDnk-ELc","colab_type":"text"},"cell_type":"markdown","source":["We will convert training dataset first into a torch tensor, and form a differentiable Variable."]},{"metadata":{"id":"7NSJxx_m-ELd","colab_type":"code","colab":{}},"cell_type":"code","source":["## Converting an array to a tensor \n","value = torch.Tensor(np.array(train_w2v_set))\n","## Creating a pytorch variable\n","text = value   \n","## Calling the model\n","encoder_values, decoder_values = model(text)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sU0c6r0X-ELj","colab_type":"code","colab":{}},"cell_type":"code","source":["### Converting pytorch variable into numpy array\n","encode = encoder_values.data.numpy()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h0_BD-Vx-ELm","colab_type":"code","colab":{}},"cell_type":"code","source":["### Storing all the words in words list\n","words =[]\n","for key in substitute_word_vecs:\n","    words.append(key)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2M6IlvFF-ELt","colab_type":"text"},"cell_type":"markdown","source":["#### Applying T-SNE\n","We will take first 50 values to visualize the data."]},{"metadata":{"id":"iHGvVyJ4-ELu","colab_type":"code","colab":{}},"cell_type":"code","source":["X = encode[:50]\n","tsne_data = manifold.TSNE(n_components=2).fit_transform(X)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UQXPDk1FfPRl","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.scatter(tsne_data[:,0],tsne_data[:,1])\n","for i in range(len(words[:50]) - 1):\n","    plt.annotate(words[i], xy = (tsne_data[i][0],tsne_data[i][1]))\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DnNJ867BkAvv","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"metadata":{"id":"9G0-EU4M-EL8","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TqXGTLowkFUn","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mCZNOFQTkH4U","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mvB_hrVFkKGd","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0faAUVzVIUN6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}