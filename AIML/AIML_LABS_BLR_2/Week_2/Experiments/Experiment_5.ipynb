{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_5.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"UH62CWxTDTpf","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"A8MycyTnDTph","colab_type":"text"},"cell_type":"markdown","source":["#### To be done in Lab"]},{"metadata":{"id":"qu26Vq9jDTpj","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to learn how to word2vec concept in text classification pipeline."]},{"metadata":{"id":"wxuDQQaMDTpk","colab_type":"text"},"cell_type":"markdown","source":["In this experiment we will be exploring the task of news classification, for which we use the 20 news classification dataset. "]},{"metadata":{"id":"e9U_bAdMDTpm","colab_type":"text"},"cell_type":"markdown","source":["#### Data Source\n","\n","http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups."]},{"metadata":{"id":"LkOdbWcbDTpm","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."]},{"metadata":{"id":"-ZYYZDRUDZQe","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"pl3kyR-_DYJG","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d5pX0FDNDfCz","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d1CAyj3-DhnV","colab_type":"code","cellView":"form","outputId":"e721697c-d980-48c9-f6b5-15d77de64fd2","executionInfo":{"status":"ok","timestamp":1542358696107,"user_tz":-330,"elapsed":133878,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"BLR_M1W2_SAT_EXP_5\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx pip3 install gensim\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_NEWSGROUPS_PICKELFILE.pkl\")\n","    #ipython.magic(\"sx unzip AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip?dl=1\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n","    ipython.magic(\"sx unzip AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n","    ipython.magic(\"sx wget https://www.dropbox.com/s/9xivz2pox1i83td/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1\")\n","    ipython.magic(\"sx mv AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1 AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\")\n","    print (\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"id\" : Id, \"file_hash\" : file_hash, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      print(\"Your submission is successful. Ref:\", submission_id)\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"NGUe0eXtcl0K","colab_type":"code","outputId":"ef4d69ae-5ad2-44a0-d001-4f5f37671fb7","executionInfo":{"status":"ok","timestamp":1542358701113,"user_tz":-330,"elapsed":3267,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\n"," AIML_DS_NEWSGROUPS_PICKELFILE.pkl\n","'AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip?dl=1'\n"," AIML_DS_WDBC_NOIDFIELD.data\n","'AIML_DS_WDBC_NOIDFIELD.data.zip?dl=1'\n"," AIML_DS_WORD2VEC2D_STD.pkl\n","'AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1'\n"," BLR_M1W2_SAT_EXP_4.ipynb\n"," BLR_M1W2_SAT_EXP_5.ipynb\n"," sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"iCwSJmXNDTpq","colab_type":"code","colab":{}},"cell_type":"code","source":["# Importing required Packages\n","import pickle\n","import re\n","import operator\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import collections\n","import gensim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"urLEOmkbDTpv","colab_type":"code","outputId":"b185cd05-a7da-4387-fcb1-fd151824a691","executionInfo":{"status":"ok","timestamp":1542358704822,"user_tz":-330,"elapsed":991,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"colab":{"base_uri":"https://localhost:8080/","height":448}},"cell_type":"code","source":["\n","dataset = pickle.load(open('AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n","print(dataset.keys())\n","\n","# Print frequencies of dataset\n","print(\"Class : count\")\n","print(\"--------------\")\n","number_of_documents = 0\n","for key in dataset:\n","    print(key, ':', len(dataset[key]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dict_keys(['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', 'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc'])\n","Class : count\n","--------------\n","talk.politics.mideast : 940\n","rec.autos : 990\n","comp.sys.mac.hardware : 961\n","alt.atheism : 799\n","rec.sport.baseball : 994\n","comp.os.ms-windows.misc : 985\n","rec.sport.hockey : 999\n","sci.crypt : 991\n","sci.med : 990\n","talk.politics.misc : 775\n","rec.motorcycles : 994\n","comp.windows.x : 980\n","comp.graphics : 973\n","comp.sys.ibm.pc.hardware : 982\n","sci.electronics : 981\n","talk.politics.guns : 910\n","sci.space : 987\n","soc.religion.christian : 997\n","misc.forsale : 972\n","talk.religion.misc : 628\n"],"name":"stdout"}]},{"metadata":{"id":"3E1md10WDTpy","colab_type":"text"},"cell_type":"markdown","source":["## Splitting into train and test sets"]},{"metadata":{"id":"R0AYtzi-DTpz","colab_type":"text"},"cell_type":"markdown","source":["Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n","\n","As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "]},{"metadata":{"id":"SjDQqmbMDTp1","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = {}\n","test_set = {}\n","\n","# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n","for key in dataset:\n","    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n","    \n","# Break dataset into 95-5 split for training and testing\n","n_train = 0\n","n_test = 0\n","for k in dataset:\n","    split = int(0.95*len(dataset[k]))\n","    train_set[k] = dataset[k][0:split]\n","    test_set[k] = dataset[k][split:-1]\n","    n_train += len(train_set[k])\n","    n_test += len(test_set[k])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BcXuwTGWDTp5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Initialize a dictionary to store frequencies of words.\n","# Key:Value === Word:Count\n","frequency = defaultdict(int)\n","    \n","for key in train_set:\n","    for f in train_set[key]:\n","        \n","        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n","        # We ignore all special characters such as !.$ and words containing numbers\n","        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n","    \n","        for word in words:\n","            frequency[word] += 1\n","\n","sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n","print(\"Top-10 most frequent words:\")\n","for word in sorted_words[:10]:\n","    print(word)\n","\n","print('----------------------------')\n","print(\"10 least frequent words:\")\n","for word in sorted_words[-10:-1]:\n","    print(word)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VJpqahqtDTp-","colab_type":"code","colab":{}},"cell_type":"code","source":["valid_words = defaultdict(int)\n","\n","print('Number of words before preprocessing:', len(sorted_words))\n","\n","# Ignore the 25 most frequent words, and the words which appear less than 100 times\n","ignore_most_frequent = 25\n","freq_thresh = 100\n","feature_number = 0\n","for word, word_frequency in sorted_words[ignore_most_frequent:]:\n","    if word_frequency > freq_thresh:\n","        valid_words[word] = feature_number\n","        feature_number += 1\n","        \n","print('Number of words after preprocessing:', len(valid_words))\n","\n","word_vector_size = len(valid_words)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bXqK59gGDTqE","colab_type":"text"},"cell_type":"markdown","source":["# 1. Word2Vec"]},{"metadata":{"id":"csm5a0hrDTqI","colab_type":"text"},"cell_type":"markdown","source":["In the previous section we saw a naive way to represent words as dense vectors which can leverage the semantics of the words.\n","\n","The problem with count-based word representations is that they are costly in terms of memory to compute large co-occurrence matrices. Let us see another method to find representations of words without explicitly counting words.\n","\n","Here, we aim to predict the next word given the context in which the word appears. (For example, given the last $n$ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors.\n","\n","Link to pretrained 300 dimensional word2vec: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"]},{"metadata":{"id":"0QBnlIZxDTqK","colab_type":"text"},"cell_type":"markdown","source":["## 1.1 Visualizations\n","\n","Before we go to the actual 300 dimensional vectors, let's try to explore some of the more intriguing properties of word2vec.\n","\n","You have been provided with a sample of word vectors. **We have reduced the dimensionality of the 300-dimensional vectors to 2 dimensions, so that we can plot them in matplotlib.**"]},{"metadata":{"id":"yg0FXnBhDTqL","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_values(values, labels, figsize = (8,4), c = []):\n","    x = []\n","    y = []\n","    for value in values:\n","        x.append(value[0])\n","        y.append(value[1])\n","        \n","    plt.figure(figsize=figsize) \n","    for i in range(len(labels)):\n","        plt.scatter(x[i],y[i], color=c[i])\n","        plt.annotate(labels[i],\n","                     xy=(x[i], y[i]),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')\n","    plt.show()\n","\n","\n","import pickle\n","two_dim_model = pickle.load(open('AIML_DS_WORD2VEC2D_STD.pkl', 'rb'))\n","\n","wv_labels = {}\n","for vec, word in two_dim_model:\n","    wv_labels[word] = vec\n","    \n","colors = ['blue' for i in range(len(wv_labels))]\n","\n","plot_values(wv_labels.values(), list(wv_labels.keys()), figsize = (16, 9), c = colors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W2Yxw_7FDTqP","colab_type":"text"},"cell_type":"markdown","source":["Now, we have given you the 2D representation of different word vectors. Plot the word vectors for the words 'King', 'Queen', 'man', 'women', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest' in green color:"]},{"metadata":{"id":"JMF6vO05DTqQ","colab_type":"code","colab":{}},"cell_type":"code","source":["wv_list = ['king', 'queen', 'man', 'woman', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest']\n","wv_new_labels = {}\n","for word in wv_list:\n","    wv_new_labels[word] = wv_labels[word]\n","\n","colors = ['green' for i in range(len(wv_new_labels))]\n","plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2PbuN6OaDTqU","colab_type":"text"},"cell_type":"markdown","source":["Consider the word analogy question: \"Queen is to King, as Woman is to what?\"\n","\n","To answer this question, we aim to find what the difference between a \"King\" and \"Queen\" is, and apply that difference to a \"Woman\". If we try to put this mathematically, we can write:- \n","$$\n"," Answer = Woman + King - Queen\n","$$\n","\n","Compute the value of the vector on the right hand side of the above equation and plot the resulting vector in red in the same plot as before. "]},{"metadata":{"id":"GHer2vrbDTqV","colab_type":"code","colab":{}},"cell_type":"code","source":["answer = wv_new_labels['woman']  + wv_new_labels['queen'] - wv_new_labels['king']\n","\n","wv_new_labels['answer1'] = answer\n","\n","colors = ['green' if word not in ['answer1'] else 'red' for word in wv_new_labels]\n","\n","plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5kCh_NClDTqY","colab_type":"text"},"cell_type":"markdown","source":["Notice how the answer vector is very close to the vector of the word \"Man\"? Incidentally, \"Man\" is the right answer to the word analogy question! This is the power of Word2Vec representations."]},{"metadata":{"id":"TsODt3Z9DTqa","colab_type":"text"},"cell_type":"markdown","source":["### Exercise 1"]},{"metadata":{"id":"47AFoEUeDTqd","colab_type":"text"},"cell_type":"markdown","source":["Using this technique answer the following questions."]},{"metadata":{"id":"fpoeXWV9DTqf","colab_type":"text"},"cell_type":"markdown","source":["* Germany is to France, as Paris is to?\n","* Best is to Good, as Strongest is to?\n"]},{"metadata":{"id":"19D8C40N2kC3","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mOcxKzuvDTql","colab_type":"text"},"cell_type":"markdown","source":["## 2.2 Load pre-trained Word2Vec\n","\n","Let us now proceed to load the complete pretrained vectors."]},{"metadata":{"id":"yXoawLkSDTqn","colab_type":"code","colab":{}},"cell_type":"code","source":["model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wFdjtmZLDTqq","colab_type":"text"},"cell_type":"markdown","source":["## 2.3 Word2Vec representation\n","\n","Convert each document into average of the word2vec vectors of all valid words in document"]},{"metadata":{"id":"DiZxIihmDTqr","colab_type":"code","colab":{}},"cell_type":"code","source":["word2vec_vector_size = 300\n","\n","'''\n"," This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency \n"," threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid \n"," vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time,\n"," we store the same noise vector for training and test time in substitute_word_vecs variable.\n","'''\n","def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n","    labels = np.zeros((number_of_documents, 1))\n","    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n","    \n","    # Iterate over the dataset and split into words\n","    i = 0\n","    for label, class_name in enumerate(dataset):\n","        for f in dataset[class_name]:\n","            text = ' '.join(f).split(' ')\n","            valid_count = 1\n","            for word in text:\n","                \n","                # Check if word is valid or not according to original dataset pruning\n","                if word in valid_words:\n","                    try:\n","                        w2v_rep[i] += model[word]\n","                    except:\n","                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n","                         to account for this. We store the random vector we assigned to the word, and reuse \n","                         the same vector during test time to ensure consistency.'''\n","                        \n","                        if word not in substitute_word_vecs.keys():\n","                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n","                            \n","                        w2v_rep[i] += substitute_word_vecs[word]\n","                    \n","                    valid_count += 1\n","            \n","            # Average\n","            w2v_rep[i] = w2v_rep[i] / valid_count\n","            \n","            # Save label\n","            labels[i] = label\n","            \n","            i += 1\n","    \n","    return w2v_rep, labels, substitute_word_vecs\n","\n","# Convert the train and test datasets into their word2vec representations\n","train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n","test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lhGbqmGYWX3k","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VaUD0G5QWmls","colab_type":"code","colab":{}},"cell_type":"code","source":["neigh = KNeighborsClassifier(n_neighbors=3)\n","neigh.fit(train_w2v_set, train_w2v_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hrs_bVWhDTqy","colab_type":"text"},"cell_type":"markdown","source":["## 2.4 Document classification using Word2Vec\n","\n","For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."]},{"metadata":{"id":"7Wlk0vKXW3Q_","colab_type":"code","colab":{}},"cell_type":"code","source":["predicted_value = neigh.predict(test_w2v_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fpsGiz45XQ6X","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dyk-FTCiXSBX","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy_score(test_w2v_labels,predicted_value)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zuuW5pauDTq4","colab_type":"text"},"cell_type":"markdown","source":["### Exercise 2"]},{"metadata":{"id":"A_rLlG_zDTq5","colab_type":"text"},"cell_type":"markdown","source":["The frequency thresholds represents the minimum frequency a word must have to be considered relevant. Experiment with the following values of frequency threshold in your preprocessing step from section 1.2. Re-run all the codes with the new set of valid words and report the accuracies. Use the following values:\n","\n","`freq_thresh` = \n","* 10\n","* 1000\n","\n","Report the accuracies using word2vec features."]},{"metadata":{"id":"q6ENxkD92oEV","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mN9_U292DTq-","colab_type":"text"},"cell_type":"markdown","source":["### Exercise 3"]},{"metadata":{"id":"kcurZoWaDTq_","colab_type":"text"},"cell_type":"markdown","source":["In section 2.3, substitute_word_vectors is used as a proxy for a word vector which we do not know. We used a normal gaussian to represent this in that section. Experiment with the type of substitute word vectors used when you do not have a pretrained word vector. Use with the following:\n","\n","`substitute_word_vecs` : \n","* np.ones\n","* np.zeros\n","\n","Report the accuracies using only word2vec features."]},{"metadata":{"id":"GX-DgEQ42sFB","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ot2965O2DTrD","colab_type":"text"},"cell_type":"markdown","source":["### Exercise 4"]},{"metadata":{"id":"IGBzFigADTrE","colab_type":"text"},"cell_type":"markdown","source":["To classify news articles into their 20 news groups, experiment with three different parameter values with the following parameter choices.\n","\n","* K-NN \n"," ** K : 10, 50\n"," ** Distance Metric : Euclidean.\n","\n","\n","Report the accuracies using  word2vec features."]},{"metadata":{"id":"PFFxKvDH2vCG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8YIha6yMtyuu","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"N7IGEVYztuqt","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_W8f70N9t1ej","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hKI_65_Zt31W","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x1RNSXbVuKcP","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]}]}